---
title: "Thinking of data as space"
layout: post
hashtag: "#linear-algebra"
comment: true
splitter: 5
featured: false
inprogress: true
thumbnail: /img/intuitive-understanding-linear-algebra-6.png
---

<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
<style>
    .mjx-chtml {
        font-size: 100% !important; /* Adjust the size as needed */
    }
</style>

<div id="toc"></div>

<div style="text-align: center;">
    <i>This article's contents are based on the book <a href="https://product.kyobobook.co.kr/detail/S000001792467">Linear Algebra for Programmers</a>.</i>
</div><br><br>

<h3>When thinking of data as space, intuition works üí°</h3>
<div class="article">

    We live in a three-dimensional space. To handle problems in a 3D world, we need terms that describe this 'space' well. 
    Examples include computer graphics, car navigation, and games. 
    The vector spaces in linear algebra abstract certain properties of our real space. Therefore, linear algebra provides convenient terms and concepts to describe space. 
    For instance, if you're thinking about 'how to draw a 3D object on a 2D plane,' you encounter the problem of 'what 2D image appears when you move and rotate the viewpoint in 3D space.' 
    Linear algebra plays a fundamental role in solving such problems.

    <br><br>
    
    However, we don't learn linear algebra just to solve problems in space.

    <br><br>
    
    You'll often want to handle data composed of multiple numbers, not just single values. 
    These cases might not be directly related to 'space,' so you can handle them without explicitly thinking about 'space.' 
    But if you interpret this data as 'points in high-dimensional space,' you can use your <code>spatial intuition</code>.

    <br><br>
    
    Although we can only recognize three-dimensional space, many 'general n-dimensional phenomena' can be intuitively understood by <code>reasoning</code> in three-dimensional space. 
    This interpretation is effective in data analysis. When it comes to 'space' problems, linear algebra steps in. 
    <!--break-->

    <br><br>

    \[
        Data = \begin{bmatrix}
        -4.310345 & 4.655172 & -0.517241 & -1.551724 & 3.965517 & 3.275862 & 4.310345 & \cdots \\
        -4.310345 & -4.310345 & -4.310345 & -4.310345 & -4.310345 & -4.310345 & -4.310345 & \cdots \\
        0.701754 & 4.350436 & 2.368040 & 4.004554 & 2.602387 & 3.394398 & 3.603163 & \cdots \\
        \end{bmatrix}
    \]

    <br><br>



    <figure style="display: flex;">
        <img src="/img/intuitive-understanding-linear-algebra-4.png" width="32%">
        <img src="/img/intuitive-understanding-linear-algebra-6.png" width="32%">
        <img src="/img/intuitive-understanding-linear-algebra-5.png" width="32%">
    </figure>
    <figcaption>
        <a href="https://github.com/PARKCHEOLHEE-lab/intuitive-understanding-of-linear-algebra/blob/main/intuitive_understanding_of_linear_algebra/notebooks/introduction.ipynb">Thinking of \( Data \) as 3D space</a>, it easier to understand the meaning of data.<br>
        From the left, Data points ¬∑ Data points projection ¬∑ Intuitive understanding of data shape
    </figcaption>

</div>
<br><br>

<h3>Vector and Space</h3>
<div class="article">
    <b>üß¨ <a href="https://github.com/PARKCHEOLHEE-lab/intuitive-understanding-of-linear-algebra/blob/main/intuitive_understanding_of_linear_algebra/notebooks/basis.ipynb">Basis</a></b><br>
    A basis is a set of vectors that defines a <code>coordinate system</code> for a vector space. By choosing a set of basis vectors, we can specify the position of any vector within that space relative to these basis vectors.<br><br>
    For example, consider the standard basis vectors \( \mathbf{e_1} \) and \( \mathbf{e_2} \) in \( \mathbb{R}^2 \). 
    These vectors form a convenient coordinate system. Any vector \( \mathbf{v} \) in this space can be expressed as a <code>linear combination</code> of these basis vectors.
    This means \( \mathbf{v_1} \) can be decomposed into 2 units of \( \mathbf{e_1} \) and 3 units of \( \mathbf{e_2} \). <br>
    
    <p style="padding-left: 2em;">\( \mathbf{e_1} = [1, 0]^T \) <br> \( \mathbf{e_2} = [0, 1]^T \)</p>
    <p style="padding-left: 2em;">\( \mathbf{v_1} = [2, 3]^T = 2\mathbf{e_1} + 3\mathbf{e_2} \) <br> \( \mathbf{v_2} = [3.3, 1.3]^T = 3.3\mathbf{e_1} + 1.3\mathbf{e_2} \)</p>

    <figure style="display: flex;">
        <img src="/img/intuitive-understanding-linear-algebra-0.png" width="40%">
        <img src="/img/intuitive-understanding-linear-algebra-2.png" width="40%">
    </figure>
    <figcaption>
        Standard basis vectors in \( \mathbb{R}^2 \)
    </figcaption>

    <br><br><br>

    Basis vectors do not need to be <code>orthogonal</code> or have unit length. Let's consider a different set of basis vectors in \( \mathbb{R}^2 \)!
    We can express any vector in terms of these new basis vectors with the below specific basis. Here, \( \mathbf{v_2} \) is decomposed into 1.2 units of \( \mathbf{e_1} \) and 2.7 units of \( \mathbf{e_2} \). 
    <p style="padding-left: 2em;">\( \mathbf{e_1} = [1, -0.1]^T \) <br> \( \mathbf{e_2} = [1, 1]^T \)</p>
    <p style="padding-left: 2em;">\( \mathbf{v_1} = [4, 1.8]^T = 2\mathbf{e_1} + 2\mathbf{e_2} \) <br> \( \mathbf{v_2} = [3.9, 2.58]^T = 1.2\mathbf{e_1} + 2.7\mathbf{e_2} \) </p>

    <figure style="display: flex;">
        <img src="/img/intuitive-understanding-linear-algebra-1.png" width="40%">
        <img src="/img/intuitive-understanding-linear-algebra-3.png" width="40%">
    </figure>
    <figcaption>
        Specific basis vectors in \( \mathbb{R}^2 \)
    </figcaption>

    <br><br><br>

    <b>üß¨ Conditions for becoming Basis</b><br>
    In linear algebra, a basis for a vector space is a set of vectors that satisfy two fundamental conditions:

    <ul style="padding-left: 2em;">
        <li class="decimal">
            <b>Spanning the Vector Space</b>:
            A set of vectors spans a vector space if every vector in that space can be expressed as a <code>linear combination</code> of the vectors in the set. 
            This means that any vector in the space can be reached by scaling and adding the basis vectors. 
        
        </li>
        <li class="decimal">
            <b>Linear Independence</b>: 
            The vectors in the set must be linearly independent, which means that no vector in the set can be written as a linear combination of the others. 
            Put simply, each vector creates a <code>unique</code> dimension to the space that cannot be duplicated by any combination of the other vectors.
        </li>
    </ul><br>

    Imagine a vector space as a room, and the basis vectors as different directions you can travel within that room. 
    If you can move in every possible direction (<code>span</code>), then the basis vectors cover the entire space.
    If \( \{ \mathbf{v}_1, \mathbf{v}_2, \ldots, \mathbf{v}_n \} \) is a set of vectors in a vector space \( V \), then \( V \) is spanned by these vectors if every vector \( \mathbf{v} \in V \) can be expressed as \( \mathbf{v} = c_1 \mathbf{v}_1 + c_2 \mathbf{v}_2 + \ldots + c_n \mathbf{v}_n \), where \( c_1, c_2, \ldots, c_n \) are scalars.


    <br><br>
    <figure>
        <img src="/img/intuitive-understanding-linear-algebra-7.png" width="100%">
        <figcaption>
            Example of whether a set of vectors is a basis or not <br>
            From the left, ‚úîÔ∏è ¬∑ ‚ùå ¬∑ ‚ùå ¬∑ ‚ùå
        </figcaption>
    </figure><br><br>

    <b>üß¨ Dimension</b><br>
    The number of elements (or coordinates) needed to describe a vector in a given space corresponds to the dimension of that space. 
    For instance, in 3-dimensional space, a vector is typically represented as (x, y, z), requiring three coordinates. Therefore, <br><br>
    <div style="text-align: center;">
        <i>Dimension = the number of basis vectors = the number of elements of coordinates</i>
    </div>
</div>
<br><br>

<h3>Matrix and Mapping</h3>
<div class="article">
    <b>üß¨ <a href="https://github.com/PARKCHEOLHEE-lab/intuitive-understanding-of-linear-algebra/blob/main/intuitive_understanding_of_linear_algebra/notebooks/matrix.ipynb">Matrix is mapping</a></b><br>
    When an \( m \times n \) matrix \( A \) is multiplied by an \( n \)-dimensional vector \( x \), 
    an \( m \)-dimensional vector \( y = Ax \) is obtained. 
    In other words, specifying matrix \( A \) determines a mapping that transforms one vector into another. 
    This is the most important role of a matrix. 
    From now on, when you see a matrix, do not simply think of it as a collection of numbers, 
    but rather as a given <code>mapping</code>.
    
    <br><br>

    If you can think of 'how the entire space changes', it will help you intuitively understand linear algebra. 
    Let's look at the animations below.
    These animations depict the transformation of data and the standard basis vectors by matrix mappings.
    Each animation uses the following matrix as its <code>transformation</code> matrix. The entire space is mapped from the standard basis to \(A1\) and \(A2\) respectively:
    <br><br>

    <div style="display: flex; justify-content: space-around;">
        <div>
            \[
            \text{A1} = 
            \begin{bmatrix}
                1 & -0.7 \\
                -0.3 & 0.6
            \end{bmatrix}
            \]
        </div>
        <div>
            \[
            \text{A2} = 
            \begin{bmatrix}
                -1 & 0.3 \\
                -0.3 & -1.2
            \end{bmatrix}
            \]
        </div>
    </div>

    <figure style="display: flex;">
        <img src="/img/intuitive-understanding-linear-algebra-8.gif" width="40%">
        <img src="/img/intuitive-understanding-linear-algebra-9.gif" width="40%">
    </figure>
    <figcaption>
        how the entire space changes
    </figcaption><br><br>

    Let's look at matrix \( A1 \). It was mentioned above that a matrix is a mapping. 
    What \( A1 \) represents is the transformation of the existing space's basis into the basis represented by \( A1 \).
    Therefore, \( [1, 0]^T \) is mapped to \( [1, -0.3]^T \), and \( [0, 1]^T \) is mapped to \( [-0.7, 0.6]^T \).

    In other words, the first column of \( A1 \) represents the destination of \( [1, 0]^T \), and the second column represents the destination of \( [0, 1]^T \).
    Imagining where each vector moves helps you visualize the form of the mapping.

    <br><br>

    In summary, an \(m \times n\) matrix \(A\) represents a mapping that moves an \(n\)-dimensional space to an \(m\)-dimensional space. 
    The first column of \(A\) represents the <code>destination</code> of \(e_1 = (1, 0, 0, \ldots)^T\), and the second column of \(A\) represents the destination of \(e_2 = (0, 1, 0, \ldots)^T\).

    <br><br>

    The matrix \( A \) accepts vectors from an \( n \)-dimensional space as inputs and transforms them into vectors in an \( m \)-dimensional space. 
    For example, if we have an \( n \)-dimensional vector \( \mathbf{x} \), the <code>matrix-vector product</code> \( A\mathbf{x} \) results in an \( m \)-dimensional vector.
    
    <br><br>
    
    <div style="display: flex; justify-content: space-around;">
        <div>
            \[
            \begin{bmatrix}
                1 & 0 \\
                0 & 1
            \end{bmatrix}
            „ÄÄ\rightarrow„ÄÄ
            \begin{bmatrix}
                2 & 1 \\
                1 & 3
            \end{bmatrix}
            \]
        </div>
        <div>
            \[
            \begin{bmatrix}
                1 & 0 & 0 \\
                0 & 1 & 0 \\
                0 & 0 & 1
            \end{bmatrix}
            „ÄÄ\rightarrow„ÄÄ
            \begin{bmatrix}
                2 & 9 & 4 \\
                7 & 5 & 3 \\
                6 & 8 & 1
            \end{bmatrix}
            \]
        </div>
    </div>

    <div>
        \[
            \vdots
        \]
        <br>
    </div>

    <div>
        \[
        \begin{bmatrix}
            1 & 0 & 0 & \cdots & 0 \\
            0 & 1 & 0 & \cdots & 0 \\
            0 & 0 & 1 & \cdots & 0 \\
            \vdots & \vdots & \vdots & \ddots & \vdots \\
            0 & 0 & 0 & \cdots & 1
        \end{bmatrix}
            „ÄÄ\rightarrow„ÄÄ
        \begin{bmatrix}
            a_{11} & a_{12} & a_{13} & \cdots & a_{1n} \\
            a_{21} & a_{22} & a_{23} & \cdots & a_{2n} \\
            a_{31} & a_{32} & a_{33} & \cdots & a_{3n} \\
            \vdots & \vdots & \vdots & \ddots & \vdots \\
            a_{n1} & a_{n2} & a_{n3} & \cdots & a_{nn}
        \end{bmatrix}
        \]
    </div><br><br>

    <figure>
        <img src="/img/intuitive-understanding-linear-algebra-10.png" width="50%">
        <figcaption>
            Destinations of the standard basis vectors <br>
            \[
            \begin{bmatrix}
                1 & 0 \\
                0 & 1
            \end{bmatrix}
            „ÄÄ\rightarrow„ÄÄ
            \begin{bmatrix}
                2 & 1 \\
                1 & 3
            \end{bmatrix}
            \]
        </figcaption><br><br>
    </figure>

    <b>üß¨ <a href="https://github.com/PARKCHEOLHEE-lab/intuitive-understanding-of-linear-algebra/blob/main/intuitive_understanding_of_linear_algebra/notebooks/matrix-multiplication.ipynb">Matrix multiplication</a></b><br>
    Multiplication between matrices is a combination of mappings.
    If there are matrices \( B \in \mathbb{R}^{m \times n} \) and \( A \in \mathbb{R}^{n \times k} \),
    its multiplication is defined as follows:
    <br><br>
    \[
    \begin{bmatrix}
        b_{11} & \cdots & b_{1n} \\
        \vdots & „ÄÄ & \vdots \\
        b_{m1} & \cdots & b_{mn}
    \end{bmatrix}
    \begin{bmatrix}
        a_{11} & \cdots & a_{1k} \\
        \vdots & „ÄÄ & \vdots \\
        a_{n1} & \cdots & a_{nk}
    \end{bmatrix}
    \]
    <br>
    \[
    =
    \begin{bmatrix}
        (b_{11}a_{11}) + \cdots + (b_{1n}a_{n1}) & \cdots & (b_{11}a_{1k}) + \cdots + (b_{1n}a_{nk}) \\
        \vdots & „ÄÄ & \vdots \\
        (b_{m1}a_{11} + \cdots + b_{mn}a_{n1}) & \cdots & (b_{m1}a_{1k} + \cdots + b_{mn}a_{nk})
    \end{bmatrix}
    \]
    \[
    <br>
    \]
    \[
        \begin{bmatrix}
            2 & 7 \\
            9 & 5 \\
            4 & 3
        \end{bmatrix}
        \begin{bmatrix}
            1 & 3 \\
            2 & -1
        \end{bmatrix}
        =
        \begin{bmatrix}
            (2 \cdot 1 + 7 \cdot 2) & (2 \cdot 3 + 7 \cdot -1) \\
            (9 \cdot 1 + 5 \cdot 2) & (9 \cdot 3 + 5 \cdot -1) \\
            (4 \cdot 1 + 3 \cdot 2) & (4 \cdot 3 + 3 \cdot -1)
        \end{bmatrix}
        =
        \begin{bmatrix}
            16 & -1 \\
            19 & 22 \\
            10 & 9
        \end{bmatrix}
    \]

    <br><br>

    What we need to pay attention to here is the dimension of the matrix multiplication result.
    The matrix multiplication result in the above case where multiplying a \( 3 \times 2 \) matrix by a \( 2 \times 2 \) matrix 
    has dimensions of \( 3 \times 2 \). 

    In other words, the multiplication of an \( m \times n \) matrix and an \( n \times k \) matrix can be expressed as \( m \times k \).

    <br><br>

    Next, let's think about the multiplication of three or more matrices.
    In the matrix multiplication, the <code>associative law</code> is valid. 
    Therefore, 
    
    <p style="padding-left: 2em;">\( C(BA) = (CB)A = CBA \)</p>
    <p style="padding-left: 2em;">\( D(C(BA)) = D((CB)A) = (D(CB))A = ((DC)B)A = (DC)(BA) = DCBA \)</p>

    <br>

    The order of applying matrix multiplication is right-to-left. 
    In the above, \( CBA \) means that the first operation is \( A \times B \), and then the result is multiplied by \( C \)

    <br><br>

    But in the matrix multiplication, the <code>commutative law</code> is invalid. So, \( AB \neq BA \).
    Depending on the sizes of matrices \( A \) and \( B \), the multiplication may not be defined.

    For example, consider two matrices where \( A \) is a \( 2 \times 3 \) matrix and \( B \) is a \( 3 \times 4 \) matrix.
    <br><br>

    \[
    \begin{bmatrix}
        * & * & * \\
        * & * & *
    \end{bmatrix}
    \begin{bmatrix}
        * & * & * & * & * \\
        * & * & * & * & * \\
        * & * & * & * & *
    \end{bmatrix}
    =
    \begin{bmatrix}
        * & * & * & * & * \\
        * & * & * & * & *
    \end{bmatrix}
    \]
    \[
        <br>
    \]
    \[
    \begin{bmatrix}
    * & * & * & * & * \\
    * & * & * & * & * \\
    * & * & * & * & *
    \end{bmatrix}
    \begin{bmatrix}
        * & * & * \\
        * & * & *
    \end{bmatrix}
    „ÄÄ\rightarrow „ÄÄ‚ùå
    \]

    <br><br>

    If the multiplications \(AB\) and \(BA\) are possible, the results are different.
    Let's think about the following matrices:
    <p style="padding-left: 2em;">\( A = \begin{bmatrix} 0 & -1 \\ 1 & 0 \end{bmatrix} \)</p>
    <p style="padding-left: 2em;">\( B = \begin{bmatrix} 2 & 0 \\ 0 & 1 \end{bmatrix} \)</p>

    
    <figure style="display: flex;">
        <img src="/img/intuitive-understanding-linear-algebra-11.gif" width="40%">
        <img src="/img/intuitive-understanding-linear-algebra-12.gif" width="40%">
    </figure>
    <figcaption>
        Different results of matrix multiplication for \( AB \) and \( BA \) <br>
        \[
            AB = \begin{bmatrix}
                0 & -2 \\
                1 & 0
            \end{bmatrix}
            „ÄÄ
            BA = \begin{bmatrix}
                0 & -1 \\
                2 & 0
            \end{bmatrix}
        \]
    </figcaption> <br><br>

    The matrix \(A\) <code>rotates</code> the space by 90 degrees relative to the standard basis,
    and matrix \(B\) <code>extends</code> the space horizontally relative to the standard basis.
    Therefore, the result of the space by \(AB\) is firstly extended horizontally and then rotated 90 degrees.
    On the other hand, the result of the space by \(BA\) is firstly rotated 90 degrees and then extended horizontally.

    <br><br><br>

    <b>
        üß¨ <a href="https://github.com/PARKCHEOLHEE-lab/intuitive-understanding-of-linear-algebra/blob/main/intuitive_understanding_of_linear_algebra/notebooks/zero-matrix.ipynb">Zero matrix</a>, 
        <a href="https://github.com/PARKCHEOLHEE-lab/intuitive-understanding-of-linear-algebra/blob/main/intuitive_understanding_of_linear_algebra/notebooks/identity-matrix.ipynb">Identity matrix</a>, 
        <a href="https://github.com/PARKCHEOLHEE-lab/intuitive-understanding-of-linear-algebra/blob/main/intuitive_understanding_of_linear_algebra/notebooks/diagonal-matrix.ipynb">Diagonal matrix</a>
    </b>
    <br>

    The matrix consisting of all zeros is called the <code>zero matrix</code>.
    \[
        O_{2, 3} = \begin{bmatrix}
            0 & 0 \\
            0 & 0
        \end{bmatrix}
        „ÄÄ
        O_{3} = \begin{bmatrix}
            0 & 0 & 0 \\
            0 & 0 & 0 \\
            0 & 0 & 0
        \end{bmatrix}
    \]

    
    The zero matrix mapping moves all elements in the vector space to the origin.
    This is because for any vector \( x \), \( Ax = O \).

    <br><br>

    <figure>
        <img src="/img/intuitive-understanding-linear-algebra-13.gif" width="40%">
        <figcaption>
            The zero matrix mapping process <br>
            \[
                \begin{bmatrix}
                    1 & 0 \\
                    0 & 1
                \end{bmatrix}
                \rightarrow
                \begin{bmatrix}
                    0 & 0 \\
                    0 & 0
                \end{bmatrix}
            \]
        </figcaption>
    </figure>
    <br><br>

    In the square matrix, the <code>identity matrix</code> is a matrix where the diagonal elements are all ones and the rest are all zeros.
    The identity matrix is expressed as \( I \).
    \[
        I_{2} = \begin{bmatrix}
            1 & 0 \\
            0 & 1
        \end{bmatrix}
        „ÄÄ
        I_{3} = \begin{bmatrix}
            1 & 0 & 0 \\
            0 & 1 & 0 \\
            0 & 0 & 1
        \end{bmatrix}
        „ÄÄ
        I_{n} = \begin{bmatrix}
            1 & 0 & 0 & \cdots & 0 \\
            0 & 1 & 0 & \cdots & 0 \\
            \vdots & \vdots & \vdots & \ddots & \vdots \\
            0 & 0 & 0 & \cdots & 1
        \end{bmatrix}
    \]

    <br><br>
    The identity matrix \( I \) represents the transformation that does not change the vector space. 
    In other words, the identity matrix moves any vector \( x \) to the same vector \( x \).
    This is because for any vector \( x \), \( Ix = x \).

    <br><br>

    <figure>
        <img src="/img/intuitive-understanding-linear-algebra-14.gif" width="40%">
        <figcaption>
            The identity matrix mapping process <br>
            There is no change in the vector space
            \[
                \begin{bmatrix}
                    1 & 0 \\
                    0 & 1
                \end{bmatrix}
                \rightarrow
                \begin{bmatrix}
                    1 & 0 \\
                    0 & 1
                \end{bmatrix}
            \]
        </figcaption>
    </figure>
    <br><br>

    In a square matrix, all elements located at the diagonal are called the <code>diagonal elements</code>.
    If the non-diagonal elements are all 0, this matrix is called a <code>diagonal matrix</code>.
    \[
        \begin{bmatrix}
            2 & 0 \\
            0 & 5
        \end{bmatrix}
        „ÄÄ
        \begin{bmatrix}
            -1.3 & 0 & 0 \\
            0 & \sqrt{7} & 0 \\
            0 & 0 & 1/\pi
        \end{bmatrix}
        „ÄÄ
        \begin{bmatrix}
            3 & 0 & 0 & 0 & 0 \\
            0 & 1 & 0 & 0 & 0 \\
            0 & 0 & 4 & 0 & 0 \\
            0 & 0 & 0 & 1 & 0 \\
            0 & 0 & 0 & 0 & 5
        \end{bmatrix}
    \]

    <br><br>

    The mapping for the diagonal matrix is <code>scaling</code>.
    Each diagonal element is a scaling factor for the corresponding basis vector.
    
    <figure style="display: flex;">
        <img src="/img/intuitive-understanding-linear-algebra-15.gif" width="32%">
        <img src="/img/intuitive-understanding-linear-algebra-16.gif" width="32%">
        <img src="/img/intuitive-understanding-linear-algebra-17.gif" width="32%">
    </figure>
    <figcaption>
        The mapping process for the diagonal matrix <br>
        \[
            \begin{bmatrix}
                1 & 0 \\
                0 & 1
            \end{bmatrix}
            \\
            „ÄÄ
            \\
            \downarrow
            \\
            „ÄÄ
            \\
            \begin{bmatrix}
                0.7 & 0 \\
                0 & 1.5
            \end{bmatrix}
            „ÄÄ
            \begin{bmatrix}
                0 & 0 \\
                0 & 1.5
            \end{bmatrix}
            „ÄÄ
            \begin{bmatrix}
                -0.7 & 0 \\
                0 & -1.5
            \end{bmatrix}
        \]
    </figcaption><br><br>

    In the above figure 2 where \( D \) has a 0 for the diagonal element, the vector space is <code>flattened</code>.
    In the above figure 3 where \( D \) has negative values for the diagonal elements, the vector space is <code>reflected</code>.

</div>
<br><br>



<h3>References</h3>
<div class="article">

    <ul style="padding-left: 2em;">
        <li><a href="https://product.kyobobook.co.kr/detail/S000001792467">https://product.kyobobook.co.kr/detail/S000001792467</a></li>
        <li><a href="https://www.3blue1brown.com/topics/linear-algebra">https://www.3blue1brown.com/topics/linear-algebra</a></li>
    </ul>
    
</div><br><br>