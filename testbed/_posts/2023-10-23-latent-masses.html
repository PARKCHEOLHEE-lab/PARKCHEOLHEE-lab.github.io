---
title: "Latent masses"
layout: post
hashtag: "#deep-learning #generative-adversarial-networks #machine-learning #gan"
featured: false
comment: true
splitter: 2
thumbnail: /img/latent-masses-thumbnail.gif
---

<div id="toc"></div>
<h3>Objectives</h3>
<div class="article">
    Generative Adversarial Networks (GANs) have paved the way for unprecedented advancements in numerous areas, from art creation to deepfake video generation. 
    However, the potential of GANs isn't restricted to 2D space. The development and application of 3D GANs have opened new possibilities, especially in the realm of design.

    <br><br>

    This project delves deep into the <code>possibilities of 3D GANs in the design field</code> with the following objectives:
    <ul style="padding-left: 2em;">
        <li>Grasp the fundamental concepts behind GANs and their 3D extension</li>
        <li>Appreciate the power and nuances of 3D GANs through hands-on experiments</li>
        <li>Examine how 3D GANs can be harnessed for product design, architectural modeling, and virtual environment creation</li>
        <li>visualize and manipulate the <a href="https://medium.com/hackernoon/latent-space-visualization-deep-learning-bits-2-bd09a46920df">latent space</a> to generate novel and innovative designs</li>
        <li>Understand the limitations of current 3D GAN models and the potential areas of improvement</li>
        <!--break-->
    </ul><br>

    <figure>
        <img src="/img/latent-masses/latent-masses-7.png" style="width: 70%;" onerror=handle_image_error(this)><br>
        <figcaption>Interpolation in <a href="https://hsejun07.tistory.com/100">latent space</a></figcaption>
    </figure></div>

</div><br>

<h3>By the way, what is GANs(Generative Adversarial Networks)? ðŸ§¬</h3>
<div class="article">

    Generative Adversarial Networks, commonly referred to as GANs, are a class of artificial intelligence algorithms <code>designed to generate new data</code> that resemble a given set of data. 
    The architecture of a GAN consists of two primary components:
    <br><br>
    1. Generator
    <ul style="padding-left: 2em;">
        <li>The role of the generator is to create fake data</li>
        <li>It takes in random noise from a latent space and produces data samples as its output</li>
        <li> The primary objective of the generator is to <code>produce data that is indistinguishable</code> from real data</li>
    </ul><br>
    2.Discriminator
    <ul style="padding-left: 2em;">
        <li>The discriminator functions as a binary classifier</li>
        <li>It aims to <code>differentiate between real and fake data</code></li>
        <li>
            The discriminator receives both real data samples and the fake data generated by the generator, 
            and its task is to correctly label them as 'real' or 'fake'
        </li>
    </ul><br>

    The provided diagram illustrates this process, showing how the generator's output is evaluated by the discriminator, resulting in a <code>loss</code> that helps both parts improve.

    <figure>
        <img src="/img/latent-masses/latent-masses-10.jpg" style="width: 70%;" onerror=handle_image_error(this)><br>
        <figcaption><a href="https://whatisai.blog/GAN1/">Generative adversarial networks</a> concept diagram</figcaption>
    </figure>

</div><br>

<h3>3D shape representations for the generative adversarial networks</h3>
<div class="article">
    
        1. Point cloud
        <ul style="padding-left: 2em;">
            <li>
                A point cloud is a set of data points in space. 
                In 3D shape representation, point clouds are typically used to <code>represent the external surface</code> of an object
                Each point in the point cloud has an (x, y, z) coordinate
            </li>
            <li>Can represent any 3D shape without being limited to a specific topology or grid (Good at flexibility)</li>
            <li>Points are disconnected, so additional processing is often required to extract surfaces or other features (Not good at lack of connectivity)</li>
        </ul>

        <figure>
            <img src="/img/latent-masses/latent-masses-1-0.png" style="width: 30%;" onerror=handle_image_error(this)><br>
            <figcaption>
                <a href="https://medium.com/@phamtdong0406/1-3d-ai-data-representation-99c98ec402c6">Shape representation</a> for point cloud<br>
            </figcaption>
        </figure>
        2. Voxel
        <ul style="padding-left: 2em;">
            <li>
                Voxels (short for volumetric pixels) are the 3D equivalent of 2D pixels. 
                A voxel representation <code>divides the 3D space into a regular grid, and each cell</code> (or voxel) in the grid can be either occupied or empty
            </li>
            <li>Operations like convolution are straightforward to apply on voxel grids (Simplicity)</li>
            <li>To represent fine details, a very high-resolution grid is needed, which can be computationally prohibitive (Limited resolution)</li>
        </ul>

        <figure>
            <img src="/img/latent-masses/latent-masses-1-1.png" style="width: 30%;" onerror=handle_image_error(this)><br>
            <figcaption>
                <a href="https://medium.com/@phamtdong0406/1-3d-ai-data-representation-99c98ec402c6">Shape representation</a> for voxel<br>
            </figcaption>
        </figure>
        3. Mesh
        <ul style="padding-left: 2em;">
            <li>
                A 3D mesh consists of vertices, edges, and faces that define the shape of a 3D object in space. 
                The most common type of mesh is a triangular mesh, where the shape is <code>represented using triangles</code>
            </li>
            <li>Can represent both simple and complex geometries (Good expressiveness)</li>
            <li>Provides information about how points are connected, which is useful for many applications (Good at continuous surface representation)</li>
            <li>Operations on meshes, like subdivision or simplification, can be computationally demanding (Complexity)</li>
        </ul>

        <figure>
            <img src="/img/latent-masses/latent-masses-1-2.png" style="width: 30%;" onerror=handle_image_error(this)><br>
            <figcaption>
                <a href="https://medium.com/@phamtdong0406/1-3d-ai-data-representation-99c98ec402c6">Shape representation</a> for mesh<br>
            </figcaption>
        </figure>
</div><br>

<h3>Simple implementation: A single sphere GAN</h3>
<div class="article">
    First, I'll implement a practical application of training a GAN on point cloud data, aiming to generate a single sphere, represented by point cloud
    Before implementing the neural networks, we begin by loading our target sphere point cloud from a file. I modeled just one sphere shape using Rhino.

    <br><br>
    Typically, the normalization can be particularly beneficial if your training data consists of similar objects in various sizes or if the absolute size isn't critical for your task.
    In our dataset concerning a single sphere, the absolute size is not of significance. Therefore, let's normalize it. The sphere can be normalized easily using <code>numpy</code> as follows:

<pre><code class="python">
    class Normalize:
        def __call__(self, pointcloud):
            assert len(pointcloud.shape) == 2
            
            norm_pointcloud = pointcloud - np.mean(pointcloud, axis=0) 
            norm_pointcloud /= np.max(np.linalg.norm(norm_pointcloud, axis=1))
            
            return norm_pointcloud
</pre></code><br>

    If different 3D models have a different number of vertices, sampling a consistent number of points from each model <code>ensures that the input size</code> remains uniform. This is crucial when feeding data to neural networks that expect consistent input sizes.
    Please refer to the following <a href="https://github.com/PARKCHEOLHEE-lab/gan-exercise/blob/main/sphereGAN/utils.py#L9-L54">link</a> for the code related to PointSampler.

    <figure style="display: flex;">
        <img src="/img/latent-masses/latent-masses-2.png" style="width: 32%;" onerror=handle_image_error(this)><br>
        <img src="/img/latent-masses/latent-masses-5.png" style="width: 32%;" onerror=handle_image_error(this)><br>
        <img src="/img/latent-masses/latent-masses-6.png" style="width: 32%;" onerror=handle_image_error(this)><br>
    </figure>
    <figcaption>
        A sphere, represented by point cloud <br>
        From the left, original sphere Â· random sampled sphere Â· normalized and random sampled sphere
    </figcaption><br><br>

    Now, we have completed the data preprocessing and it is now ready for training model. 
    Let us establish and train models comprising a simple generator and discriminator as follows:

<pre><code class="python">
    class Generator(nn.Module):
        def __init__(self, input_dim=3, output_dim=3, hidden_dim=128):
            super(Generator, self).__init__()
            self.fc1 = nn.Linear(input_dim, hidden_dim)
            self.fc2 = nn.Linear(hidden_dim, hidden_dim)
            self.fc3 = nn.Linear(hidden_dim, hidden_dim)
            self.fc4 = nn.Linear(hidden_dim, output_dim)
    
        def forward(self, x):
            x = torch.relu(self.fc1(x))
            x = torch.relu(self.fc2(x))
            x = torch.relu(self.fc3(x))
            x = torch.tanh(self.fc4(x))
            
            return x
    
    class Discriminator(nn.Module):
        def __init__(self, input_dim=3, hidden_dim=128):
            super(Discriminator, self).__init__()
            self.fc1 = nn.Linear(input_dim, hidden_dim)
            self.fc2 = nn.Linear(hidden_dim, hidden_dim)
            self.fc3 = nn.Linear(hidden_dim, hidden_dim)
            self.fc4 = nn.Linear(hidden_dim, 1)
    
        def forward(self, x):
            x = torch.relu(self.fc1(x))
            x = torch.relu(self.fc2(x))
            x = torch.relu(self.fc3(x))
            x = torch.sigmoid(self.fc4(x))
            
            return x
</pre></code><br>

    The comprehensive code, which includes details on the generator, discriminator, data, training process, and more, can be found at the following <a href="">link</a>. 
    Additionally, the training process visualized using Matplotlib can be viewed below.
    Upon examining the loss status graph, it becomes evident that a sphere begins its generation around the 2700-epoch mark. 
    Subsequent to this point, the <code>loss values</code> cease to oscillate and exhibit a <code>convergent</code> graph.

    <figure style="display: flex;">
        <img src="/img/latent-masses/latent-masses-9.gif" style="width: 49%;" onerror=handle_image_error(this)>
        <img src="/img/latent-masses/latent-masses-8.gif" style="width: 49%;" onerror=handle_image_error(this)>
    </figure>
    <figcaption>
        Training process of a single sphere GAN<br>
        From the left, losses status Â· generated point cloud sphere 
    </figcaption><br>
     

</div><br>

<h3>Implementing MassGAN ðŸ§±</h3>
<div class="article">
    From the above, we have gained some understanding of GANs through the implementation of fundamentals and <code>a single sphere GAN</code>. 
    Now, based on this understanding, let's train the model with buildings (Masses) designed by architects and create a generator that produces fake Masses

    <br><br>
    The procedure for the implementation of <code>MassGAN</code> follows the below processes:

    <ul style="padding-left: 2em;">
        <li>Preparation and preprocessing of the dataset</li>
        <li>Implementation of models and training them</li>
        <li>Evaluating generator, and exploration for the latent spaces</li>
    </ul>

</div><br>    

<h3>Preparation and preprocessing of the dataset</h3>
<div class="article">
    I collected building models designed by several famous architects for model training. 
    The figure below shows the actual buildings from the modeling data I gathered.

    <figure style="display: flex;">
        <img src="/img/latent-masses/latent-masses-11.png" style="width: 31%;" onerror=handle_image_error(this)>
        <img src="/img/latent-masses/latent-masses-13.png" style="width: 31%;" onerror=handle_image_error(this)>
        <img src="/img/latent-masses/latent-masses-12.png" style="width: 31%;" onerror=handle_image_error(this)>
    </figure>
    <figcaption>
        Voxel-shaped buildings <br>
        From the left, RED7(MVRDV architects) Â· 79 and Park(BIG architects) Â· Mountain dwelling(BIG architects)  
    </figcaption><br><br>

    The buildings aforementioned possess a common characteristic: their voxel-shaped configuration. 
    As stated above, we learned three modalities of 3D shape representations pertinent to GANs. 
    The primary limitation of the voxel-shaped representation lies in its challenge to articulate high-resolution. 
    However, within the realm of architectural design, this constraint might be reconceived as an opportunity. 
    The voxel-shaped form is prevalently utilized in the architecture field, and there is no imperative demand for high-resolution depictions of such forms.

    <br><br>

    Therefore, we'll create a generative model that generates masses like to the aforementioned using voxel data with appropriate resolutions.
    Firstly, to train models utilizing modeling data, it is imperative to transform the data structure from the <code>.obj</code> format to the more suitable <code>.binvox</code> format.
    The .binvox format delineates data as a binary voxel grid structure, representing <code>True (1)</code> for solid regions and <code>False (0)</code> for vacant spaces.
    Let us look the illustrative example that is preprocessed to the .binvox format below. 
    <br><br>

    <figure>
        <img src="/img/voxelate-2.png" style="width: 90%;" onerror=handle_image_error(this)>
        <figcaption>
            Binary voxel grid representations <br>
            From the left, Given sphere Â· Voxelated sphere Â· Binary voxel grid(9th voxels grid) <br>
            These were aforementioned in the part of my postings titled <a href="https://parkcheolhee-lab.github.io/voxelate/#d-grid--conditions-map">Voxelate</a>
        </figcaption>
    </figure><br>

    As stated above in the binary voxel grid, one can observe the vacant regions are represented by 0s, while the solid regions are denoted by 1s. 
    All detailed code of preprocessing to the .binvox format is showing in the following <a href="https://github.com/PARKCHEOLHEE-lab/gan-exercise/blob/main/massGAN/utils.py#L328">link</a> 
    and I preprocessed to possess <code>32 x 32 x 32 resolution</code> for 6 models below utilizing it.

    <br><br>
    <p style="font-size: smaller;">
        (Initially, I had curated 24 models for the training MassGAN. However, due to the limited computational efficiency of my RTX 3060 laptop graphics card, 
        which resulted in protracted learning durations, I decided to reduce the dataset size. ðŸ˜­)
    </p>

    <figure>
        <img src="/img/latent-masses/latent-masses-14.png" style="width: 90%;" onerror=handle_image_error(this)>
        <img src="/img/latent-masses/latent-masses-15.png" style="width: 90%;" onerror=handle_image_error(this)>
        <br>
        <figcaption>
            Preprocessed data to the binary voxel grid utilizing <a href="https://www.patrickmin.com/binvox/">binvox</a><br>
            From the top left, 79 and Park Â· Lego tower, RED7 <br>
            From the bottom left, Vancouver house Â· CCTV Headquarter, Mountain dwelling
        </figcaption>
    </figure>
    
</div><br>    

<h3>Implementation of models and training them</h3>
<div class="article">
    We have now completed the procedures for data collection and preprocessing. 
    Subsequently, we are now poised to commence the implementation of both the generator and the discriminator.

    <br><br>
    
    Therefore I implemented DCGAN with a gradient penalty(WGAN) by referring to the GitHub repositories where several 3D generation models are implemented as follows.
    The comprehensive code delineating the model definitions can be accessed at the following link: 
    <a href="https://github.com/PARKCHEOLHEE-lab/gan-exercise/blob/main/massGAN/model.py#L67-L137">massGAN/model.py</a>

<pre><code class="python">
    class Generator(nn.Module, Config):
        def __init__(self, z_dim, init_out_channels: int = None):
            super().__init__()
            
            out_channels_0 = self.GENERATOR_INIT_OUT_CHANNELS if init_out_channels is None else init_out_channels
            out_channels_1 = int(out_channels_0 / 2)
            out_channels_2 = int(out_channels_1 / 2)

            self.main = nn.Sequential(
                nn.ConvTranspose3d(z_dim, out_channels_0, kernel_size=4, stride=1, padding=0, bias=False),
                nn.BatchNorm3d(out_channels_0),
                nn.ReLU(True),
                nn.ConvTranspose3d(out_channels_0, out_channels_1, kernel_size=4, stride=2, padding=1, bias=False),
                nn.BatchNorm3d(out_channels_1),
                nn.ReLU(True),
                nn.ConvTranspose3d(out_channels_1, out_channels_2, kernel_size=4, stride=2, padding=1, bias=False),
                nn.BatchNorm3d(out_channels_2),
                nn.ReLU(True),
                nn.ConvTranspose3d(out_channels_2, 1, kernel_size=4, stride=2, padding=1, bias=False),
                nn.Sigmoid()
            )
            
            self.to(self.DEVICE)
            
        def forward(self, x):
            return self.main(x)
        
        
    class Discriminator(nn.Module, Config):
        def __init__(self, init_out_channels: int = None):
            super().__init__()
            
            out_channels_0 = self.DISCRIMINATOR_INIT_OUT_CHANNELS if init_out_channels is None else init_out_channels
            out_channels_1 = out_channels_0 * 2
            out_channels_2 = out_channels_1 * 2

            self.main = nn.Sequential(
                nn.Conv3d(1, out_channels_0, kernel_size=4, stride=2, padding=1, bias=False),
                nn.LeakyReLU(0.2, inplace=True),
                nn.Conv3d(out_channels_0, out_channels_1, kernel_size=4, stride=2, padding=1, bias=False),
                nn.BatchNorm3d(out_channels_1),
                nn.LeakyReLU(0.2, inplace=True),
                nn.Conv3d(out_channels_1, out_channels_2, kernel_size=4, stride=2, padding=1, bias=False),
                nn.BatchNorm3d(out_channels_2),
                nn.LeakyReLU(0.2, inplace=True),
                nn.Conv3d(out_channels_2, 1, kernel_size=4, stride=1, padding=0, bias=False),
                nn.Sigmoid()
            )
            
            self.to(self.DEVICE)
            
        def forward(self, x):
            return self.main(x).view(-1, 1).squeeze(1)
</code></pre><br>

    
    We further defined the <a href="https://github.com/PARKCHEOLHEE-lab/gan-exercise/blob/main/massGAN/model.py#L140-L490">MassganTrainer</a> for model supervision, including model training, evaluation, and storage. 
    Throughout this process, I monitored any issues that occurred during the training phase. 
    The recorded outcomes are presented below:

    <figure>
        <img src="/img/latent-masses/latent-masses-16.png" style="width: 85%;" onerror=handle_image_error(this)>
        <img src="/img/latent-masses/latent-masses-0.gif" onerror=handle_image_error(this)>
        <figcaption>
            Visualized training process at each 200 epochs from 0 to 20000 <br>
            From the top, losses status Â· generated masses when training model
        </figcaption>
    </figure><br>

    Contrary to the <code>a single sphere GAN</code> that we previously trained, <code>MassGAN</code> does not exhibit a loss value converging to a singular point due to the complexity of the data.
    Neverthelesee, if you compare the early and final stages of learning, you can observe that the loss value <code>oscillates within a low range</code>.
    Furthermore, by observing the monitored fake masses, one can discern that they progressively approximate the shapes of real masses.
</div><br>    

<h3>Evaluating generator, and exploration for the latent spaces</h3>
<div class="article">

    The <a href="https://github.com/PARKCHEOLHEE-lab/gan-exercise/blob/main/massGAN/config.py#L7">parameters for model training</a>, such as learning rate, batch size, noise dimension, and so forth, were used as follows:

<pre><code class="python">
    class ModelConfig:
        """Configuration related to the GAN models
        """

        DEVICE = "cpu"
        if torch.cuda.is_available():
            DEVICE = "cuda"
            
        SEED = 777
        
        GENERATOR_INIT_OUT_CHANNELS = 256
        DISCRIMINATOR_INIT_OUT_CHANNELS = 64
        
        EPOCHS = 20000
        LEARNING_RATE = 0.0001
        BATCH_SIZE = 6
        BATCH_SIZE_TO_EVALUATE = 6
        Z_DIM = 128
        BETAS = (0.5, 0.999)
        
        LAMBDA_1 = 10
        
        LOG_INTERVAL = 200
</code></pre><br>

    Now, let's load and evaluate the model trained with the corresponding <code>ModelConfig</code>. 
    In GAN, it is important to evaluate the model quantitatively as the status of loss, 
    but <code>qualitatively</code> evaluating the data generated by the <code>Generator</code> is also effective in evaluating the model.
    The following figures are generated masses by MassGAN model through the utilization of the <a href="https://github.com/PARKCHEOLHEE-lab/gan-exercise/blob/main/massGAN/model.py#L386C5-L386C17"><code>evaluate</code> function</a>.
    <figure>
        <img src="/img/latent-masses/latent-masses-20.png" style="width: 80%;" onerror=handle_image_error(this)>
        <img src="/img/latent-masses/latent-masses-17.png" style="width: 80%;" onerror=handle_image_error(this)>
        <img src="/img/latent-masses/latent-masses-18.png" style="width: 80%;" onerror=handle_image_error(this)>
        <img src="/img/latent-masses/latent-masses-19.png" style="width: 80%;" onerror=handle_image_error(this)>
        <img src="/img/latent-masses/latent-masses-21.png" style="width: 80%;" onerror=handle_image_error(this)>
        <img src="/img/latent-masses/latent-masses-22.png" style="width: 80%;" onerror=handle_image_error(this)>
        <br>
        <figcaption>Generated masses by MassGAN model</figcaption>
    </figure><br>
    
    All in all, it appears to produce decent data. Subsequently, let's select some of the masses created by the generator and observe the <code>interpolation</code> of latent mass shapes between them.
    
    <figure style="display: flex;">
        <img src="/img/latent-masses/latent-masses-24.png" style="width: 33%;" onerror=handle_image_error(this)>
        <img src="/img/latent-masses/latent-masses-25.gif" style="width: 33%;" onerror=handle_image_error(this)>
        <img src="/img/latent-masses/latent-masses-26.png" style="width: 33%;" onerror=handle_image_error(this)>
    </figure>
    <figcaption>
        Interpolation in latent space <br>
        From the left, RED7 Â· interpolating Â· CCTV Headquarter
    </figcaption>
    
    <figure style="display: flex;">
        <img src="/img/latent-masses/latent-masses-27.png" style="width: 33%;" onerror=handle_image_error(this)>
        <img src="/img/latent-masses/latent-masses-29.gif" style="width: 33%;" onerror=handle_image_error(this)>
        <img src="/img/latent-masses/latent-masses-28.png" style="width: 33%;" onerror=handle_image_error(this)>
    </figure>
    <figcaption>
        Interpolation in latent space <br>
        From the left, Lego tower Â· interpolating Â· Mountain dwelling
    </figcaption>

    <figure style="display: flex;">
        <img src="/img/latent-masses/latent-masses-30.png" style="width: 33%;" onerror=handle_image_error(this)>
        <img src="/img/latent-masses/latent-masses-32.gif" style="width: 33%;" onerror=handle_image_error(this)>
        <img src="/img/latent-masses/latent-masses-31.png" style="width: 33%;" onerror=handle_image_error(this)>
    </figure>
    <figcaption>
        Interpolation in latent space <br>
        From the left, Vancouver house Â· interpolating Â· Lego tower
    </figcaption><br>

</div><br>    

<h3>References</h3>
<div class="article">
    <ul>
        <li><a href="https://medium.com/hackernoon/latent-space-visualization-deep-learning-bits-2-bd09a46920df">https://medium.com/hackernoon/latent-space-visualization-deep-learning-bits-2-bd09a46920df</a></li>
        <li><a href="https://github.com/ChrisWu1997/SingleShapeGen">https://github.com/ChrisWu1997/SingleShapeGen</a></li>
        <li><a href="https://github.com/znxlwm/pytorch-MNIST-CelebA-GAN-DCGAN">https://github.com/znxlwm/pytorch-MNIST-CelebA-GAN-DCGAN</a></li>
        <li><a href="https://developer-ping9.tistory.com/108">https://developer-ping9.tistory.com/108</a></li>
    </ul>
    
</div><br><br>
