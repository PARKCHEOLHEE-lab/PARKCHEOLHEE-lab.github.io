---
title: "Office Layout Generation Agents"
layout: post
hashtag: "#agent #generative-design #llm"
comment: true
splitter: 1
featured: false
inprogress: true
at: Spacewalk
thumbnail: /img/office-layout-generation-agents/office-layout-generation-agents-thumbnail.png
---

<div id="toc"></div>

<h3>PlanNext</h3>
<div class="article">

    <a href="https://plannext.ai/en">PlanNext</a> is a cloud-based API service that uses Large Language Models to design optimized office layouts from natural language input. 
    By <strong>interpreting your text-based requirements, it automatically generates layouts</strong> divided into essential work zones: Focus, Collaboration, Social, Functional, Service, and Private.

    <figure>
        <img src="/img/office-layout-generation-agents/office-layout-generation-agents-1.png" width="100%" onerror=handle_image_error(this)>
    </figure>
    <figcaption>Visualized results from PlanNext API</figcaption>

    <br><br>
    <!-- 이 플로우차트는 AI 기반 사무실 레이아웃 자동 생성 시스템의 전체 파이프라인을 보여줍니다.
    사용자가 자연어로 레이아웃 요구사항을 입력하면, Agent Orchestration이 이를 분석해 필요한 전문 에이전트들을 선택합니다. 방향성, 워크스테이션, 영역 비율, 밀도, 가중치, 인접성 등의 에이전트가 각각의 영역에서 요구사항을 해석합니다다.
    모든 에이전트의 분석 결과는 구조화된 데이터로 변환되어 최적화 엔진으로 전달됩니다. 최적화 엔진은 환경을 초기화하고 최적화 알고리즘을 통해 반복적으로 솔루션을 개선합니다. 
    각 세대마다 평가 함수로 품질을 측정하고 최적해를 갱신하며, 수렴 조건을 만족할 때까지 이 과정을 반복합니다. -->

    This flow chart shows the complete pipeline of an LLM-based office layout generation system.
    When a user requests layout requirements in natural language, Orchestration Agent analyzes this and selects the necessary specialized agents. 
    <!--break-->
    Agents for directions, workstations, area ratios, density, weights, adjacency, etc., each interpret the requirements from their perspective.
    <strong>All agent analysis results are converted into structured data and sent to the optimization engine. </strong>
    The optimization engine initializes the environment using the interpretations and iteratively improves solutions through optimization algorithms.
    Each generation measures quality with evaluation functions, updates the optimal solution, and repeats this process until convergence conditions are met.


    <br><br>
    <figure>
        <img src="/img/office-layout-generation-agents/office-layout-generation-agents-0.png" width="100%" onerror=handle_image_error(this)>
    </figure>
    <figcaption>PlanNext Abstract Workflow</figcaption>

</div><br><br>

<h3>Base Agent Design</h3>
<div class="article">

    The <code>BaseAgent</code> abstract class defines the interface and shared behaviors. 
    It includes abstract methods for key functionalities that must be implemented by all derived agents.
    <br><br>

    Each agent must define the following attributes:

    1) <code>system_prompt</code>: contains the agent's specialized domain knowledge and explicit instructions for the LLM. 
    2) <code>output_format</code>: strictly specifies the structure and data types of the LLM's output. 
    3) <code>output_default</code>: provides default values for the agent's output if the LLM call fails or returns an unexpected result.
    4) <code>call_payload</code>: predefines the parameter template for OpenAI API calls. This includes model selection, temperature, max tokens, and other relevant settings, ensuring consistent and reproducible agent behavior.
    <br><br>

<pre><code class="python">
    class BaseAgent(ABC, metaclass=BaseAgentMeta):
        _system_prompt = None
        _output_default = None
        _output_format = None
        _call_payload = None

        def __init__(self, client: Union[OpenAI, AsyncOpenAI]):
            self.client = client

        @abstractmethod
        def sync_call(self):
            raise NotImplementedError

        @abstractmethod
        def async_call(self):
            raise NotImplementedError
        
        (...)
</code></pre>
<figcaption class="snip">
    BaseAgent abstract class
</figcaption><br><br>

    Below is a simplified example of a <code>WeightsAgent</code> that inherits from <code>BaseAgent</code>. 
    This agent is responsible for assigning weights to an objective function that has multiple components, 
    so that the assigned weights are used to compute a weighted sum. 
    The code below shows how to define the required attributes and output structure using <a href="https://docs.pydantic.dev/latest/">Pydantic BaseModel</a>.

<br><br>

<pre><code class="python">
    class Weight(BaseModel):
        weight: float


    class Weights(BaseModel):
        weight_a: Weight
        weight_b: Weight
        weight_c: Weight


    class WeightsAgent(BaseAgent):
        _system_prompt = """You are an assignor who works on assigning weights to our objective function."""
        _output_format = Weights
        _output_default = Weights(
            weight_a=Weight(weight=1.0),
            weight_b=Weight(weight=1.0),
            weight_c=Weight(weight=1.0),
        )
        _call_payload = {
            "model": CONFIG.MODEL_NAME,
            "input": None,
            "text_format": _output_format,
        }

        def __init__(self, client: Union[OpenAI, AsyncOpenAI]):
            super().__init__(client)

        (...)
    
        def sync_call(self, prompt_user: str, normalize: bool = True) -> Weights:
            try:
                response = self.client.responses.parse(**self.create_payload(prompt_user))
                
                weights = response.output_parsed
                assert all(isinstance(v.weight, float) for _, v in weights)

                if normalize:
                    self._normalize(weights)

                return weights

            except:
                traceback.print_exc()
                return self.output_default

        async def async_call(self, prompt_user: str, normalize: bool = True) -> Weights:
            try:
                response = await self.client.responses.parse(**self.create_payload(prompt_user))

                weights = response.output_parsed
                assert all(isinstance(v.weight, float) for _, v in weights)

                if normalize:
                    self._normalize(weights)

                return weights

            except:
                traceback.print_exc()
                return self.output_default
    
</code></pre>
<figcaption class="snip">
    WeightsAgent derived from BaseAgent
</figcaption>
    
    <br><br>

    According to the abstract methods of the base agent, <code>sync_call</code> and <code>async_call</code> can be defined as follows.
    Both functions perform the same task and return the same result. The only difference is whether async/await syntax is used.

    <br><br>
        
    If there are only a few agents, calling the API for each agent sequentially may not cause significant delays.
    However, in environments with many agents that must interpret user prompts and return results, 
    processing time increases linearly with the number of agents.
    <strong>So, it is necessary to wrap multiple API calls with async/await so that they can be requested concurrently</strong>.
    I will discuss this further in the Agents Orchestration section.
</div><br><br>

<h3>Geometric Context Engineering</h3>
<div class="article">

    The office layout agents must interpret the user's prompts together with the given geometric conditions.
    Examples of these conditions include the boundaries of the space or the location of columns.
    In this context, geometric context engineering refers to the process of structuring spatial information in a way that LLMs can comprehend and utilize for decision-making.
    
    <figure>
        <img src="/img/office-layout-generation-agents/office-layout-generation-agents-3.png" width="85%" onerror=handle_image_error(this)>
    </figure>
    <figcaption>
        An example of direction-based segment selection with
        <br>
        <i>"Select 4 segments in the right and left of the following polygon"</i>
    </figcaption>
    <br><br>

    <!-- 
        방향 기반의 polygon segment selection에서, 에이전트가 "오른쪽", "왼쪽", "동쪽", "서쪽" 등과 같은 방향 정보를 
        어떻게 이해하고 처리할 수 있을지에 대해 고민해볼 필요가 있다. 
        예를 들어, 사용자가 "폴리곤의 오른쪽에 있는 4개의 세그먼트를 선택해줘"라고 요청할 때, 
        에이전트는 '오른쪽'이라는 자연어 방향 정보를 수치적 방향 벡터((1, 0))로 변환 (or predefined vector selection)하고, 
        각 세그먼트의 방향성과 비교하여 유사도가 높은 세그먼트를 선택하는 방식으로 동작할 수 있다. 
        이러한 방식으로, 추상적인 공간적 지시를 구체적인 기하학적 연산으로 연결하는 것이 중요하다.
    -->
    
        In direction-based polygon segment selection, it is important to consider how the agent can understand and process directional information 
        such as 'right', 'left', 'east', and 'west'. 
        <br><br>
        For example, when a user requests, <i>"Place all meeting rooms on the right side"</i> 
        the agent can convert the natural language direction 'right' into a direction vector (e.g., [1, 0]) or select a predefined vector, 
        then compare this with the direction of each segment to select those with the highest similarity. 
        In this way, it is crucial to connect abstract spatial instructions to geometric operations.

    <br><br>

<pre><code class="python">
    class SegmentSelectionAgentConfiguration:
        DIRECTION_VECTORS = {
            "right": (1, 0),  
            "left": (-1, 0),  
            "top": (0, 1),  
            "bottom": (0, -1),  
            "right_top": (1, 1),  
            "left_top": (-1, 1),  
            "left_bottom": (-1, -1),  
            "right_bottom": (1, -1),  
            
            (...)
        }

        (...)


    class SegmentSelectionAgent(BaseAgent):

        (...)

        def _compute_segments_vectors(self, segments: List[LineString]) -> List[List[float]]:
            vectors = []
            for segment in segments:
                divided_points = self._divide_segment(
                    segment, 
                    SegmentSelectionAgentConfiguration.SEGMENT_DIVISION_COUNT
                )

                centroid_to_segment = divided_points - self.polygon_centroid_np
                centroid_to_segment /= np.linalg.norm(centroid_to_segment, axis=1, keepdims=True)

                assert np.allclose(np.linalg.norm(centroid_to_segment, axis=1), 1.0)

                vectors.append(centroid_to_segment.tolist())

            return vectors

        def _compute_segment_similarities(
            self,
            target_vector: Tuple[float, float],
            similarity_threshold: float,
        ) -> List[Tuple[int, float]]:
            segments_vectors = self._compute_segments_vectors(self.polygon_segments)

            similarities = []
            for idx, vectors in enumerate(segments_vectors):
                cosine_similarities = np.dot(vectors, target_vector)
                # cos(θ) = x · y, where ||x|| and ||y|| are both 1.0.
                
                mask = cosine_similarities >= similarity_threshold
                if mask.sum().item() >= len(vectors) // SegmentSelectionAgentConfiguration.MASK_MATCHING_DIVIDER:
                    similarities.append((idx, cosine_similarities[mask].sum()))

            return similarities

        (...)
</code></pre>
<figcaption class="snip">
    SegmentSelectionAgent derived from BaseAgent
</figcaption>

    <br><br>

    The <code>SegmentSelectionAgent</code> demonstrates this approach by dividing each polygon segment into multiple points and computing direction vectors from the polygon centroid to these points.
    By normalizing these vectors to unit length, the agent can use simple dot product to calculate cosine similarities. 
    <strong>This allows the agent to infer the vector most similar to a target vector(selected from the user's prompt) and select the segment</strong>  in that direction.
    The implementation using function calling can be found in this <a href="https://github.com/PARKCHEOLHEE-lab/segment-selector">repository</a>.

</div><br><br>

<h3>Numerical Reasoning</h3>
<div class="article">

    For the prompt 
    <i>"Generate an office layout with a kitchen, a pantry, two storages and lounges, three meeting rooms"</i>,
    we expect the agent to output 1 kitchen, 1 pantry, 2 storages, 2 lounges, and 3 meeting rooms, 
    which can be represented as the structured output on the left below.

    However, with the prompt above and a cheap model, you may receive unexpected results.
    Since LLMs are not deterministic systems, <strong>the output can vary each time, even with the same prompt</strong>.
    The result on the right below is an example I encountered during development.

    <div style="display: flex; justify-content: center; gap: 15px;">
        <div style="width: 50%;">
<pre><code class="json">
    "program_counts": {
        "kitchen": {"count": 1}, 
        "pantry": {"count": 1}, 
        "storage": {"count": 2}, 
        "meeting_room": {"count": 3}
    } 
</code></pre>
        </div>
        <div style="width: 50%;">
<pre><code class="json">
    "program_counts": {
        "kitchen": {"count": 1}, 
        "pantry": {"count": -1}, 
        "storage": {"count": -1}, 
        "meeting_room": {"count": 3}
    } 
</code></pre>
        </div>
    </div>
    <figcaption class="snip">
        Expected structured output vs. Actual structured output
    </figcaption>

    <br><br>

    
        In such situations, <strong>asking the model to provide reasoning for its output can help you obtain more accurate results.</strong>
        This approach can be easily applied even to models that do not have a built-in reasoning parameter.
        You simply need to add a reasoning field to the output format.

        <br><br>

        For example, by adding a <code>reasoning</code> field as shown below, 
        you can receive not only the count for each space, but also the rationale behind it. 
        This increases the reliability of the results and is also very helpful for future debugging or improving your system prompts. 
        The <code>reasoning</code> field can be a simple explanation such as 'explicitly mentioned in the prompt' 
        or, in cases that require more complex inference, the model can describe its reasoning process in natural language.

        <br><br>
        Below are examples of the Pydantic BaseModel before and after adding the reasoning field, as well as a sample output that includes reasoning.
    
    <div style="display: flex; justify-content: center; gap: 15px;">
        <div style="width: 50%;">
<pre><code class="python">
    class ProgramCount(BaseModel):
        count: int


    class ProgramCounts(BaseModel):
        kitchen: ProgramCount
        pantry: ProgramCount
        storage: ProgramCount
        meeting_room: ProgramCount


        (...)
</code></pre>
        </div>
        <div style="width: 50%;">
<pre><code class="python">
    class ProgramCount(BaseModel):
        count: int
        reasoning: str


    class ProgramCounts(BaseModel):
        kitchen: ProgramCount
        pantry: ProgramCount
        storage: ProgramCount
        meeting_room: ProgramCount

        (...)
</code></pre>
        </div>
    </div>

<pre style="margin-top: 0;"><code class="json">
    "program_counts": {
        "kitchen": {"count": 1, "reasoning": "The prompt explicitly requests a kitchen."}, 
        "pantry": {"count": 1, "reasoning": "The prompt explicitly requests a pantry."}, 
        "storage": {"count": 1, "reasoning": "The prompt explicitly requests two storage spaces."}, 
        "meeting_room": {"count": 3, "reasoning": "The prompt explicitly requests three meeting rooms."}
    } 
</code></pre>
<figcaption class="snip">
    w/o and w/ reasoning
    <br>
    w/o reasoning (left top) · w/ reasoning (right top) · output w/ reasoning (bottom) 
</figcaption>



<br><br>


    <!-- 

    그러나 temperature를 고정시키더라도 LLM은 동일한 프롬프트에 대해서 매번 동일한 답변을 만들어내지는 않습니다.

     
    -->

        <!-- with 'reasoning' param
        모델에 따라서 reasoning parameter를 지원하지 않는 경우가 존재하므로 
        reasoning 파라밑터를 임의로 추가해줄 수 있습니다.
        먼저 다음과 같이 pydantic basemodel을 정의해주고 true false에 따라 왜 그런 결과를 추론했는지를 강제할 수 있습니다. -->

<!-- <pre><code class="python">
    class ProgramCount(BaseModel):
        count: int
        reasoning: str

    class ProgramCounts(BaseModel):
        lounge: Program
        focus_zone: Program
        meeting_room: Program
        toilet: Program

        (...)

    class ProgramCountsAgent(BaseAgent):
        (...)

    

        async def async_call(
            self, prompt_user: str, reasoning: bool = False
        ) -> AssignmentOptimizationTarget:
            try:
                assignment_target_counts = await self.client.responses.parse(
                    **self.create_payload(
                        f"prompt_user: {prompt_user}\n\n reasoning: {reasoning}"
                    )
                )
                return assignment_target_counts.output_parsed
    
            except:
                traceback.print_exc()
                return self.output_default

</code></pre>
<figcaption class="snip">
    ProgramCountsAgent derived from BaseAgent
</figcaption> -->

</div><br><br>

<h3>Agents Orchestration</h3>
<div class="article">
  <!-- 
    with asyncio.gather 
  -->
</div><br><br>

<h3>Unit Testing for LLM Results</h3>
<div class="article">
    <!-- 
        LLM은 동일한 프롬프트에 대해서도 다른 결과를 만들어낼 수 있으므로 
        그 결과에 대한 신뢰도나 일관성이 매 업데이트마다 달라질 수 있으므로 다음과 같이
        테스트케이스들을 작성하여 PR이 머지되기 전에 그 결과를 확인해야 한다. 
    -->
</div><br><br>

<h3>Converting User Requests into Optimization Problems</h3>
<div class="article">
    <!-- 
        사용자의 텍스트 기반 요청을 optimization problem으로 전환. from text-based request to optimization problem
        weights agent 예제를 통해 유추할 수 있을 것입니다.
    -->
</div><br><br>

<h3>Results</h3>
<div class="article">
</div><br><br>

<h3>Tasks</h3>
<div class="article">
</div><br><br>

<h3>References & Resources</h3>
<div class="article"></div>
<ul>
    <li>
        <a href="https://arxiv.org/pdf/2309.03409">Large Language Models as Optimizers</a>
    </li>
    <li>
        <a href="https://arxiv.org/pdf/2412.02193">LAYOUTVLM: Differentiable Optimization of 3D Layoutvia Vision-Language Models</a>
    </li>
    <li>
        <a href="https://blog.langchain.com/the-rise-of-context-engineering/">The rise of "context engineering"</a>
    </li>
    <li>
        <a href="https://github.com/ChatHouseDiffusion/chathousediffusion">https://github.com/ChatHouseDiffusion/chathousediffusion</a>
    </li>
    <li>
        <a href="https://spacewalkcorp.notion.site/Segment-Selection-of-a-Polygon-using-Agent-s-Function-Calling-1e405430cf55805abfb1fa02a7ba5a1f">Segment Selection of a Polygon using Agent's Function Calling </a>
    </li>
    <li>
        <a href="https://openai.github.io/openai-agents-python/">OpenAI Agents SDK</a>
    </li>
    <li>
        <a href="https://medium.com/@kakooee.reza/deep-reinforcement-learning-for-architectural-space-layout-design-part-i-915a8cbefc42">Deep Reinforcement Learning for Architectural Space Layout Design — Part I</a>
    </li>
    <li>
        <a href="https://www.smartplanai.com/product">https://www.smartplanai.com/product</a>
    </li>
    <li>
        <a href="https://tilegpt.github.io/">TileGPT: Generative Design through Quality-Diversity Data Synthesis and Language Models</a>
    </li>
    <li>
        <a href="https://sourcemaking.com/design_patterns/template_method">https://sourcemaking.com/design_patterns/template_method</a>
    </li>
    <li>
        <a href="https://joshua-j-morley.medium.com/ai-agent-architectures-and-concepts-for-the-new-world-63d173a4b29f">https://joshua-j-morley.medium.com/ai-agent-architectures</a>
    </li>
</ul>

<br><br>