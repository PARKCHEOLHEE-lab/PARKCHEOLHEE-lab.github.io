---
title: "Latent masses"
layout: post
hashtag: "#deep-learning #generative-adversarial-networks #machine-learning #gan"
featured: true
comment: true
splitter: 2
thumbnail: /img/latent-masses-thumbnail.gif
---

<div id="toc"></div>
<h3>Objectives</h3>
<div class="article">
    Generative Adversarial Networks (GANs) have paved the way for unprecedented advancements in numerous areas, from art creation to deepfake video generation. 
    However, the potential of GANs isn't restricted to 2D space. The development and application of 3D GANs have opened new possibilities, especially in the realm of design.

    <br><br>

    This project delves deep into the <code>possibilities of 3D GANs in the design field</code> with the following objectives:
    <ul style="padding-left: 2em;">
        <li>Grasp the fundamental concepts behind GANs and their 3D extension</li>
        <li>Appreciate the power and nuances of 3D GANs through hands-on experiments</li>
        <li>Examine how 3D GANs can be harnessed for product design, architectural modeling, and virtual environment creation</li>
        <li>visualize and manipulate the <a href="https://medium.com/hackernoon/latent-space-visualization-deep-learning-bits-2-bd09a46920df">latent space</a> to generate novel and innovative designs</li>
        <li>Understand the limitations of current 3D GAN models and the potential areas of improvement</li>
    </ul><br>
    <!--break-->

    <figure>
        <img src="/img/latent-masses-7.png" style="width: 70%;"><br>
        <figcaption>Interpolation in <a href="https://hsejun07.tistory.com/100">latent space</a></figcaption>
    </figure></div>

</div><br>

<h3>3D shape representations for the generative adversarial networks</h3>
<div class="article">
    
        1. Point cloud
        <ul style="padding-left: 2em;">
            <li>
                A point cloud is a set of data points in space. 
                In 3D shape representation, point clouds are typically used to <code>represent the external surface</code> of an object
                Each point in the point cloud has an (x, y, z) coordinate
            </li>
            <li>Can represent any 3D shape without being limited to a specific topology or grid (Good at flexibility)</li>
            <li>Points are disconnected, so additional processing is often required to extract surfaces or other features (Not good at lack of connectivity)</li>
        </ul><br>
        2. Voxel
        <ul style="padding-left: 2em;">
            <li>
                Voxels (short for volumetric pixels) are the 3D equivalent of 2D pixels. 
                A voxel representation <code>divides the 3D space into a regular grid, and each cell</code> (or voxel) in the grid can be either occupied or empty
            </li>
            <li>Operations like convolution are straightforward to apply on voxel grids (Simplicity)</li>
            <li>To represent fine details, a very high-resolution grid is needed, which can be computationally prohibitive (Limited resolution)</li>
        </ul><br>
        3. Mesh
        <ul style="padding-left: 2em;">
            <li>
                A 3D mesh consists of vertices, edges, and faces that define the shape of a 3D object in space. 
                The most common type of mesh is a triangular mesh, where the shape is <code>represented using triangles</code>
            </li>
            <li>Can represent both simple and complex geometries (Good expressiveness)</li>
            <li>Provides information about how points are connected, which is useful for many applications (Good at continuous surface representation)</li>
            <li>Operations on meshes, like subdivision or simplification, can be computationally demanding (Complexity)</li>
        </ul><br>

        <figure>
            <img src="/img/latent-masses-1.png" style="width: 80%;"><br>
            <figcaption>
                <a href="https://medium.com/@phamtdong0406/1-3d-ai-data-representation-99c98ec402c6">Shape representations</a><br>
                From the left, point cloud · mesh · voxel
            </figcaption>
        </figure>
</div><br>

<h3>Simple implementation: A single sphere GAN</h3>
<div class="article">
    First, I'll implement a practical application of training a GAN on point cloud data, aiming to generate a single sphere, represented by point cloud
    Before implementing the neural networks, we begin by loading our target sphere point cloud from a file. I modeled just one sphere shape using Rhino.

    <br><br>
    Typically, the normalization can be particularly beneficial if your training data consists of similar objects in various sizes or if the absolute size isn't critical for your task.
    In our dataset concerning a single sphere, the absolute size is not of significance. Therefore, let's normalize it. The sphere can be normalized easily using <code>numpy</code> as follows:

<pre><code class="python">
    class Normalize:
        def __call__(self, pointcloud):
            assert len(pointcloud.shape) == 2
            
            norm_pointcloud = pointcloud - np.mean(pointcloud, axis=0) 
            norm_pointcloud /= np.max(np.linalg.norm(norm_pointcloud, axis=1))
            
            return norm_pointcloud
</pre></code><br>

    If different 3D models have a different number of vertices, sampling a consistent number of points from each model <code>ensures that the input size</code> remains uniform. This is crucial when feeding data to neural networks that expect consistent input sizes.
    Please refer to the following <a href="https://github.com/PARKCHEOLHEE-lab/gan-exercise/blob/main/sphereGAN/utils.py#L9-L54">link</a> for the code related to PointSampler.

    <figure style="display: flex;">
        <img src="/img/latent-masses-2.png" style="width: 32%;"><br>
        <img src="/img/latent-masses-5.png" style="width: 32%;"><br>
        <img src="/img/latent-masses-6.png" style="width: 32%;"><br>
    </figure>
    <figcaption>
        A sphere, represented by point cloud <br>
        From the left, original sphere · random sampled sphere · normalized and random sampled sphere
    </figcaption><br><br>

    Now, we have completed the data preprocessing and it is now ready for model training. 
    Let us establish and train models comprising a simple generator and discriminator as follows:

<pre><code class="python">
    class Generator(nn.Module):
        def __init__(self, input_dim=3, output_dim=3, hidden_dim=128):
            super(Generator, self).__init__()
            self.fc1 = nn.Linear(input_dim, hidden_dim)
            self.fc2 = nn.Linear(hidden_dim, hidden_dim)
            self.fc3 = nn.Linear(hidden_dim, hidden_dim)
            self.fc4 = nn.Linear(hidden_dim, output_dim)
    
        def forward(self, x):
            x = torch.relu(self.fc1(x))
            x = torch.relu(self.fc2(x))
            x = torch.relu(self.fc3(x))
            x = torch.tanh(self.fc4(x))
            
            return x
    
    class Discriminator(nn.Module):
        def __init__(self, input_dim=3, hidden_dim=128):
            super(Discriminator, self).__init__()
            self.fc1 = nn.Linear(input_dim, hidden_dim)
            self.fc2 = nn.Linear(hidden_dim, hidden_dim)
            self.fc3 = nn.Linear(hidden_dim, hidden_dim)
            self.fc4 = nn.Linear(hidden_dim, 1)
    
        def forward(self, x):
            x = torch.relu(self.fc1(x))
            x = torch.relu(self.fc2(x))
            x = torch.relu(self.fc3(x))
            x = torch.sigmoid(self.fc4(x))
            
            return x
</pre></code><br>

    The comprehensive code, which includes details on the generator, discriminator, data, training process, and more, can be found at the following <a href="">link</a>. 
    Additionally, the training process visualized using Matplotlib can be viewed below.

    <figure style="display: flex;">
        <img src="/img/latent-masses-9.gif" style="width: 45%;"><br>
        <img src="/img/latent-masses-8.gif" style="width: 45%;"><br>
    </figure>
    <figcaption>
        Training process of a single sphere GAN<br>
        From the left, losses status · generated point cloud sphere 
    </figcaption><br><br>


</div><br>

<h3></h3>
<div class="article">

</div>    

<h3>References</h3>
<div class="article">
    <ul>
        <li><a href="https://medium.com/hackernoon/latent-space-visualization-deep-learning-bits-2-bd09a46920df">https://medium.com/hackernoon/latent-space-visualization-deep-learning-bits-2-bd09a46920df</a></li>
    </ul>
    
    <!-- <figure>
        <img src="/img/latent-masses-0.gif" width="100%">
        <figcaption>Visualized training process at each 200 epochs</figcaption>
    </figure> -->
</div>


<!-- 

    3. Implementing MassGAN
        - Use voxel representation
            . the voxel is appropriate in the architectural field. because it has no deadspace ...
        
        - Preparing data that is shaped mesh and converting them to binary voxel grid
        - training
            . training process per 100 epoch -> .gif

    4. Exploring latent masses between multiple masses generated
        - interpolating process -> .gif

    5. References
        - SingleShapeGan
        - Other articles related to this post
        
 -->