---
title: "Latent masses"
layout: post
hashtag: "#deep-learning #generative-adversarial-networks #machine-learning #gan"
featured: true
comment: true
splitter: 2
thumbnail: /img/latent-masses-thumbnail.gif
---

<div id="toc"></div>
<h3>Objectives</h3>
<div class="article">
    Generative Adversarial Networks (GANs) have paved the way for unprecedented advancements in numerous areas, from art creation to deepfake video generation. 
    However, the potential of GANs isn't restricted to 2D space. The development and application of 3D GANs have opened new possibilities, especially in the realm of design.

    <br><br>

    This project delves deep into the <code>possibilities of 3D GANs in the design field</code> with the following objectives:
    <ul style="padding-left: 2em;">
        <li>Grasp the fundamental concepts behind GANs and their 3D extension</li>
        <li>Appreciate the power and nuances of 3D GANs through hands-on experiments</li>
        <li>Examine how 3D GANs can be harnessed for product design, architectural modeling, and virtual environment creation</li>
        <li>visualize and manipulate the <a href="https://medium.com/hackernoon/latent-space-visualization-deep-learning-bits-2-bd09a46920df">latent space</a> to generate novel and innovative designs</li>
        <li>Understand the limitations of current 3D GAN models and the potential areas of improvement</li>
        <!--break-->
    </ul><br>

    <figure>
        <img src="/img/latent-masses-7.png" style="width: 70%;"><br>
        <figcaption>Interpolation in <a href="https://hsejun07.tistory.com/100">latent space</a></figcaption>
    </figure></div>

</div><br>

<h3>By the way, what is GANs(Generative Adversarial Networks)? 🧬</h3>
<div class="article">

    Generative Adversarial Networks, commonly referred to as GANs, are a class of artificial intelligence algorithms <code>designed to generate new data</code> that resemble a given set of data. 
    The architecture of a GAN consists of two primary components:
    <br><br>
    1. Generator
    <ul style="padding-left: 2em;">
        <li>The role of the generator is to create fake data</li>
        <li>It takes in random noise from a latent space and produces data samples as its output</li>
        <li> The primary objective of the generator is to <code>produce data that is indistinguishable</code> from real data</li>
    </ul><br>
    2.Discriminator
    <ul style="padding-left: 2em;">
        <li>The discriminator functions as a binary classifier</li>
        <li>It aims to <code>differentiate between real and fake data</code></li>
        <li>
            The discriminator receives both real data samples and the fake data generated by the generator, 
            and its task is to correctly label them as 'real' or 'fake'
        </li>
    </ul><br>

    The provided diagram illustrates this process, showing how the generator's output is evaluated by the discriminator, resulting in a <code>loss</code> that helps both parts improve.

    <figure>
        <img src="/img/latent-masses-10.jpg" style="width: 70%;"><br>
        <figcaption><a href="https://whatisai.blog/GAN1/">Generative adversarial networks</a> concept diagram</figcaption>
    </figure>

</div><br>

<h3>3D shape representations for the generative adversarial networks</h3>
<div class="article">
    
        1. Point cloud
        <ul style="padding-left: 2em;">
            <li>
                A point cloud is a set of data points in space. 
                In 3D shape representation, point clouds are typically used to <code>represent the external surface</code> of an object
                Each point in the point cloud has an (x, y, z) coordinate
            </li>
            <li>Can represent any 3D shape without being limited to a specific topology or grid (Good at flexibility)</li>
            <li>Points are disconnected, so additional processing is often required to extract surfaces or other features (Not good at lack of connectivity)</li>
        </ul><br>
        2. Voxel
        <ul style="padding-left: 2em;">
            <li>
                Voxels (short for volumetric pixels) are the 3D equivalent of 2D pixels. 
                A voxel representation <code>divides the 3D space into a regular grid, and each cell</code> (or voxel) in the grid can be either occupied or empty
            </li>
            <li>Operations like convolution are straightforward to apply on voxel grids (Simplicity)</li>
            <li>To represent fine details, a very high-resolution grid is needed, which can be computationally prohibitive (Limited resolution)</li>
        </ul><br>
        3. Mesh
        <ul style="padding-left: 2em;">
            <li>
                A 3D mesh consists of vertices, edges, and faces that define the shape of a 3D object in space. 
                The most common type of mesh is a triangular mesh, where the shape is <code>represented using triangles</code>
            </li>
            <li>Can represent both simple and complex geometries (Good expressiveness)</li>
            <li>Provides information about how points are connected, which is useful for many applications (Good at continuous surface representation)</li>
            <li>Operations on meshes, like subdivision or simplification, can be computationally demanding (Complexity)</li>
        </ul><br>

        <figure>
            <img src="/img/latent-masses-1.png" style="width: 80%;"><br>
            <figcaption>
                <a href="https://medium.com/@phamtdong0406/1-3d-ai-data-representation-99c98ec402c6">Shape representations</a><br>
                From the left, point cloud · mesh · voxel
            </figcaption>
        </figure>
</div><br>

<h3>Simple implementation: A single sphere GAN</h3>
<div class="article">
    First, I'll implement a practical application of training a GAN on point cloud data, aiming to generate a single sphere, represented by point cloud
    Before implementing the neural networks, we begin by loading our target sphere point cloud from a file. I modeled just one sphere shape using Rhino.

    <br><br>
    Typically, the normalization can be particularly beneficial if your training data consists of similar objects in various sizes or if the absolute size isn't critical for your task.
    In our dataset concerning a single sphere, the absolute size is not of significance. Therefore, let's normalize it. The sphere can be normalized easily using <code>numpy</code> as follows:

<pre><code class="python">
    class Normalize:
        def __call__(self, pointcloud):
            assert len(pointcloud.shape) == 2
            
            norm_pointcloud = pointcloud - np.mean(pointcloud, axis=0) 
            norm_pointcloud /= np.max(np.linalg.norm(norm_pointcloud, axis=1))
            
            return norm_pointcloud
</pre></code><br>

    If different 3D models have a different number of vertices, sampling a consistent number of points from each model <code>ensures that the input size</code> remains uniform. This is crucial when feeding data to neural networks that expect consistent input sizes.
    Please refer to the following <a href="https://github.com/PARKCHEOLHEE-lab/gan-exercise/blob/main/sphereGAN/utils.py#L9-L54">link</a> for the code related to PointSampler.

    <figure style="display: flex;">
        <img src="/img/latent-masses-2.png" style="width: 32%;"><br>
        <img src="/img/latent-masses-5.png" style="width: 32%;"><br>
        <img src="/img/latent-masses-6.png" style="width: 32%;"><br>
    </figure>
    <figcaption>
        A sphere, represented by point cloud <br>
        From the left, original sphere · random sampled sphere · normalized and random sampled sphere
    </figcaption><br><br>

    Now, we have completed the data preprocessing and it is now ready for model training. 
    Let us establish and train models comprising a simple generator and discriminator as follows:

<pre><code class="python">
    class Generator(nn.Module):
        def __init__(self, input_dim=3, output_dim=3, hidden_dim=128):
            super(Generator, self).__init__()
            self.fc1 = nn.Linear(input_dim, hidden_dim)
            self.fc2 = nn.Linear(hidden_dim, hidden_dim)
            self.fc3 = nn.Linear(hidden_dim, hidden_dim)
            self.fc4 = nn.Linear(hidden_dim, output_dim)
    
        def forward(self, x):
            x = torch.relu(self.fc1(x))
            x = torch.relu(self.fc2(x))
            x = torch.relu(self.fc3(x))
            x = torch.tanh(self.fc4(x))
            
            return x
    
    class Discriminator(nn.Module):
        def __init__(self, input_dim=3, hidden_dim=128):
            super(Discriminator, self).__init__()
            self.fc1 = nn.Linear(input_dim, hidden_dim)
            self.fc2 = nn.Linear(hidden_dim, hidden_dim)
            self.fc3 = nn.Linear(hidden_dim, hidden_dim)
            self.fc4 = nn.Linear(hidden_dim, 1)
    
        def forward(self, x):
            x = torch.relu(self.fc1(x))
            x = torch.relu(self.fc2(x))
            x = torch.relu(self.fc3(x))
            x = torch.sigmoid(self.fc4(x))
            
            return x
</pre></code><br>

    The comprehensive code, which includes details on the generator, discriminator, data, training process, and more, can be found at the following <a href="">link</a>. 
    Additionally, the training process visualized using Matplotlib can be viewed below.

    <figure style="display: flex;">
        <img src="/img/latent-masses-9.gif" style="width: 45%;"><br>
        <img src="/img/latent-masses-8.gif" style="width: 45%;"><br>
    </figure>
    <figcaption>
        Training process of a single sphere GAN<br>
        From the left, losses status · generated point cloud sphere 
    </figcaption><br><br>
</div><br>

<h3>Implementing MassGAN 🧱</h3>
<div class="article">
    From the above, we have gained some understanding of GANs through the implementation of fundamentals and <code>a single sphere GAN</code>. 
    Now, based on this understanding, let's train the model with buildings (Masses) designed by architects and create a generator that produces fake Masses

    <br><br>
    The procedure for the implementation of <code>MassGAN</code> follows the below processes:

    <ul style="padding-left: 2em;">
        <li>Preparation and preprocessing of the dataset</li>
        <li>Training models</li>
        <li>Evaluating the trained generator</li>
        <li>Exploration for latent spaces and manipulating generated data</li>
    </ul>

</div><br>    

<h3>Preparation and preprocessing of the dataset</h3>
<div class="article">
    Firstly, I collected building models designed by several famous architects for model training. 
    The figure below shows the actual buildings from the modeling data I gathered.

    <figure style="display: flex;">
        <img src="/img/latent-masses-11.png" style="width: 31%;">
        <img src="/img/latent-masses-13.png" style="width: 31%;">
        <img src="/img/latent-masses-12.png" style="width: 31%;">
    </figure>
    <figcaption>
        Voxel-shaped buildings <br>
        From the left, RED7(MVRDV architects) · 79andPark(BIG architects) · Mountain dwelling(BIG architects)  
    </figcaption><br><br>

    The buildings aforementioned possess a common characteristic: their voxel-shaped configuration. 
    As stated above, we learned three modalities of 3D shape representations pertinent to GANs. 
    The primary limitation of the voxel-shaped representation lies in its challenge to articulate high-resolution. 
    However, within the realm of architectural design, this constraint might be reconceived as an opportunity. 
    The voxel-shaped form is prevalently utilized in the architecture field, and there is no imperative demand for high-resolution depictions of such forms.

    <!-- 그러므로 우리는 적절한 해상도의 복셀 데이터를 이용해서 학습을 진행할 것입니다. 데이터는 다음과 같이 전처리됩니다. -->

    
</div><br>    

<h3>Training models</h3>
<div class="article">
</div><br>    

<h3>Evaluating the trained generator</h3>
<div class="article">
</div><br>    

<h3>Exploration for latent spaces and manipulating generated data</h3>
<div class="article">
</div><br>    

<h3>References</h3>
<div class="article">
    <ul>
        <li><a href="https://medium.com/hackernoon/latent-space-visualization-deep-learning-bits-2-bd09a46920df">https://medium.com/hackernoon/latent-space-visualization-deep-learning-bits-2-bd09a46920df</a></li>
        <li><a href="https://github.com/ChrisWu1997/SingleShapeGen">https://github.com/ChrisWu1997/SingleShapeGen</a></li>
    </ul>
    
    <!-- <figure>
        <img src="/img/latent-masses-0.gif" width="100%">
        <figcaption>Visualized training process at each 200 epochs</figcaption>
    </figure> -->
</div>


<!-- 

    3. Implementing MassGAN
        - Use voxel representation
            . the voxel is appropriate in the architectural field. because it has no deadspace ...
        
        - Preparing data that is shaped mesh and converting them to binary voxel grid
        - training
            . training process per 100 epoch -> .gif

    4. Exploring latent masses between multiple masses generated
        - interpolating process -> .gif

    5. References
        - SingleShapeGan
        - Other articles related to this post
        
 -->