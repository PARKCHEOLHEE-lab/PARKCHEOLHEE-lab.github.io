---
title: "Polygon segmentations"
layout: post
hashtag: "#geometry #deep-learning #graph-neural-network #link-prediction"
comment: true
splitter: 2
featured: true
thumbnail: /img/polygon-segmentation-thumbnail-1.png
---

<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
<style>
    .mjx-chtml {
        font-size: 100% !important; /* Adjust the size as needed */
    }
</style>

<div id="toc"></div>

<h3>Objective ‚úÇÔ∏è</h3>
<div class="article">

    In the early stages of architectural design, there is a concept about the <code>axis</code> in which direction the building will be placed.
    This plays an important role in determining the optimal layout considering the functionality, aesthetics, and environmental conditions of the building.
    The goal of this project is to develop a segmenter that can determine how many axes a given 2D polygon should be segmented into and how to make those segmentations. 
    <br><br>
    
    <figure style="display: flex;">
        <img src="/img/polygon-segmentation-0.png" width="45%">
        <img src="/img/polygon-segmentation-1.png" width="45%">
    </figure>
    
    <figure style="display: flex;">
        <img src="/img/polygon-segmentation-3.png" width="45%">
        <img src="/img/polygon-segmentation-2.png" width="45%">
    </figure>

    <figcaption>
        Segmented lands by a human architect
    </figcaption><br><br>

    The figures above are imaginary dividing lines arbitrarily set by the architect. The apartments in the figures are placed based on segmented polygons.
    Human architects <code>intuitively know</code> how many axes a given 2D polygon should be segmented into and how to create these segmentations, but explaining this intuition to a computer is difficult.

    <br><br>

    To achieve this, I will use a combination of deep learning and graph theory. 
    In the graph, each point of the polygon will be a node and the connections between points will be edges.
    Based on this concept, I will implement a GNN-based model, which will learn how to optimally segment given polygons.

    <!--break-->

</div>
<br><br>

<h3>A simple understanding of graph and GNN</h3>
<div class="article">

    Before generating data, let's understand graphs and Graph Neural Networks.
    Basically, the graph is defined \( G = (V, E) \). At this expression, \( V \) is the set of vertices (nodes) and \( E \) is the set of edges.

    <br><br>

    Graphs are mainly expressed as an <code>adjacency matrix</code>. When the number of points is \( n \), the size of the adjacency matrix \( A \) is \( n \times n \). 
    When dealing with a graph in machine learning, it is expressed as a feature matrix depicting the characteristics of points. When the number of features is \( f \), the dimension of the feature matrix \( X \) is \( n \times f \).

    <figure>
        <img src="/img/polygon-segmentation-4.png" width="90%">
        <figcaption>
            <a href="https://medium.com/watcha/gnn-%EC%86%8C%EA%B0%9C-%EA%B8%B0%EC%B4%88%EB%B6%80%ED%84%B0-%EB%85%BC%EB%AC%B8%EA%B9%8C%EC%A7%80-96567b783479">Understanding of the graph expression</a> <br>
            In this figure, \( n = 4 \), \( f = 3 \) <br> 
            \( A = n \times n = 4 \times 4 \) <br>
            \( X = n \times f = 4 \times 3 \).
        </figcaption>
    </figure><br><br>

    <code>Graphs</code> are used in various fields to represent data, and they are useful when teaching geometry to deep learning models for the following reasons:
    
    <ul style="padding-left: 2em;">
        <li class="decimal"><b>Representing Complex Structures Simply</b>: Graphs effectively represent complex structures and relationships, making them suitable for modeling various forms of geometric data.</li>
        <li class="decimal"><b>Flexibility</b>: Graphs can accommodate varying numbers of nodes and edges, making them advantageous when dealing with objects of diverse shapes and sizes.</li>
    </ul>

    <br><br>

    <code>Graph Neural Networks (GNN)</code> is a type of neural network designed to operate on graph structures. 
    Unlike traditional neural networks, which work on fixed-size inputs like vectors or matrices, GNNs can handle graph-structured data with variable size and connectivity. 
    This makes GNNs particularly suitable for tasks where relationships are important, such as social networks, <code>geometries</code>, etc. 
    It primarily use connections and the states of neighboring nodes to update (learn) the state of each node (<a href="https://process-mining.tistory.com/164">Message Passing</a>). 
    Predictions are then made based on the final states. This final state is generally referred to as the <code>node embedding</code> (or I think also <code>encoding</code> is right because raw features of nodes are changed to other representations).

    <br><br>

    There are various methods for Message Passing, but since this task will be dealing with geometry, let's focus on models based on <code>Spatial Convolutionan Network</code>. 
    This method is known to be suitable for representing data with important geometric and spatial structures.
    GNNs using Spatial Convolutional Network enable each node to integrate information from neighboring nodes, allowing for a more accurate understanding of local characteristics. 
    Through this, the model can better comprehend the complex shapes and features of geometry.

    <figure>
        <img src="/img/polygon-segmentation-5.png" width="90%">
        <figcaption>
            Convolution operations<br>
            From the left, <a href="https://arxiv.org/abs/1901.00596">2D Convolution ¬∑ Graph Convolution</a>
        </figcaption>
    </figure><br><br>

    The idea of a Spatial Convolutional Network (SCN) is similar to that of a Convolutional Neural Network (CNN). An image can be transformed into a <code>grid-shaped graph</code>.

    <br><br>

    CNN processes images by using filters to combine the surrounding pixels of a central pixel. 
    <code>SCN</code> extends this idea to graph structures by combining the features of neighboring connected nodes instead of neighboring pixels.

    Specifically, CNNs are useful for processing images in a regular grid structure, where the filter considers the surrounding area of each pixel to extract features. 
    In contrast, <code>SCNs</code> operate on general graph structures, combining the features of each node with those of its connected neighbors to generate <code>embeddings</code>.

</div>
<br><br>

<h3>Data preparation</h3>
<div class="article">

    Since I have briefly looked into graphs and GNNs in the above, let's now prepare the data! There are already many raw polygons around us, and that is the land.
    First, let's collect raw polygons from <a href="https://www.vworld.kr/dtna/dtna_fileDataList_s001.do">vworld</a>. Below is a part of all the raw polygons I collected from vworld.

    <figure>
        <img src="/img/polygon-segmentation-6.png" width="95%">
        <figcaption>
            Somewhere in Seoul.shp
        </figcaption>
    </figure><br><br>

    Now that we have collected the raw polygons, let's define the characteristics of the polygons to be included in the feature matrix. They are as follows:
    <ul style="padding-left: 2em;">
        <li>X coordinate (normalized)</li>
        <li>Y coordinate (normalized)</li>
        <li>Inner angle of each vertex (normalized)</li>
        <li>Incoming edge length (normalized)</li>
        <li>Outgoing edge length (normalized)</li>
        <li>Concavity or convexity of each vertex</li>
        <li>Minimum rotated rectangle ratio</li>
        <li>Minimum rotated rectangle aspect ratio</li>
        <li>Polygon area</li>
    </ul><br><br>

    Then, I need to convert these data into graph form. Here is an example of hand labeling:
    
    <figure>
        <img src="/img/polygon-segmentation-8.png" width="100%">
        <figcaption>
            Land polygon in Gangbukgu, Seoul <br>
            From the left, raw polygon & labeled link ¬∑ adjacency matrix ¬∑ feature matrix
        </figcaption>
    </figure><br><br>

    However since it is impossible to label countless raw polygons by hand, I need to generate this data automatically. 
    It would be nice if it could be fully automated with an algorithm, but if that were possible, there wouldn't be a need for deep learning ü§î.
    
    So, I created a naive algorithm that can reduce manual work even a little. 
    This algorithm is inspired by the <code>triangulations</code> of the polygon. The process of this algorithm is as follows:
    
    <ul style="padding-left: 2em;">
        <li class="decimal"><b>Triangulate a polygon</b>: Retrieve the triangulation edges.</li>
        <li class="decimal"><b>Generate Splitter Combinations</b>: Generate all possible combinations of splitters for iterating over them.</li>
        <li class="decimal"><b>Iterate Over Combinations</b>: Iterate over all possible combinations of splitters to find the best segmented polygon.</li>
        <li class="decimal"><b>Check Split Validity</b>: Check if all segmentations are valid.</li>
        <li class="decimal"><b>Compute Scores and evaluation</b>: Compute scores to store the best segmentations.</li>
    </ul><br><br>

    The scores (<code>even_area_score</code>, <code>ombr_ratio_score</code>, <code>slope_similarity_score</code>) used in the fifth step are computed as follows, and each score is aggregated as a weighted sum to obtain the segmentations with the lowest score.

    <br><br>

    <p>
        \[
        even\_area\_score = \frac{(A_1 - \sum_{i=2}^{n} A_i)}{A_{polygon}} \times {w_1}
        \]
    </p>
    <br>
    <p>
        \[
        ombr\_ratio\_score = |(1 - \frac{A_{split1}}{A_{ombr1}}) - \sum_{i=2}^{n} (1 - \frac{A_{spliti}}{A_{ombri}})| \times {w_2}
        \]
    </p>
    <br>
    <p>
        \[
        avg\_slope\_similarity_i = \frac{\sum_{j=1}^{k_i} |\text{slope}_j - \text{slope}_{\text{main}}|}{k_i}
        \]
    </p>
    <br>
    <p>
        \[
        slope\_similarity\_score = \frac{\sum_{i=1}^{n} avg\_slope\_similarity_i}{n} \times {w_3}
        \]
    </p>
    <br><br>
    <p>
        \[
        score = even\_area\_score + ombr\_ratio\_score + slope\_similarity\_score
        \]
    </p>
    <br><br>


    The whole code for this algorithm can be found <a href="https://github.com/PARKCHEOLHEE-lab/polygon-segmentation-with-gcn/blob/b8e8e7516326c428f435a17af35705308e14decc/polygon_segmentation_with_gcn/src/data_creator.py#L670-L855">here</a>,
    and the key part of the algorithm is as follows.

<pre><code class="python">
    def segment_polygon(
        self,
        polygon: Polygon,
        number_to_split: int,
        segment_threshold_length: float,
        even_area_weight: float,
        ombr_ratio_weight: float,
        slope_similarity_weight: float,
        return_splitters_only: bool = True,
    ):
        """Segment a given polygon

        Args:
            polygon (Polygon): polygon to segment
            number_to_split (int): number of splits to segment
            segment_threshold_length (float): segment threshold length
            even_area_weight (float): even area weight
            ombr_ratio_weight (float): ombr ratio weight
            slope_similarity_weight (float): slope similarity weight
            return_splitters_only (bool, optional): return splitters only. Defaults to True.

        Returns:
            Tuple[List[Polygon], List[LineString], List[LineString]]: splits, triangulations edges, splitters
        """

        _, triangulations_edges = self.triangulate_polygon(
            polygon=polygon,
            segment_threshold_length=segment_threshold_length,
        )

        splitters_selceted = None
        splits_selected = None
        splits_score = None

        for splitters in list(itertools.combinations(triangulations_edges, number_to_split - 1)):
            exterior_with_splitters = ops.unary_union(list(splitters) + self.explode_polygon(polygon))

            exterior_with_splitters = shapely.set_precision(
                exterior_with_splitters, self.TOLERANCE_LARGE, mode="valid_output"
            )

            exterior_with_splitters = ops.unary_union(exterior_with_splitters)

            splits = list(ops.polygonize(exterior_with_splitters))

            if len(splits) != number_to_split:
                continue

            if any(split.area < polygon.area * 0.25 for split in splits):
                continue

            is_acute_angle_in = False
            is_triangle_shape_in = False
            for split in splits:
                split_segments = self.explode_polygon(split)
                splitter_indices = []

                for ssi, split_segment in enumerate(split_segments):
                    if split_segment.length <= self.TOLERANCE_LARGE * 2:
                        continue

                    reduced_split_segment = DataCreatorHelper.extend_linestring(
                        split_segment, -self.TOLERANCE_LARGE, -self.TOLERANCE_LARGE
                    )
                    buffered_split_segment = reduced_split_segment.buffer(self.TOLERANCE, cap_style=CAP_STYLE.flat)

                    if buffered_split_segment.intersects(MultiLineString(splitters)):
                        splitter_indices.append(ssi)
                        splitter_indices.append(ssi + 1)

                degrees = self.compute_polyon_inner_degrees(split)
                degrees += [degrees[0]]

                if (np.array(degrees)[splitter_indices] < 20).sum():
                    is_acute_angle_in = True
                    break

                if len(self.explode_polygon(self.simplify_polygon(split))) == 3:
                    is_triangle_shape_in = True
                    break

            if is_acute_angle_in or is_triangle_shape_in:
                continue

            sorted_splits_area = sorted([split.area for split in splits], reverse=True)
            even_area_score = (sorted_splits_area[0] - sum(sorted_splits_area[1:])) / polygon.area * even_area_weight

            ombr_ratio_scores = []
            slope_similarity_scores = []

            for split in splits:
                ombr = split.minimum_rotated_rectangle
                each_ombr_ratio = split.area / ombr.area
                inverted_ombr_score = 1 - each_ombr_ratio
                ombr_ratio_scores.append(inverted_ombr_score)

                slopes = []
                for splitter in splitters:
                    if split.buffer(self.TOLERANCE_LARGE).intersects(splitter):
                        slopes.append(self.compute_slope(splitter.coords[0], splitter.coords[1]))

                splitter_main_slope = max(slopes, key=abs)

                split_slopes_similarity = []
                split_segments = self.explode_polygon(split)
                for split_seg in split_segments:
                    split_seg_slope = self.compute_slope(split_seg.coords[0], split_seg.coords[1])
                    split_slopes_similarity.append(abs(splitter_main_slope - split_seg_slope))

                avg_slope_similarity = sum(split_slopes_similarity) / len(split_slopes_similarity)
                slope_similarity_scores.append(avg_slope_similarity)

            ombr_ratio_score = abs(ombr_ratio_scores[0] - sum(ombr_ratio_scores[1:])) * ombr_ratio_weight
            slope_similarity_score = sum(slope_similarity_scores) / len(splits) * slope_similarity_weight

            score_sum = even_area_score + ombr_ratio_score + slope_similarity_score

            if splits_score is None or splits_score > score_sum:
                splits_score = score_sum
                splits_selected = splits
                splitters_selceted = splitters

        if return_splitters_only:
            return splitters_selceted

        return splits_selected, triangulations_edges, splitters_selceted
</code></pre><br><br>

    However, this algorithm is not perfect, and there are some problems. Because this algorithm uses the weights for computing scores, it may be sensitive to them. Look at the below figures.
    The first is good case, and the second is bad case. 
    
    <figure>
        <img src="/img/polygon-segmentation-9.png" width="70%">
        <!-- <img src="/img/polygon-segmentation-11.png" width="70%"> -->
        <img src="/img/polygon-segmentation-10.png" width="70%">
        <!-- <img src="/img/polygon-segmentation-12.png" width="70%"> -->
        <figcaption>
            Results of the algorithm<br>
            From the left, triangulations ¬∑ segmentations ¬∑ oriented bounding boxes for segmentations
        </figcaption>
    </figure><br><br>

    Since these problems cannot be handled by my naive algorithm, I first used the algorithm to process the raw polygon data and then labeled it manually like the following.
    So now I have approximately 40000 data with augmented originals. The all dataset can be found <a href="https://github.com/PARKCHEOLHEE-lab/polygon-segmentation-with-gcn/tree/main/polygon_segmentation_with_gcn/data/processed">here</a>.
    <figure>
        <img src="/img/polygon-segmentation-13.png" width="100%">
        <figcaption>
            Some data for training<br>
        </figcaption>
    </figure>

</div>
<br>

<h3>Model for link prediction</h3>
<div class="article">

    Now, let's create a graph model using <a href="https://pytorch-geometric.readthedocs.io/en/latest/">Pytorch Geometric</a> and teach the graph data.
    Pytorch Geometric is a library based on PyTorch to easily write and train graph neural networks.

    <br><br>

    The role I need to assign to the model is to generate lines that will segment polygons. This can be translated into a task primarily used in GNNs, known as <code>link prediction</code>.
    Link prediction models usually use an <code>encoder-decoder</code> structure. 
    The encoder creates node embeddings, which are vector representations of the nodes that extract their features. 
    The decoder then uses these embeddings to predict the probability that a pair of nodes is connected.

    <br><br>

    When inference, the model inputs all possible node pairs into the decoder. 
    It then calculates the probability of each pair being connected. 
    Only pairs with probabilities above a certain threshold are kept, indicating likely connections.

    <br><br>

    Generally, a simple operation like the <code>dot product</code> is used to predict links based on the similarity of node pairs. 
    However, I thought this approach was not suitable for tasks using geometric data, so I additionally trained a decoder. 
    Below are the <code>encode</code> and <code>decode</code> methods of the model. The complete model code can be found <a href="https://github.com/PARKCHEOLHEE-lab/polygon-segmentation-with-gcn/blob/b8e8e7516326c428f435a17af35705308e14decc/polygon_segmentation_with_gcn/src/model.py#L124">here</a>.

<pre><code class="python">
    class PolygonSegmenter(nn.Module):
        def __init__(
            self,
            conv_type: str,
            in_channels: int,
            hidden_channels: int,
            out_channels: int,
            encoder_activation: nn.Module,
            decoder_activation: nn.Module,
            predictor_activation: nn.Module,
            use_skip_connection: bool = Configuration.USE_SKIP_CONNECTION,
        ):
                
            ( ... )

        def encode(self, data: Batch) -> torch.Tensor:
            """Encode the features of polygon graphs

            Args:
                data (Batch): graph batch

            Returns:
                torch.Tensor: Encoded features
            """

            encoded = self.encoder(data.x, data.edge_index, edge_weight=data.edge_weight)

            return encoded

        def decode(self, data: Batch, encoded: torch.Tensor, edge_label_index: torch.Tensor) -> torch.Tensor:
            """Decode the encoded features of the nodes to predict whether the edges are connected

            Args:
                data (Batch): graph batch
                encoded (torch.Tensor): Encoded features
                edge_label_index (torch.Tensor): indices labels

            Returns:
                torch.Tensor: whether the edges are connected
            """

            # Merge raw features and encoded features to inject geometric informations
            if self.use_skip_connection:
                encoded = torch.cat([data.x, encoded], dim=1)

            decoded = self.decoder(torch.cat([encoded[edge_label_index[0]], encoded[edge_label_index[1]]], dim=1)).squeeze()

            return decoded
</code></pre><br>

    The encoding process transforms the original feature matrix \( F \) into a new matrix \( E \) with potentially different dimensions. 
    The following is an expression of the feature matrix before and after the encode function.

    <ul>
        <p>\( n \) as the number of nodes</p>
        <p>\( m \) as the number of features</p>
        <p>\( c \) as the number of channels</p>
        <p>\( F \in \mathbb{R}^{n \times m} \) as the feature matrix before the <code>encode</code> function</p>
        <p>\( E \in \mathbb{R}^{n \times c} \) as the feature matrix after the <code>encode</code> function</p>
    </ul>

    <div style="display: flex; justify-content: space-around;">
        <div>
            \[
            F = \begin{bmatrix}
            f_{11} & f_{12} & \cdots & f_{1m} \\
            f_{21} & f_{22} & \cdots & f_{2m} \\
            \vdots & \vdots & \ddots & \vdots \\
            f_{n1} & f_{n2} & \cdots & f_{nm}
            \end{bmatrix}
            \]
        </div>
        <div>
            \[
            E = \begin{bmatrix}
            e_{11} & e_{12} & \cdots & e_{1c} \\
            e_{21} & e_{22} & \cdots & e_{2c} \\
            \vdots & \vdots & \ddots & \vdots \\
            e_{n1} & e_{n2} & \cdots & e_{nc}
            \end{bmatrix}
            \]
        </div>
    </div><br><br>


    In the <code>decode</code> method, the encoded features of the nodes and <code>raw features</code> are used to predict the connections (links) between them. 
    This skip connection merges raw features and encoded features to inject geometric information. 
    Using skip connections helps preserve original features, and enhances overall model performance by combining low-level and high-level information.

    <br><br>


    In the <a href="https://github.com/PARKCHEOLHEE-lab/polygon-segmentation-with-gcn/blob/b8e8e7516326c428f435a17af35705308e14decc/polygon_segmentation_with_gcn/src/model.py#L273-L323">feedforward</a> process, the decoder is trained with connected labels and labels that should not be connected. This is a technique called <code>negative sampling</code>, which is used to improve model performance in link prediction tasks.

    By providing examples of what should not be linked, negative sampling helps the model better distinguish between actual links and non-links, leading to improved accuracy in predicting future or missing links.

    <br><br>
    In most networks, actual links are significantly fewer than non-links, which can <code>bias</code> the model towards predicting no link. Negative sampling allows for controlled selection of negative examples, balancing the training data and enhancing the learning process.

<pre><code class="python">
        def forward(self, data: Batch) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
            """Forward method of the models, segmenter and predictor

            Args:
                data (Batch): graph batch

            Returns:
                Tuple[torch.Tensor, torch.Tensor, torch.Tensor]: whether the edges are connected, predicted k and target k
            """

            # Encode the features of polygon graphs
            encoded = self.encode(data)

            ( ... )

            # Sample negative edges
            negative_edge_index = negative_sampling(
                edge_index=data.edge_label_index_only,
                num_nodes=data.num_nodes,
                num_neg_samples=int(data.edge_label_index_only.shape[1] * Configuration.NEGATIVE_SAMPLE_MULTIPLIER),
                method="sparse",
            )

            # Decode the encoded features of the nodes to predict whether the edges are connected
            decoded = self.decode(data, encoded, torch.hstack([data.edge_label_index_only, negative_edge_index]).int())

            return decoded, k_predictions, k_targets
</code></pre><br><br>

    Up to this point, I defined the encoder and decoder.
    Since the encoder and decoder use nodes in batches, it seems that they cannot recognize each graph separately.

    Because I wanted to train the model on how many segmentations to divide the graph into when a graph is input, 
    I defined a <code>predictor</code> to train <code>k</code> separately within the <code>segmenter</code> class.

    <br><br>

    The segmenter generates the segmentations using <code>topk</code> through the encoder and decoder processes described above. 
    It then sorts the generated links in order of connection strength, and the predictor decides how many links to use.

    <figure>
        <img src="/img/polygon-segmentation-15.png" width="100%">
        <figcaption>
            <a href="https://github.com/PARKCHEOLHEE-lab/polygon-segmentation-with-gcn/blob/5052a9e5ea1d740f7a98c3d07b249e233030435c/polygon_segmentation_with_gcn/src/model.py#L327-L462">Inference</a> process<br>
            From the top, topk segmentations ¬∑ segmentation selected by predictor
        </figcaption>
    </figure>

</div>
<br><br>

<h3>Training and evaluating</h3>
<div class="article">
    It's time to train the model. 
    The model has been trained for 500 epochs, during which both the training loss and validation loss were recorded to monitor the training progress and convergence. As shown in the plots:

    <ul style="padding-left: 2em;">
        <li>According to the top plots, both the training loss and validation loss decrease quickly initially and then generalize.</li>
        <li>All evaluation metrics (accuracy, F1 score, recall, and AUROC) show rapid improvement initially and then stabilize, having good generalization.</li>
    </ul>
    
    <figure>
        <img src="/img/polygon-segmentation-18.png" width="80%">
        <figcaption>
            Losses and metrics for 500 epochs
        </figcaption>
    </figure><br><br>
    
    All the metrics look good, but there is a question about whether these metrics can be trusted 100%. 
    This may be due to the impact of <code>negative sampling</code>. <br><br>
    Based on the visualization of some of the test data, it is evident that the model accurately predicts polygons that do not require segmenting. 
    This suggests that the model may be <code>overfitting on negative samples</code>. 
    High performance on negative samples might not accurately reflect the model's ability to identify positive cases correctly.

    <figure>
        <img src="/img/polygon-segmentation-19.png" width="100%">
        <figcaption>
            Evaluate some test data qualitatively
        </figcaption>
    </figure>

    <br><br>

    <!-- Ïù¥Ïóê Îî∞ÎùºÏÑú ÎÇòÎäî Ïã†Î¢∞Ìï† Ïàò ÏûàÎäî ÏÑ±Îä•ÏßÄÌëúÎ•º ÎßåÎì§ÏóàÏäµÎãàÎã§.  -->

    Taking inspiration from <code>IoU loss</code>, therefore I defined <code>GeometricLoss</code> to evaluate segmentation quality and create a reliable evaluation metric.
    The <code>GeometricLoss</code> aims to quantify the discrepancy between predicted and ground-truth geometric structures within a <code>graph</code>. 
    The process of the geometric loss calculation is as follows:
        
    <ul style="padding-left: 2em;">
        <li class="decimal"><b>Polygon Creation from Node Features</b>: Each graph's node features, representing coordinates, are converted into a polygon.</li>
        <li class="decimal"><b>Connecting Predicted Edges</b>: These edges represent the predicted segmentation of the polygon.</li>
        <li class="decimal"><b>Connecting Ground-Truth Edges</b>: These edges represent the correct segmentation of the polygon.</li>
        <li class="decimal"><b>Creating Buffered Unions</b>: The predicted and ground-truth edges are combined and then buffered.</li>
        <li class="decimal"><b>Calculating Intersection</b>: This intersection represents the common area between the predicted and true geometric structures.</li>
        <li class="decimal"><b>Computing Geometric Loss</b>: The intersection area is normalized by the ground-truth area and then negated</li>
    </ul>
    
    <figure>
        <img src="/img/polygon-segmentation-20.png" width="90%">
        <figcaption>
            An example for the geometric loss calculation <br>
            From the left, loss: -0.000478528 ¬∑ loss: -0.999999994
        </figcaption>
    </figure><br><br>

    The geometric loss serves solely as an <code>evaluation metric</code> for the model. Therefore, it has not been added to the BCE loss. 
    This is because the model's training batches are based on nodes rather than graphs. Hence, calculating this loss for every graph per epoch would significantly slow down the process. 
    Therefore, I have calculated this loss only for <code>4 samples</code> per batch to use it exclusively as a model evaluation metric. The results are as follows:

    
    <figure>
        <img src="/img/polygon-segmentation-21.png" width="85%">
        <figcaption>
            Geometric losses for 500 epochs <br>
            From the left, train geometric loss ¬∑ validation geometric loss
        </figcaption>
    </figure><br><br>

    The <code>GeometricLoss</code> class is defined as follows and the code can be found <a href="https://github.com/PARKCHEOLHEE-lab/polygon-segmentation-with-gcn/blob/main/polygon_segmentation_with_gcn/src/model.py#L39-L113">here</a>.

    
</div>
<br><br>

<h3>Limitations and future works</h3>
<div class="article">

    GNNs are node embedding-based models, so they seem to recognize the characteristics of <code>individual nodes</code> rather than the <code>overall shape of the graph</code>. 
    While GNNs have a good ability to generalize shapes during training, it has been challenging to overfit them accurately to the intended labels.
    
    <ul style="padding-left: 2em;">
        <li><b>Comparison with CNNs</b>: CNNs excel at capturing local patterns and combining them into more abstract representations, which can be advantageous for tasks involving polygons.</li>
        <li><b>Imbalanced labels for predictor</b>: The predictor is trained to select one of 0, 1, or 2, but the number of data for 2 is very few.</li>
        <li><b>Exploring reliable metrics</b>: Explore metrics that are more suitable for geometric tasks.</li>
        <li><b>Overfitting</b>: Studying how positive labels can be completely overfitted.</li>
    </ul>
</div>
<br><br>

<h3>References</h3>
<div class="article">

    <ul style="padding-left: 2em;">
        <li><a href="https://pytorch-geometric.readthedocs.io/en/latest/">https://pytorch-geometric.readthedocs.io/en/latest/</a></li>
        <li><a href="https://en.wikipedia.org/wiki/Graph_theory/">https://en.wikipedia.org/wiki/Graph_theory/</a></li>
        <li><a href="https://mathworld.wolfram.com/AdjacencyMatrix.html/">https://mathworld.wolfram.com/AdjacencyMatrix.html/</a></li>
        <li><a href="https://process-mining.tistory.com/164">https://process-mining.tistory.com/164</a></li>
        <li><a href="https://blog.naver.com/gyrbsdl18/222556439520">https://blog.naver.com/gyrbsdl18/222556439520</a></li>
        <li><a href="https://medium.com/watcha/gnn-%EC%86%8C%EA%B0%9C-%EA%B8%B0%EC%B4%88%EB%B6%80%ED%84%B0-%EB%85%BC%EB%AC%B8%EA%B9%8C%EC%A7%80-96567b783479/">https://medium.com/watcha/gnn-%EC%86%8C%EA%B0%9C-%EA%B8%B0%EC%B4%88%EB%B6%80%ED%84%B0-%EB%85%BC%EB%AC%B8%EA%B9%8C%EC%A7%80-96567b783479/</a></li>
    </ul>
    
</div><br><br>