---
title: "Shape-conditional GANs"
layout: post
hashtag: "#geometry #conditional-generative-adversairal-networks #deep-learning"
featured: false
comment: true
splitter: 0
thumbnail: /img/polygons-thumbnail-2.gif
---

<div id="toc"></div>

<h3>What is Conditional Generative Adversarial Networks ❓</h3>
<div class="article">

    <code>Conditional Generative Adversarial Networks</code> (cGANs) is an extension of the original Generative Adversarial Networks.
    cGANs take the concept of conventional GANs further by feeding additional information into both the generator and discriminator, allowing the generation of data that is more specific and controlled.

    This information acts as a directive or constraint for the generator on what type of data to produce. 

    <br><br>
    
    The paper for <a href="https://arxiv.org/pdf/1411.1784.pdf">Conditional Generative Adversarial Nets</a> says:
    <br>
    <b>"</b> Generative adversarial nets can be extended to a conditional model if both the generator and discriminator are conditioned on some <code>extra information y.</code> 
    y could be any kind of auxiliary information, such as class labels or data from other modalities. We can perform the conditioning by feeding y into the both the discriminator and generator as additional input layer. <b>"</b>
    
    <!--break-->

    <figure>
        <img src="../img/geometric-losses-7.png" style="width: 80%; display: block; margin-left: auto; margin-right: auto;">
        <figcaption style="text-align: center; margin-top: 1em">
            Conditional adversarial net
        </figcaption>
    </figure><br>
    
    Since I am interested in handling geometry and generative design, I wanted to combine my interests with cGAN to solve a simple problem and understand these concepts.

</div><br>

<h3>Problem definition</h3>
<div class="article">

    To experiment with the above, 
    let me define a simple geometric problem. 
    It is to find the <code>largest inscribed rectangle</code> on a given 2d polygon like the following.
    <br><br>

    <figure style="display: flex;">
        <img src="/img/geometric-losses-2.png" width="25%">
        <img src="/img/geometric-losses-3.png" width="25%">
        <img src="/img/geometric-losses-1.png" width="25%">
    </figure>
    <figcaption style="text-align: center; margin-top: 1em">
        <a href="https://community.esri.com/t5/spatial-data-science-questions/how-to-find-the-maximum-rectangle-contained-within/td-p/408977">The largest inscribed rectangle</a>    
    </figcaption>

    <br><br>

    After setting a geometric problem, I defined the algorithm for finding <a href="https://github.com/PARKCHEOLHEE-lab/toytorch/blob/766853852c332c66decf3478eace9fd034c0602d/lirGAN/data/largest_inscribed_rectangle.py#L12-L133">LIR</a>.
    In this algorithm, a given polygon is represented as a <code>binary grid</code> of 1s and 0s.
    1 is represented as a solid part and 0 is represented as a void part. 
    The following representation of a binary grid-shaped polygon has 100 x 100 grid size. If the size is higher, it can be represented more precisely.

    <br><br>

    <figure>
        <img src="../img/geometric-losses-4.png" style="width: 80%; display: block; margin-left: auto; margin-right: auto;">
        <figcaption style="text-align: center; margin-top: 1em">
            Representation of a geometry<br>
            From the left, Vector-shaped polygon · Binary grid-shaped polygon
        </figcaption>
    </figure>


</div><br>

<h3>Data preparation</h3>
<div class="article">

    We have set up a problem and a way to represent geometric data to experiment with the problem in the above section. 
    In this part, we create a dataset to train a generator that estimates the <code>LIR</code> given an input polygon.

    <br><br>

    First of all, I'm going to define the function for creating a polygon with random coordinates called <code>_get_random_coordinates</code> as follows:
    
<pre><code class="python">
    def _get_random_coordinates(
        self, vertices_count_min: int, vertices_count_max: int, scale_factor: float = 1.0
    ) -> np.ndarray:
        """Generate non-intersected polygon randomly

        Args:
            vertices_count_min (int): random vertices count minimum value
            vertices_count_max (int): random vertices count maximum value
            scale_factor (float, optional): constant to scale. Defaults to 1.0.

        Returns:
            np.ndarray: random coordinates
        """

        vertices_count = np.random.randint(vertices_count_min, vertices_count_max)
        vertices = np.random.rand(vertices_count, 2)
        vertices_centroid = np.mean(vertices, axis=0)

        coordinates = sorted(vertices, key=lambda p, c=vertices_centroid: np.arctan2(p[1] - c[1], p[0] - c[0]))

        coordinates = np.array(coordinates)
        coordinates[:, 0] *= scale_factor
        coordinates[:, 1] *= scale_factor

        return coordinates
</pre></code><br><br>

    This algorithm sorts polygon vertices by angle from the center to each vertex so that they do not intersect.
    The whole process of creating a random polygon is as follows and you can see the code in this <a href="https://github.com/PARKCHEOLHEE-lab/toytorch/blob/766853852c332c66decf3478eace9fd034c0602d/lirGAN/data/data_creator.py#L45-L69">link</a>.
    <br><br>

    <figure>
        <img src="../img/geometric-losses-5.png" style="width: 70%; display: block; margin-left: auto; margin-right: auto;">
        <figcaption style="text-align: center; margin-top: 1em">
            The process of creating a random polygon
        </figcaption>
    </figure>

    <br><br>

    After creating each random polygon, it needs to <a href="https://github.com/PARKCHEOLHEE-lab/toytorch/blob/766853852c332c66decf3478eace9fd034c0602d/lirGAN/data/data_creator.py#L71-L98">resize</a> as much as the grid size(I set 256x256).
    We then need to <a href="https://github.com/PARKCHEOLHEE-lab/toytorch/blob/766853852c332c66decf3478eace9fd034c0602d/lirGAN/data/utils.py#L59-L74">convert</a> these <code>vector-shaped polygons to binary grid-shaped polygons</code> consisting of 1s and 0s. It's easy with OpenCV.
    Through this process, <a href="https://github.com/PARKCHEOLHEE-lab/toytorch/tree/main/lirGAN/data/binpy">5000 datasets</a> were created as shown in the figure below. If you need more data sets, you can get them easily.

    <br><br>

    <figure>
        <img src="../img/geometric-losses-6.png" style="width: 70%; display: block; margin-left: auto; margin-right: auto;">
        <img src="../img/geometric-losses-8.png" style="width: 70%; display: block; margin-left: auto; margin-right: auto;">
        <img src="../img/geometric-losses-9.png" style="width: 70%; display: block; margin-left: auto; margin-right: auto;">
        <img src="../img/geometric-losses-10.png" style="width: 70%; display: block; margin-left: auto; margin-right: auto;">
        <img src="../img/geometric-losses-11.png" style="width: 70%; display: block; margin-left: auto; margin-right: auto;">
        <img src="../img/geometric-losses-12.png" style="width: 70%; display: block; margin-left: auto; margin-right: auto;">
        <img src="../img/geometric-losses-13.png" style="width: 70%; display: block; margin-left: auto; margin-right: auto;">
        <img src="../img/geometric-losses-14.png" style="width: 70%; display: block; margin-left: auto; margin-right: auto;">
        <img src="../img/geometric-losses-15.png" style="width: 70%; display: block; margin-left: auto; margin-right: auto;">
        <img src="../img/geometric-losses-16.png" style="width: 70%; display: block; margin-left: auto; margin-right: auto;">
        <figcaption style="text-align: center; margin-top: 1em">
            The created random polygons
        </figcaption>
    </figure>


</div><br>

<h3>Building models and loss functions</h3>
<div class="article">

    In this section, we will build a model based on DCGANs architecture for 256x256 data and implement a geometric loss function.

    <br><br>

    In the LIR problem contexts we defined, <code>extra information y</code> corresponds to an input polygon.
    The following <a href="https://github.com/PARKCHEOLHEE-lab/toytorch/blob/766853852c332c66decf3478eace9fd034c0602d/lirGAN/model.py#L235-L244">forward</a> propagation method takes two inputs: noise and input polygon.
    The <code>input_polygon</code> is first flattened and then this reshaped tensor is converted to feature space by passing through fully connected layers.

    <br><br>

    The output(128) of the linear transformation is concatenated with the noise(128) tensor. 
    This concatenation(256) allows the model to use both the random noise and the information from the <code>input_polygon</code> to generate the output.
    
<pre><code class="python">
    class LirGenerator(nn.Module, ModelConfig):

        ( ... )

        def forward(self, noise, input_polygon):
            fc = self.linear(input_polygon.reshape(input_polygon.shape[0], -1))
            x = torch.cat([noise, fc], dim=1)
            x = x.reshape(x.shape[0], 256, 1, 1)
            x = self.main(x)

            if self.use_tanh:
                return nn.Tanh()(x)

            return nn.Sigmoid()(x)
</pre></code><br><br>

    Similarly, the <a href="https://github.com/PARKCHEOLHEE-lab/toytorch/blob/766853852c332c66decf3478eace9fd034c0602d/lirGAN/model.py#L247C10-L272">discriminator</a> takes the input polygon as additional input. 
    In the forward propagation method of the discriminator, the <code>rectangle</code> and <code>input_polygon</code> have the same shape as the tensor. 
    So we just need to connect them and then pass them to the main layer.

    <pre><code class="python">
        class LirDiscriminator(nn.Module, ModelConfig):
            def __init__(self):
                super().__init__()
        
                self.main = nn.Sequential(
                    nn.Conv2d(2, 64, kernel_size=4, stride=2, padding=1, bias=False),
                    nn.LeakyReLU(0.2, inplace=True),
                    nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1, bias=False),
                    nn.BatchNorm2d(128),
                    nn.LeakyReLU(0.2, inplace=True),
                    nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1, bias=False),
                    nn.BatchNorm2d(256),
                    nn.LeakyReLU(0.2, inplace=True),
                    nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1, bias=False),
                    nn.BatchNorm2d(512),
                    nn.LeakyReLU(0.2, inplace=True),
                    nn.Conv2d(512, 1, kernel_size=4, stride=2, padding=0, bias=False),
                    nn.AdaptiveAvgPool2d(1),
                    nn.Sigmoid(),
                )
        
                self.to(self.DEVICE)

            def forward(self, rectangle, input_polygon):
                x = torch.cat([rectangle, input_polygon], dim=1)
                return self.main(x).view(-1, 1).squeeze(1)
    </pre></code><br><br>
    
    Next, let us define the <a href="https://github.com/PARKCHEOLHEE-lab/toytorch/blob/766853852c332c66decf3478eace9fd034c0602d/lirGAN/model.py#L55C7-L168">additional loss functions</a> that compute the geometric features of generated data. I think these losses will help train the generator stably.
    
    The losses consist of:
    
    <ul style="padding-left: 2em;">
        <li class="decimal"><u>BCE loss</u> is a standard loss function used for binary classification tasks(conventional GANs do not use any loss other than the adversarial loss, but without these losses when training the generator, the model did not create rectangles) </li>
        <li class="decimal"><u><a href="https://deep-learning-study.tistory.com/634">DIoU</a> loss</u>, or Distance Intersection over Union Loss, is a metric used to evaluate the similarity between two boxes</li>
        <li class="decimal"><u>Feasibility loss</u> measures how well the generated rectangle fits within the input polygon(and target rectangle) without overextending beyond its boundaries or underfitting within them</li>
        <li class="decimal"><u>Connectivity loss</u> checks whether the generated rectangle is a single piece using the labeling function</li>
    </ul>
    
    <br><br>
    We have now finished defining the base models and loss functions. Let's train the model using only one data point to check whether it has a structure that can be learned.

    <figure>
        <img src="../img/with-geometric-loss-polygons-2000.gif" style="width: 90%; display: block; margin-left: auto; margin-right: auto;">
        <figcaption style="text-align: center; margin-top: 1em">
            The process of training one data for 2000 epochs<br>
            From the left, ground truth · training process
        </figcaption>
    </figure>

</div><br>

<h3>Training and evaluating</h3>
<div class="article">
    Now, let's proceed with <a href="https://github.com/PARKCHEOLHEE-lab/toytorch/blob/main/lirGAN/lirGAN.ipynb">training</a> using the 5,000 datasets we have prepared. 
    The configuration for training the models is as follows:
    
<pre><code class="python">
    """Train models with Geometric loss and Tanh using all data"""
    """Test base models using all data"""

    lir_dataset = LirDataset()
    lir_dataloader = DataLoader(
        dataset=lir_dataset,
        batch_size=32,
        shuffle=True,
        drop_last=True,
    )

    lir_generator = LirGenerator(use_tanh=True)
    lir_discriminator = LirDiscriminator()

    lir_geometric_loss_function = LirGeometricLoss(
        bce_weight=1.0, diou_weight=0.5, feasibility_weight=0.01, connectivity_weight=0.01
    )

    lir_gan_trainer = LirGanTrainer(
        epochs=1000,
        lir_generator=lir_generator,
        lir_discriminator=lir_discriminator,
        lir_dataloader=lir_dataloader,
        lir_geometric_loss_function=lir_geometric_loss_function,
        initial_weights_key=ModelConfig.XAVIER,
        log_interval=1,
        use_gradient_penalty=True,
        use_lr_scheduler=True,
        is_record=True,
        record_name="with-geometric-loss-all-data",
    )

    lir_gan_trainer.set_seed()
    lir_gan_trainer.train()
</pre></code><br><br>

I set the epochs and batch sizes to 1000 and 32, respectively. 
Additionally, to qualitatively measure the quality of the generated data, I have configured the function to visualize the generated data at each epoch, as demonstrated in the following figure, when the given <code>log_interval</code> is reached.

The following figures show the training process corresponding to the above configuration:


<figure>
    <img src="../img/geometric-losses-17.gif" style="width: 100%; display: block; margin-left: auto; margin-right: auto;">
    <img src="../img/geometric-losses-18.gif" style="width: 100%; display: block; margin-left: auto; margin-right: auto;">
    <figcaption style="text-align: center; margin-top: 1em">
        Training process <br>
        The left 3 datasets are not included in the dataloader, whereas the right 3 are.
    </figcaption>
</figure><br>

<!-- 약 200 epochs 정도부터는 더이상 손실값이 감소하지 않는 것을 볼 수 있습니다. 이쯤에서 학습을 멈추고 (700epochs) 모델을 로드해서 정성적 평가를 해봅시다. 이번에는 ground truth를 함께 출력하겠습니다.-->
At approximately 200 epochs, the loss no longer decreases. Let's stop training at this point (700 epochs) and load the model for qualitative evaluation. This time, we will print the ground truth together.

<br><br>

<figure>
    <img src="../img/generated-0.png" style="width: 70%; display: block; margin-left: auto; margin-right: auto;">
    <img src="../img/truth-0.png" style="width: 70%; display: block; margin-left: auto; margin-right: auto;">
    <figcaption style="text-align: center; margin-top: 1em">
        Evaluating the generator <br>
        From the top, generated data · ground truth <br>
        The left 3 datasets are not included in the dataloader, whereas the right 3 are.
    </figcaption>
</figure><br>

<figure>
    <img src="../img/generated-1.png" style="width: 70%; display: block; margin-left: auto; margin-right: auto;">
    <img src="../img/truth-1.png" style="width: 70%; display: block; margin-left: auto; margin-right: auto;">
    <figcaption style="text-align: center; margin-top: 1em">
        Evaluating the generator <br>
        From the top, generated data · ground truth <br>
        The left 3 datasets are not included in the dataloader, whereas the right 3 are.
    </figcaption>
</figure><br>

<figure>
    <img src="../img/generated-2.png" style="width: 70%; display: block; margin-left: auto; margin-right: auto;">
    <img src="../img/truth-2.png" style="width: 70%; display: block; margin-left: auto; margin-right: auto;">
    <figcaption style="text-align: center; margin-top: 1em">
        Evaluating the generator <br>
        From the top, generated data · ground truth <br>
        The left 3 datasets are not included in the dataloader, whereas the right 3 are.
    </figcaption>
</figure><br>

<figure>
    <img src="../img/generated-3.png" style="width: 70%; display: block; margin-left: auto; margin-right: auto;">
    <img src="../img/truth-3.png" style="width: 70%; display: block; margin-left: auto; margin-right: auto;">
    <figcaption style="text-align: center; margin-top: 1em">
        Evaluating the generator <br>
        From the top, generated data · ground truth <br>
        The left 3 datasets are not included in the dataloader, whereas the right 3 are.
    </figcaption>
</figure><br>

<figure>
    <img src="../img/generated-4.png" style="width: 70%; display: block; margin-left: auto; margin-right: auto;">
    <img src="../img/truth-4.png" style="width: 70%; display: block; margin-left: auto; margin-right: auto;">
    <figcaption style="text-align: center; margin-top: 1em">
        Evaluating the generator <br>
        From the top, generated data · ground truth <br>
        The left 3 datasets are not included in the dataloader, whereas the right 3 are.
    </figcaption>
</figure><br>

<figure>
    <img src="../img/generated-5.png" style="width: 70%; display: block; margin-left: auto; margin-right: auto;">
    <img src="../img/truth-5.png" style="width: 70%; display: block; margin-left: auto; margin-right: auto;">
    <figcaption style="text-align: center; margin-top: 1em">
        Evaluating the generator <br>
        From the top, generated data · ground truth <br>
        The left 3 datasets are not included in the dataloader, whereas the right 3 are.
    </figcaption>
</figure><br>

<figure>
    <img src="../img/generated-6.png" style="width: 70%; display: block; margin-left: auto; margin-right: auto;">
    <img src="../img/truth-6.png" style="width: 70%; display: block; margin-left: auto; margin-right: auto;">
    <figcaption style="text-align: center; margin-top: 1em">
        Evaluating the generator <br>
        From the top, generated data · ground truth <br>
        The left 3 datasets are not included in the dataloader, whereas the right 3 are.
    </figcaption>
</figure><br>

<figure>
    <img src="../img/generated-7.png" style="width: 70%; display: block; margin-left: auto; margin-right: auto;">
    <img src="../img/truth-7.png" style="width: 70%; display: block; margin-left: auto; margin-right: auto;">
    <figcaption style="text-align: center; margin-top: 1em">
        Evaluating the generator <br>
        From the top, generated data · ground truth <br>
        The left 3 datasets are not included in the dataloader, whereas the right 3 are.
    </figcaption>
</figure><br>

<figure>
    <img src="../img/generated-8.png" style="width: 70%; display: block; margin-left: auto; margin-right: auto;">
    <img src="../img/truth-8.png" style="width: 70%; display: block; margin-left: auto; margin-right: auto;">
    <figcaption style="text-align: center; margin-top: 1em">
        Evaluating the generator <br>
        From the top, generated data · ground truth <br>
        The left 3 datasets are not included in the dataloader, whereas the right 3 are.
    </figcaption>
</figure>
</div><br>

<h3>Conclusions & future works</h3>
<div class="article">

    As you can see above, it produces perfect results for the data it was trained on. 
    That's why there was no further decline in losses. Performance on test data is not very good. The thing interesting is that when a relatively simple polygon is received as input, it produces results that are similar to the ground truth.

    <br><br>

    Also when defining the loss function, I intuitively thought that incorporating a geometric loss function would aid in <code>generalizing</code> the model. 
    I experimented with two versions: one using BCELoss with geometric loss and another using BCELoss without geometric loss. 
    However, the additional loss functions seem to have no effect. 
    
    <br><br>

    I think we could try the following:
    
    <br><br>
    <ul style="padding-left: 2em;">
        <li>Increasing data volume:</li>
        <ul style="list-style-type: none;">
            Since we have <a href="https://github.com/PARKCHEOLHEE-lab/toytorch/blob/main/lirGAN/data/data_creator.py#L101-L158">DataCreator</a> class and it is created by LIR algorithm, so we can increase the volume of the dataset as much as you want.
            Therefore, I think it is worth increasing the amount of training data, making the model structure more complex, overfitting, and checking whether the model generalizes.
        </ul>
        <br>
        <li>Experimenting with a dataset that has fewer vertices and simpler geometry:</li>
        <ul style="list-style-type: none;">
            In this project, I generated training data by creating random geometries with a number of vertices between <code>3 and 25</code> and creating the Largest Inscribed Rectangle for them.
            That's why polygon datasets are so complex, as seen in the <code>Data preparation</code> part.
            Therefore, I think it is necessary to simplify and generalize the data for training and then train the model.
        </ul>
    </ul>

</div><br>

<h3>References</h3>
<div class="article">

    <ul>
        <li><a href="https://towardsdatascience.com/a-brief-introduction-to-geometric-deep-learning-dae114923ddb">https://towardsdatascience.com/a-brief-introduction-to-geometric-deep-learning-dae114923ddb</a></li>
        <li><a href="https://deep-learning-study.tistory.com/634">https://deep-learning-study.tistory.com/634</a></li>
        <li><a href="https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html">https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html</a></li>
        <li><a href="https://www.kaggle.com/code/arturlacerda/pytorch-conditional-gan">https://www.kaggle.com/code/arturlacerda/pytorch-conditional-gan</a></li>
        <li><a href="https://arxiv.org/pdf/1411.1784.pdf">https://arxiv.org/pdf/1411.1784.pdf</a></li>
        <li><a href="https://deep-learning-study.tistory.com/634">https://deep-learning-study.tistory.com/634</a></li>
    </ul>
    
</div><br><br>