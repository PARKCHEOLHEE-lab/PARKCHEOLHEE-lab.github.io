---
title: "Drafting with AI"
layout: post
hashtag: "#stable-diffusion #controlnet #drawing #rendering"
featured: true
---

<div id="toc"></div>
<h3>Stable diffusion 🤖</h3>
<div class="article">
    <a href="https://en.wikipedia.org/wiki/Stable_Diffusion">Stable Diffusion</a> is a deep learning, text-to-image model released in 2022. 
    It is primarily used to generate detailed images conditioned on text descriptions, though it can also be applied to other tasks such as inpainting, outpainting, and generating image-to-image translations guided by a text prompt.

    <figure>
        <img src="/img/d2r-0.png" width="40%">
        <figcaption>
            An image generated by Stable Diffusion based on the text prompt: <br>"a photograph of an astronaut riding a horse"
        </figcaption>
    </figure><br>

    <!--break-->
    If you use this AI to create images, you can get plausible rendered images easily and shortly with a few lines of text.
    Let's think about how this can be utilized in the field of architecture.


</div><br><br>
<h3>Application in the field of architecture</h3>
<div class="article">
    There are several rendering engines that are frequently used in the field of architectural design, but they require precise modeling for good rendering.
    At the start of a design project, we can throw a very rough model or sketch to the AI, and type in the prompts to get a rough idea of what we're envisioning. 

    <br><br>
    First, let's see the simple example. 
    The example below is an example of capturing the rhino viewport in Rhino's <code>GhPython</code> environment 
    and then rendering the image in the desired direction using <code>ControlNet of the stable diffusion API</code>.
    <br><br>
    <figure>
        <img src="/img/d2r-4.png" width="100%">
        <figcaption>Using stable diffusion in Rhino environment</figcaption>
    </figure><br>

    <figure style="display: flex;">
        <img src="/img/d2r-3.png" width="31%">
        <img src="/img/d2r-1.png" width="31%">
        <img src="/img/d2r-2.png" width="31%">
    </figure>
    <figcaption>
        From the left, Rhino viewport · Controlnet scribble · Rendered image with stable diffusion<br>
        Prompt: <u>Interior view with sunlight, Curtain wall with city view, Colorful sofas, Cushions on the sofas, Transparent glass table, Fabric stools, Some flower pots</u>
    </figcaption><br><br>

    First, We need to write code for capturing the rhino viewport.
    I made it to be saved in the path where the current *.gh file is located.

<pre class="highlight">
    def <span class="defName">capture_activated_viewport</span>(
        self, save_name="draft.png", return_size=False
    ):
        <span class="blue">"""
            Capture and save the currently activated viewport 
            to the location of the current *.gh file path
        """</span>
        
        save_path = os.path.join(<span class="method">CURRENT_DIR</span>, save_name)
        
        viewport = <span class="method">Rhino.RhinoDoc.ActiveDoc.Views.ActiveView.CaptureToBitmap</span>()
        viewport.<span class="method">Save</span>(save_path, Imaging.ImageFormat.Png)
        
        if return_size:
            return save_path, viewport.Size
        
        return save_path
</pre><br>

Next, we should to run our local API server after cloning <code>stable-diffusion-webui</code> repository. 
Please refer this <a href="https://github.com/AUTOMATIC1111/stable-diffusion-webui">AUTOMATIC1111/stable-diffusion-webui</a> 
repository for settings required for running local API server. When all settings are done, now you can request the methods you want through API calls.
I referred API guide at <a href="https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/API">AUTOMATIC1111/stable-diffusion-webui/wiki/API</a>
</div><br><br>
<h3>Drafting</h3>
<div class="article">
    Python serves a built-in module called <code>urllib</code> to use HTTP. 
    You can check the code to the API request used in the <code>GhPython</code> at this <a href="https://github.com/PARKCHEOLHEE-lab/stable-diffusion-webui/blob/5eead22e36b1293824a1005915ee1ea24ef9538b/d2r.py#L182-L219">link</a>.
    The below figure is the demo for <code>drafting with AI</code> by calling API within Grasshopper environment.

    <figure>
        <img src="/img/d2r-5.gif" width="100%">
        <figcaption>
            Demo within Grasshopper<br>
            Prompt: <u>Colorful basketball court, Long windows, Sunlight</u>
        </figcaption>
    </figure><br>

    <figure style="display: flex;">
        <img src="/img/d2r-5.png" width="46.5%">
        <img src="/img/d2r-6.png" width="46.5%">
    </figure>
    <figcaption>
        Isometric model <br>
        From the left, Drfat view · Rendered view <br>
        Prompt: <u>Isometric view, River, Trees, 3D printed white model with illumination</u>
    </figcaption><br>

    <figure style="display: flex;">
        <img src="/img/d2r-7.png" width="46.5%">
        <img src="/img/d2r-8.png" width="46.5%">
    </figure>
    <figcaption>
        Skyscrappers in the city <br>
        From the left, Drfat view · Rendered view <br>
        Prompt: <u>
            Skyscrapers in the city,
            Night scene with many stars in the sky,
            Neon sign has shine brightly,
            Milky way in the sky
        </u>
    </figcaption><br>

</div>
<br><br>