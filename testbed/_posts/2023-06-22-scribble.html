---
title: "Render your modeling with AI"
layout: post
hashtag: "#automation #algorithm #open-cv"
done: false
---

<div id="toc"></div>
<h3>Stable diffusion</h3>
<div class="article">
    <a href="https://en.wikipedia.org/wiki/Stable_Diffusion">Stable Diffusion</a> is a deep learning, text-to-image model released in 2022. 
    It is primarily used to generate detailed images conditioned on text descriptions, though it can also be applied to other tasks such as inpainting, outpainting, and generating image-to-image translations guided by a text prompt.

    <figure>
        <img src="/img/d2r-0.png" width="30%">
        <figcaption>
            An image generated by Stable Diffusion based on the text prompt: <br>"a photograph of an astronaut riding a horse"
        </figcaption>
    </figure><br>

    <!--break-->
    If you use this AI to create images, you can get plausible rendered images easily and shortly with a few lines of text.
    Let's think about how this can be utilized in the field of architecture.


</div><br><br>
<h3>Application in the field of architecture</h3>
<div class="article">
    There are several rendering engines that are frequently used in the field of architectural design, but they require precise modeling for good rendering.
    At the start of a design project, we can throw a very rough model or sketch to the AI, and type in the prompts to get a rough idea of what we're envisioning. 

    <br><br>
    First, let's see the simple example. 
    The example below is an example of capturing the rhino viewport in Rhino's <code>GhPython</code> environment 
    and then rendering the image in the desired direction using <code>ControlNet of the stable diffusion API</code>.
    <br><br>
    <figure>
        <img src="/img/d2r-4.png" width="70%">
        <figcaption>Using stable diffusion in Rhino environment</figcaption>
    </figure><br>

    <figure style="display: flex;">
        <img src="/img/d2r-3.png" width="31%">
        <img src="/img/d2r-1.png" width="31%">
        <img src="/img/d2r-2.png" width="31%">
    </figure>
    <figcaption>
        From the left, Rhino viewport · Controlnet scribble · Rendered image with stable diffusion<br><br>
        Prompt: 
        <br>Interior view with sunlight, 
        <br>Curtain wall with city view, 
        <br>Colorful sofas, 
        <br>Cushions on the sofas, 
        <br>Transparent glass table, 
        <br>Fabric stools, 
        Some flower pots
    </figcaption><br><br>

    First, We need to write code for capturing the rhino viewport.
    I made it to be saved in the path where the current *.gh file is located.

<pre class="highlight">
    def <span class="defName">capture_activated_viewport</span>(
        self, save_name="draft.png", return_size=False
    ):
        <span class="blue">"""
            Capture and save the currently activated viewport 
            to the location of the current *.gh file path
        """</span>
        
        save_path = os.path.join(CURRENT_DIR, save_name)
        
        viewport = <span class="method">Rhino.RhinoDoc.ActiveDoc.Views.ActiveView.CaptureToBitmap</span>()
        viewport.<span class="method">Save</span>(save_path, Imaging.ImageFormat.Png)
        
        if return_size:
            return save_path, viewport.Size
        
        return save_path
</pre><br>

Next, we should to run our local API server after cloning <code>stable-diffusion-webui</code> repository. 
Please refer this <a href="https://github.com/AUTOMATIC1111/stable-diffusion-webui">AUTOMATIC1111/stable-diffusion-webui</a> 
repository for settings required for running local API server.

<br><br>
When all settings are done, you can request method  


<!-- 
    사용하고 싶은 기능을 요청하면 됩니다.
    설정을 모두 마쳤다면, AUTOMATIC1111/stable-diffusion-webui/wiki/API 문서를 통해 api를 호출하기만 하면 됩니다.
    https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/API

-->

</div>
