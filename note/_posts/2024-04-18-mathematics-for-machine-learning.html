---
title:  "Mathematics for Machine Learning"
layout: post
done: true
emoji: /emoji/books.png
---

<style>
    .mml-example {
        background-color: rgb(245, 245, 245);
        padding: 1em;
        overflow: auto;
    }
</style>

<br>

<!-- <ul>
    <img src="/img/mathematics-for-machine-learning-1.jpg" width="20%">
    <br>
    <li>
        <a href="https://mml-book.github.io/">https://mml-book.github.io/</a>
    </li>
    <li>
        <a href="https://mml-book.github.io/book/mml-book.pdf">https://mml-book.github.io/book/mml-book.pdf</a>
    </li>
</ul> -->


<div id="toc"></div>

<ul>
    <img src="/img/mathematics-for-machine-learning/mathematics-for-machine-learning-1.jpg" width="20%" style="box-shadow: 0 4px 8px rgba(0, 0, 0, 0.2);" onerror=handle_image_error(this)>
    <br>
    <li>
        <a href="https://mml-book.github.io/">Mathematics for Machine Learning</a>, Marc Peter Disenroth, A. Aldo Faisal, Cheng Soon Ong
    </li>
</ul>

<br>

<h3>1. Introduction</h3>
<div class="article">
    We believe that the mathematical foundations of machine learning 
    are important in order to understand fundamental principles upon
    which more complicated machine learning systems are built. 
    Understanding these principles can facilitate creating new machine learning solutions,
    understanding and debugging existing approaches, and learning about the
    inherent assumptions and limitations of the methodologies we are working with.

    <br><br>

    A model is typically used to describe a process for generating data, similar 
    to the dataset at hand. Therefore, good models can also be thought
    of as simplified versions of the real (unknown) data-generating process,
    capturing aspects that are relevant for modeling the data and extracting
    hidden patterns from it. 

    <br><br>

    Let us summarize the main concepts of machine learning that we cover in this book
    <ul>
        <li>
            We represent data as vectors
        </li>
        <li>
            We choose an appropriate model, either using the probabilistic or optimization view
        </li>
        <li>
            We learn from available data by using numerical optimization methods
            with the aim that the model performs well on data not used for training.
        </li>
    </ul>
</div><br><br>

<h3>2. Linear Algebra</h3>
<div class="article">

    Linear algebra is the study of vectors and certain rules to manipulate vectors.
    The vectors many of us know from school are
    called "geometric vectors", which are usually denoted by a small arrow
    above the letter, e.g., \(\overrightarrow{x}\) and \(\overrightarrow{y}\).
    Interpreting vectors as geometric vectors 
    enables us to use our intuitions about direction and magnitude to
    reason about mathematical operations.

    <br><br>

    Here are some examples of such vector objects:
    <ol>
        <li>
            <strong>Geometric vectors</strong>.
            Two geometric vectors \(\overrightarrow{\boldsymbol{x}}\), \(\overrightarrow{\boldsymbol{y}}\)
            can be added, such that \(\overrightarrow{\boldsymbol{x}} + \overrightarrow{\boldsymbol{y}} = \overrightarrow{\boldsymbol{z}}\)
            is another geometric vector.
            Furthermore, multiplication by a scalar \(\lambda \overrightarrow{\boldsymbol{x}}, \lambda \in \mathbf{R} \),
            is also a geometric vector.

            Interpreting vectors as geometric vectors 
            enables us to use our intuitions about direction and magnitude to
            reason about mathematical operations.
        </li>
        <li>
            <strong>Polynomials are also vectors</strong>. 
            Two polynomials can be added together,  which results in another polynomial; and they can
            be multiplied by a scalar \(\lambda \in \mathbb{R}\) and the result is a polynomial as well.
            Note that polynomials are very different from geometric vectors. While
            geometric vectors are concrete “drawings”, polynomials are abstract
            concepts. However, they are both vectors in the sense.
        </li>
        <li>
            <strong>Audio signals are vectors</strong>. 
            Audio signals are represented as a series of numbers.
            We can add audio signals together, and their sum is a new
            audio signal. If we scale an audio signal, we also obtain an audio signal.
        </li>
        <li>
            <strong>Elements of \(\mathbb{R}^n\) are vectors</strong> (tuples of \(n\) real numbers).
            \(\mathbb{R}^n \) is more abstract than polynomials, and it is the concept we focus on in this book. For instance,
            
            \[
                \,\\
                \boldsymbol{a} = 
                \begin{bmatrix}
                1 \\
                2 \\
                3
                \end{bmatrix}
                \in \mathbb{R}^3
                \,\\
            \]

            Adding two vectors \(\boldsymbol{a}, \boldsymbol{b} \in \mathbb{R}^n \)
            component-wise results in another vector: \(\boldsymbol{a}+\boldsymbol{b}=\boldsymbol{c} \in \mathbb{R}^n \).
            Moreover, multiplying \(\boldsymbol{a} \in \mathbb{R}^n \) by \(\lambda \in \mathbb{R} \) results in a scaled vector \(\lambda a \in \mathbb{R}^n\).
            Considering vectors as elements of \(\mathbb{R}^n\) has an additional benefit that
            it loosely corresponds to arrays of real numbers on a computer. 
            Many programming languages support array operations, which allow for convenient implementation of algorithms that involve vector operations.
        </li>
    </ol>
    <br>

    Linear algebra focuses on the similarities between these vector concepts.
    We will largely focus on vectors in \(\mathbb{R}^n\) since most algorithms in linear algebra are formulated in \(\mathbb{R}^n\)

    <br><br>

    One major idea in mathematics is the idea of "closure". 
    This is the question: 
    What is the set of all things that can result from my proposed operations? 
    In the case of vectors: 
    What is the set of vectors that can result by
    starting with a small set of vectors, and adding them to each other and
    scaling them? This results in a vector space. The concept of
    a vector space and its properties underlie much of machine learning.

    <br><br>

    Linear algebra plays an important role in machine learning and 
    general mathematics. The concepts introduced in this chapter are further expanded to include the idea of geometry.
    <br><br>
    <figure>
        <img src="/img/mathematics-for-machine-learning/mathematics-for-machine-learning-2.png" width="80%" onerror=handle_image_error(this)>
        <figcaption>
            A mind map of the concepts where Linear Algebra is used in other parts
            
        </figcaption>
    </figure>

    <br><br>

    <h4>2.1 Systems of Linear Equations</h4>
    Systems of linear equations play a central part of linear algebra. Many
    problems can be formulated as systems of linear equations, and linear
    algebra gives us the tools for solving them.

    <br><br>
    <div class="mml-example">
        <strong>Example 2.2</strong>
        <br>
        The system of linear equations
        \[
            \,\\
            \begin{matrix}
            x_1  & + & x_2 & + & x_3  & = & 3 & (1) \\
            x_1  & - & x_2 & + & 2x_3 & = & 2 & (2) \\
            2x_1 &   &     & + & 3x_3 & = & 1 & (3)
            \end{matrix}
            \,\\
        \]
        has no solution: Adding the first two equations yields \(2x_1+3_3 = 5 \),
        which constradicts the third equation \((3)\).
        <br><br>

        Let us have a look at the system of linear equations
        \[
            \,\\
            \begin{matrix}
            x_1 & + & x_2 & + & x_3  & = & 3 & (1) \\
            x_1 & - & x_2 & + & 2x_3 & = & 2 & (2) \\
                &   & x_2 & + & x_3  & = & 2 & (3)
            \end{matrix}
            \,\\
        \]
        From the first and third equation, it follows that \(x_1 = 1\).
        From \((1)+(2)\), we get \(2x_1 + 3x_3 = 5 \), i.e., \(x_3 = 1\).
        From \((3)\), we then get that \(x_2 = 1\).
        Therefore \((1,1,1)\) is the only possible and unique solution.

        <br><br>

        As a third example, we consider
        \[
            \,\\
            \begin{matrix}
            x_1  & + & x_2 & + & x_3  & = & 3 & (1) \\ 
            x_1  & - & x_2 & + & 2x_3 & = & 2 & (2) \\
            2x_1 &   &     & + & 3x_3 & = & 5 & (3)
            \end{matrix}
            \,\\
        \]
        Since \((1)+(2)=(3)\), we can obmit the third equation.
        From \((1)\) and \((2)\), we get \(2_x1 = 5-3x_3 \) and \(2x_2 = 1 + x_3\).
        We can define \(x_3 = a \in \mathbb{R}\) as a free variable, 
        such that any triplet is a solution of the system of linear equations, i.e., we can obtain a
        solution set that contains infinitely many solutions.
        \[
            \,\\
            \left( \frac{5}{2}-\frac{3}{2}a, \quad \frac{1}{2}+\frac{1}{2}a, \quad a \right)
            \,\\
        \]
    </div>
    <br><br>

    In general, for a real-valued system of linear equations we obtain either no, exactly one, or infinitely many solutions. 

    <br><br>

    <strong><i>Remark</i></strong> (Geometric Interpretation of Systems of Linear Equations).
    In a system of linear equations with two variables \(x_1, x_2\), each linear equation
    define a line on the \(x_1, x_2\text{-plane}\).
    Since a solution to a system of linear
    equations must satisfy all equations simultaneously, the solution set is the
    intersection of these lines.
    This intersection set can be a line (if the linear
    equations describe the same line), a point, or empty (when the lines are
    parallel).
    \[
        \,\\
        \begin{align}
        4x_1 + 4x_2 &= 5 \\
        2x_1 - 4x_2 &= 1
        \end{align}
        \,\\
    \]

    For the above system, we can get geometric intuition of the solution by plotting both equations as lines in the \(x_1, x_2\)-plane. 
    The point where the two lines intersect represents the unique solution to the system.

    <br><br>
    <figure>
        <img src="/img/mathematics-for-machine-learning/mathematics-for-machine-learning-3.png" width="50%" onerror=handle_image_error(this)>
        <figcaption>
            The solution space of a system of two linear equations with two variables
            <br>
            can be geometrically interpreted as the intersection of two lines
        </figcaption>
    </figure>

    <br><br>

    <h4>2.2 Matrices</h4>
    Matrices play a central role in linear algebra. They can be used to compactly represent systems of linear equations,
    but they also represent linear functions (linear mappings).

    <br><br>

    <strong>Definition 2.1 (Matrix)</strong>. With \(m, n \in \mathbb{N} \) a real-valued \((m,n)\) matrix \(A\) is an
    \(m\cdot n\)-tuple of elements \(a_{ij}, i=1, ..., m, \, j=1, ..., n\), which ordered
    according to a rectangular scheme consisting of \(m\) rows and \(n\) columns:
    \[
        \,\\
        A = 
        \begin{bmatrix}
        a_{11} & a_{12} & \cdots & a_{1n} \\
        a_{21} & a_{22} & \cdots & a_{2n} \\
        \vdots & \vdots &        & \vdots \\
        a_{m1} & a_{m2} & \cdots & a_{mn} 
        \end{bmatrix}
        ,\quad  a_{ij} \in \mathbb{R}
        \,\\
    \]

    \((1, n)\) or \((n, 1)\) matrices are called row/column vectors.
    \(\mathbb{R}^{m \times n}\) is the set of all real-valued \((m,n)\)-matrices. \(A\in \mathbb{R}^{m \times n} \) can be
    equivalently represented as \(a \in \mathbb{R}^{mn} \) by stacking all \(n\) columns of the matrix into a long vector.

    <br><br>

    <h4>2.2.1 Matrix Addition and Multiplication</h4>
    The sum of two matrices \(A \in \mathbb{R}^{m \times n} \), \(B \in \mathbb{R}^{m \times n} \) is defined as the element-wise sum.
    For matrices \(A \in \mathbb{R}^{m \times n} \), \(B \in \mathbb{R}^{n \times k} \), the elements of the product 
    \(C = AB \in \mathbb{R}^{m \times k} \) are computed as 
    \[
        \,\\
        c_{ij} = \sum_{l=1}^{n} a_{il} b_{lj}, \quad i = 1, ..., m, \quad j = 1, ..., k
        \,\\
    \]

    This means, to compute element \(c_{ij}\) we multipy the elements of the \(i\)th row of \(A\)
    with the \(j\)th column of \(B\) and sum them up. We will call this the dot product of the corresponding row and column.

    <br><br>
    
    <strong><i>Remark</i></strong>. Matrices can only be multiplied if their "neighboring" dimensions match.
    For instance, an \(n \times k \)-matrix \(A\) can be multiplied with a \(k \times m \)-matrix \(B\),
    but only from the left side:
    \[
        \,\\
        \underbrace{A}_{n \times k} \; \underbrace{B}_{k \times m} = \underbrace{C}_{n \times m}
        \,\\
    \]

    The product \(BA\) is not defined if \(m \neq n\) since the neighboring dimensions do not match.

    <br><br>

    <strong><i>Remark</i></strong>. Matrix multiplication is not defined as an element-wise operation on matrix elements,
    i.e., \(c_{ij} \neq a_{ij} b_{ij} \), and is called a Hadamard product.
    
    <br><br>
    <div class="mml-example">
        <strong>Example 2.3</strong>
        <br>
        For \(
            A = 
            \begin{bmatrix}
                1 & 2 & 3 \\
                3 & 2 & 1
            \end{bmatrix} \in \mathbb{R}^{2 \times 3}, \quad

            B = 
            \begin{bmatrix}
                0 & 2 \\
                1 & -1 \\
                0 & 1
            \end{bmatrix} \in \mathbb{R}^{3 \times 2}
        \),　we obtain

        \[
            \,\\
            AB = 
            \begin{bmatrix}
            1 & 2 & 3 \\
            3 & 2 & 1
            \end{bmatrix}
            \begin{bmatrix}
                0 & 2 \\
                1 & -1 \\
                0 & 1
            \end{bmatrix}
            =
            \begin{bmatrix}
                2 & 3 \\
                2 & 5 
            \end{bmatrix} \in \mathbb{R}^{2 \times 2}
            \,\\
            \,\\
            BA = 
            \begin{bmatrix}
            0 & 2 \\
            1 & -1 \\
            0 & 1
            \end{bmatrix}
            \begin{bmatrix}
            1 & 2 & 3 \\
            3 & 2 & 1
            \end{bmatrix}
            =
            \begin{bmatrix}
            6 & 4 & 2 \\
            -2 & 0 & 2 \\
            3 & 2 & 1
            \end{bmatrix} \in \mathbb{R}^{3 \times 3}
            \,\\
        \]
    </div>
    <br><br>

    From this example, we can already see that the matrix multiplication is not commutative, i.e., \(AB \neq BA\).

    <br><br>
    
    <strong>Definition 2.2 (Identity Matrix)</strong>. In \(\mathbb{R}^{n\times m} \), we define the identity matrix
    as the \(n \times m \)-matrix containing \(1\) on the diagonal and \(0\) everywhere else.

    \[
        \,\\
        I_n = 
        \begin{bmatrix}
        1      & 0 & \cdots & 0 & \cdots & 0 \\
        0      & 1 & \cdots & 0 & \cdots & 0 \\
        \vdots & \vdots & \ddots & \vdots & \ddots & \vdots \\
        0 & 0 & \cdots & 1 & \cdots & 0 \\
        \vdots & \vdots & \ddots & \vdots & \ddots & \vdots \\
        0 & 0 & \cdots & 0 & \cdots & 1
        \end{bmatrix} \in \mathbb{R}^{n\times n}
        \,\\
    \]

    Now that we defined matrix multiplication, matrix addition and the
    identity matrix, let us have a look at some properties of matrices: 

    <ul>
        <li>
            Associativity:
            \[
                \forall A \in \mathbb{R}^{m\times n}, \quad B \in \mathbb{R}^{n \times p}, \quad C \in \mathbb{R}^{p \times q}: \,\, (AB)C = A(BC)
            \]
        </li>
        <li>
            Distributivity:
            \[
                \begin{align}
                \forall A, B \in \mathbb{R}^{m \times n}, \quad C, D \in \mathbb{R}^{n \times p}:& \,\, (A+B)C = AC+BC \\
                                                                                                 & \,\, A(C+D) = AC+AD
                \end{align}
            \]
        </li>
        <li>
            Multiplication with the identity matrix:
            \[
                \forall A \in \mathbb{R}^{m \times n}: \,\, I_m A = AI_n = A
            \]
        </li>
    </ul>
    <br><br>

    <h4>2.2.2 Inverse and Transpose</h4>
    <strong>Definition 2.3 (Inverse)</strong>. 
    Consider a square matrix \(A \in \mathbb{R}^{n \times n} \).
    Let matrix \(B \in \mathbb{R}^{n \times n} \) have he property that \(AB = I_n = BA\).
    \(B\) is called the inverse of \(A\) and denoted by \(A^{-1}\).

    <br><br>

    Unfortunately, not every matrix \(A\) possesses an inverse \(A^{-1}\). 
    If the inverse does exist, \(A\) is called regular/invertible/nonsingular, otherwise
    singular/noninvertible. When the matrix inverse exists, it is unique.

    <br><br>

    <strong><i>Remark</i></strong> (Existence of the Inverse of a \(2 \times 2\) matrix). Consider a matrix
    \[  
        \,\\
        A := 
        \begin{bmatrix}
        a_{11} & a_{12} \\
        a_{21} & a_{22} 
        \end{bmatrix} \in \mathbb{R}^{2 \times 2}
        \,\\
    \]

    If we multiply \(A\) with
    \[
        \,\\
        B := 
        \begin{bmatrix}
            a_{22} & -a_{12} \\
            -a_{21} & a_{11}
        \end{bmatrix}
        \,\\
    \]

    we obtain
    \[
        \,\\
        \begin{align}
        AB = 
        \begin{bmatrix}
        a_{11}a_{22} - a_{12}a_{21} & 0 \\
        0 & a_{11}a_{22} - a_{12}a_{21}
        \end{bmatrix}
        &= (a_{11}a_{22} - a_{12}a_{21}) \, I  \\
        &= \det (A) \, I
        \end{align}
        \,\\
    \]

    Therefore
    \[
        \,\\
        A^{-1} = \frac{1}{a_{11}a_{22}-a_{12}a_{21}} 
        \begin{bmatrix}
        a_{22} & -a_{12} \\
        -a_{21} & a_{11}
        \end{bmatrix}
        \,\\
    \]

    if and only if \(\det(A) \neq 0\). Furthermore we can generally use the determinant to check whether a matrix is invertible.

    <br><br>
    <div class="mml-example">
        <strong>Example 2.4 (Inverse Matrix)</strong>
        <br>
        The matrices
        \[
            \,\\
            A = 
            \begin{bmatrix}
                1 & 2 & 1 \\
                4 & 4 & 5 \\
                6 & 7 & 7
            \end{bmatrix}
            , \quad
            B = 
            \begin{bmatrix}
                -7 & -7 & 6 \\
                2 & 1 & -1 \\
                4 & 5 & -4 
            \end{bmatrix}
            \,\\
            \,\\
            \begin{align}
            AB &= 
            \begin{bmatrix}
            -7+4+4   & -7+2+5   & 6-2-4 \\
            -28+8+20 & -28+4+25 & 24-4-20 \\
            -42+14+28 & -42+7+35 & 36-7-28
            \end{bmatrix}
            = 
            \begin{bmatrix}
            1 & 0 & 0 \\
            0 & 1 & 0 \\
            0 & 0 & 1
            \end{bmatrix}
            = I

            \,\\
            \,\\
            BA &= 
            \begin{bmatrix}
               -7-28+36 & -14-28+42 & -7-35+42 \\
                2+4-6   & 4+4-7     & 2+5-7 \\
                4+20-24 & 8+20-28   & 4+25-28
            \end{bmatrix}
            = 
            \begin{bmatrix}
            1 & 0 & 0 \\
            0 & 1 & 0 \\
            0 & 0 & 1
            \end{bmatrix}
            = I
            \end{align}
            \,\\
        \]

        are inverse to each other since \(AB = I = BA\)
    </div>

    <br><br>

    <strong>Definition 2.4 (Transpose)</strong>. For \(A \in \mathbb{R}^{m \times n} \)
    the matrix \(B \in \mathbb{R}^{n \times m} \) with \(b_{ij}=a_{ji} \) is called the transpose of \(A\).
    We write \(B = A^{\top} \).

    <br><br>

    In general, \(A^{\top}\) can be obtained by writing the columns of \(A\) as the rows of \(A^{\top}\).
    The following are some important properties of inverses and transposes:
    \[
        \,\\

        \begin{align}
        AA^{-1}           &= I = A^{-1}A \\
        \,\\
        (AB)^{-1}         &= B^{-1}A^{-1} \\
        \,\\
        (A+B)^{-1}        &\neq A^{-1} + B^{-1} \\
        \,\\
        (A^{\top})^{\top} &= A \\
        \,\\
        (A+B)^{\top}      &= A^{\top}+B^{\top} \\
        \,\\
        (AB)^{\top}       &= B^{\top} + A^{\top} 
        \end{align}

        \,\\
    \]

    <strong>Definition 2.5 (Symmetric Matrix)</strong>. A matrix \(A \in \mathbb{R}^{n \times n} \) is symmetric if
    \(A = A^{\top} \).

    <br><br>
    Note that, if \(A\) is invertible, then so is \(A^{\top}\), and \((A^{-1})^{\top} = (A^{\top})^{-1} =: A^{-\top} \).

    <br><br>

    <strong><i>Remark</i></strong> (Sum and Product of Symmetric Matrices). 
    The sum of symmetric matrices \(A, B \in \mathbb{R}^{n\times n} \) is always symmetric.
    However, although their product is always defined, it is generally not symmetric:
    \[
        \,\\
        \begin{bmatrix}
            1 & 0 \\
            0 & 0 
        \end{bmatrix}
        \begin{bmatrix}
            1 & 1 \\
            1 & 1
        \end{bmatrix}
        =
        \begin{bmatrix}
            1 & 1 \\
            0 & 0
        \end{bmatrix}
        \,\\
    \]

    <h4>2.2.3 Multiplication by a Scalar</h4>
    Let us look at what happens to matrices when they are multiplied by a scalar
    \(\lambda \in \mathbb{R}\). Let \(A \in \mathbb{R}^{m\times n} \). Then \(\lambda A = K, \, K_{ij} = \lambda a_{ij} \).
    Pratically, \(\lambda\) scales each element of \(A\). For \(\lambda, \psi \in \mathbb{R} \), the following holds:

    <ul>
        <li>
            Associativity:
            \[
                (\lambda \psi)C = \lambda(\psi C), C \in \mathbb{R}^{m \times n} \\
            \]
        </li>
        <li>
            \(\lambda(BC) = (\lambda B)C = B(\lambda C) = (BC)\lambda, \quad B \in \mathbb{R}^{m \times n},\quad C \in \mathbb{R}^{n \times k}\)
            <br>
            Note that this allows us to move scalar values around.
        </li>
        <li>
            \( (\lambda C)^{\top} = C^{\top}\lambda^{\top} = C^{\top}\lambda = \lambda C^{\top} \) since \(\lambda = \lambda^{\top}, \forall \lambda \in \mathbb{R}  \)
        </li>
        <li>
            Distributivity:
            \[
                (\lambda + \psi)C = \lambda C + \psi C, \quad C \in \mathbb{R}^{m \times n}
                \\
                \lambda (B+C) = \lambda B + \lambda C, \quad B, C \in \mathbb{R}^{m \times n}  
            \]
        </li>
    </ul>

    <br><br>
    
    <div class="mml-example">
        <strong>Example 2.5 (Distributivity)</strong>
        <br>
        If we define
        \[
            \,\\
            C := 
            \begin{bmatrix}
                1 & 2 \\
                3 & 4
            \end{bmatrix},
            \,\\
        \]
        then for any \(\lambda, \psi \in \mathbb{R}\) we obtain
        \[
            \,\\
            \begin{align}
            (\lambda + \psi)C 
            &= 
            \begin{bmatrix}
            (\lambda + \psi)1 & (\lambda + \psi)2 \\
            (\lambda + \psi)3 & (\lambda + \psi)4 
            \end{bmatrix}
            =
            \begin{bmatrix}
            \lambda + \psi & 2\lambda + 2\psi \\
            3\lambda + 3\psi & 4\lambda + 4\psi
            \end{bmatrix}
            \,\\
            \,\\
            &=
            \begin{bmatrix}
            \lambda & 2\lambda \\
            3\lambda & 4\lambda
            \end{bmatrix}
            +
            \begin{bmatrix}
            \psi & 2\psi \\
            3\psi & 4\psi
            \end{bmatrix}
            = \lambda C + \psi C
            \end{align}
            \,\\
        \]
    </div>

    <br><br>

    <h4>2.3 Solving Systems of Linear Equations</h4>
    <h4>2.3.1 Particular and General Solution</h4>
    Before discussing how to generally solve systems of linear equations, let
    us have a look at an example. Consider the system of equations
    \[
        \,\\
        \begin{bmatrix}
        1 & 0 & 8 & -4 \\
        0 & 1 & 2 & 12
        \end{bmatrix}
        \begin{bmatrix}
        x_1 \\
        x_2 \\
        x_3 \\
        x_4 
        \end{bmatrix}
        =
        \begin{bmatrix}
        42 \\
        8
        \end{bmatrix}
        \,\\
    \]

    The system has two equations and four unknowns. Therefore, in general
    we would expect infinitely many solutions. This system of equations is
    in a particularly easy form, where the first two columns consist of a \(1\)
    and a \(0\).
    A solution to the problem can be found
    found immediately by taking 42 times the first column and 8 times the
    second column so that
    \[
        \,\\
        b = 
        \begin{bmatrix}
        42 \\
        8
        \end{bmatrix}
        = 42
        \begin{bmatrix}
        1 \\
        0
        \end{bmatrix}
        + 8
        \begin{bmatrix}
        0 \\
        1
        \end{bmatrix}
        \,\\
    \]

    Therefore, a solution is \([42, 8, 0, 0]^{\top}\). This solution is called a particular
    solution or special solution. However, this is not the only solution of this system of linear equations.
    To capture all the other solutions, we need
    to be creative in generating 0 in a non-trivial way using the columns of
    the matrix
    \[
        \,\\

        c_3 = 
        \begin{bmatrix}
        8 \\
        2
        \end{bmatrix}
        = 8
        \begin{bmatrix}
        1 \\
        0
        \end{bmatrix}
        + 2
        \begin{bmatrix}
        0 \\
        1 
        \end{bmatrix}
        \,\\
    \]

    so that \(8c_1 + 2c_2 = 1c_3 \), and \( 8c_1 + 2c_2 - 1c_3 + 0c_4 = 0\).
    Therefore \((x_1, x_2, x_3, x_4) = (8, 2, -1, 0)\). In fact, any scaling of this solution
    by \(\lambda_1 \in \mathbb{R}\) produces the 0 vector, i.e.,
    \[
        \,\\
        \begin{bmatrix}
        1 & 0 & 8 & -4 \\
        0 & 1 & 2 & 12
        \end{bmatrix}
        \left(
        \lambda_1
        \begin{bmatrix}
        8 \\
        2 \\
        -1 \\
        0
        \end{bmatrix}
        \right)
        = \lambda_1(8c_1 + 2c_2-c_3) = 0
        \,\\
    \]

    Following the same line of reasoning, we express the fourth column of the
    matrix using the first two columns and generate another set as 
    \[
        \,\\
        \begin{bmatrix}
        1 & 0 & 8 & -4 \\
        0 & 1 & 2 & 12
        \end{bmatrix}
        \left(
        \lambda_2
        \begin{bmatrix}
        -4 \\
        12 \\
        0 \\
        -1
        \end{bmatrix}
        \right)
        = \lambda_2(-4c_1 + 12c_2-c_4) = 0
        \,\\
    \]

    for any \(\lambda \in \mathbb{R} \). Putting everthing together, we obtain all solutions of the
    equation system, which is called the general solution, as the set
    \[
        \,\\
        \left\{
            x \in \mathbb{R}^4 :
            x = 
            \begin{bmatrix}
                42 \\
                8 \\
                0 \\
                0
            \end{bmatrix}
            + \lambda_1
            \begin{bmatrix}
                8 \\
                2 \\
                -1 \\
                0
            \end{bmatrix}
            + \lambda_2
            \begin{bmatrix}
                -4 \\
                12 \\
                0 \\
                -1
            \end{bmatrix}
            ,\quad \lambda_1, \, \lambda_2 \in \mathbb{R}
        \right\}
        \,\\
    \]

    <strong><i>Remark</i></strong>. The general approach we followed consisted of the following three steps:
    <ol>
        <li>
            Find a particular solution to \(Ax = b\).
        </li>
        <li>
            Find all solutions to \(Ax = 0\).
        </li>
        <li>
            Combine the solutions from steps 1. and 2. to the general solution.
        </li>
    </ol>

    <br>
    The system of linear equations in the preceding example was easy to
    solve because the matrix.
    However, general equation systems are not of this simple form.
    Fortunately, there exists a constructive algorithmic way of transforming
    any system of linear equations into this particularly simple form: Gaussian
    elimination. 
    Key to Gaussian elimination are elementary transformations
    of systems of linear equations, which transform the equation system into
    a simple form. Then, we can apply the three steps to the simple form

    <br><br>
    
    <h4>2.3.2 Elementary Transformations</h4>
    <ul>
        <li>
            Exchange of two equations (rows in the matrix representing the system
            of equations)
        </li>
        <li>
            Multiplication of an equation (row) with a constant \(\lambda \in \mathbb{R} \setminus \{0\} \)
        </li>
        <li>
            Addition of two equations(rows)
        </li>
    </ul>

    <br>

    <div class="mml-example">
        <strong>Example 2.6</strong>
        <br>
        For \(a \in \mathbb{R}\), we seek all solutions of the following system of equations:
        \[
            \,\\
            \begin{array}{*{11}{r}} 
            -2x_1 & + & 4x_2 & - & 2x_3 & - &  x_4 & + & 4x_5 & = & -3 \\
             4x_1 & - & 8x_2 & + & 3x_3 & - & 3x_4 & + &  x_5 & = & 2 \\
              x_1 & - & 2x_2 & + &  x_3 & - &  x_4 & + &  x_5 & = & 0 \\
              x_1 & - & 2x_2 &   &      & - & 3x_4 & + & 4x_5 & = & a 
            \end{array}
            \,\\
        \]

        the system can be converted into compact matrix \(Ax = b\) as an augmented matrix (in the form \([A \mid b]\))
        \[
            \left[
            \begin{array}{rrrrr|r}
                -2 & 4 & -2 & -1 & 4 & -3 \\
                4 & -8 & 3 & -3 & 1 & 2 \\
                 1 & -2 & 1 & -1 & 1 & 0 \\
                 1 & -2 & 0 & -3 & 4 & a
            \end{array}
            \right]
            \begin{array}{l}
                \text{Swap with } R_3 \\
                \\
                \text{Swap with } R_1 \\
                \\
            \end{array}
        \] 

        Swapping \(R_1\) and \(R_3\) leads to
        \[
            \,\\
            \left[
                \begin{array}{rrrrr|r}
                    1 & -2 & 1 & -1 & 1 & 0 \\
                    4 & -8 & 3 & -3 & 1 & 2 \\
                    -2 & 4 & -2 & -1 & 4 & -3 \\
                    1 & -2 & 0 & -3 & 4 & a
                \end{array}
            \right]
            \begin{array}{l}
                \\
                -4R_1 \\
                +2R_1 \\
                -R_1
            \end{array}
            \,\\
        \]
        When we now apply the transformations, we obtain
        \[
            \,\\
            \begin{align}
            &\left[
                \begin{array}{rrrrr|r}
                    1 & -2 & 1 & -1 & 1 & 0 \\
                    0 & 0 & -1 & 1 & -3 & 2 \\
                    0 & 0 & 0 & -3 & 6 & -3 \\
                    0 & 0 & -1 & -2 & 3 & a
                \end{array}
            \right]
            \begin{array}{l}
                \\
                \\
                \\
                -R_2 - R_3
            \end{array}
            \,\\
            \,\\
            \rightarrow \,\, &\left[
                \begin{array}{rrrrr|r}
                    1 & -2 & 1 & -1 & 1 & 0 \\
                    0 & 0 & -1 & 1 & -3 & 2 \\
                    0 & 0 & 0 & -3 & 6 & -3 \\
                    0 & 0 & 0 & 0 & 0 & a + 1
                \end{array}
            \right]
            \begin{array}{l}
                \\
                \times \, (-1) \\
                \times \, (-\frac{1}{3})
                \\
            \end{array}
            \,\\
            \,\\
            \rightarrow \,\, &\left[
                \begin{array}{rrrrr|r}
                    1 & -2 & 1 & -1 & 1 & 0 \\
                    0 & 0 & 1 & -1 & 3 & -2 \\
                    0 & 0 & 0 & 1 & -2 & 1 \\
                    0 & 0 & 0 & 0 & 0 & a + 1
                \end{array}
            \right]
            \begin{array}{l}
                \\
                \\
                \\
            \end{array}
            \end{align}
            \,\\
        \]

        This matrix is in a convenient form, the row-echelon form.
        Reverting this compact notation back into the explicit notation with
        the variables we seek, we obtain
        \[
            \,\\
            \begin{array}{*{11}{r}} 
               x_1 & - & 2x_2 & + &  x_3 & - &  x_4 & + &  x_5 & = & 0 \\
                   &   &      &   &  x_3 & - &  x_4 & + & 3x_5 & = & -2 \\
                   &   &      &   &      &   &  x_4 & - & 2x_5 & = & 1 \\
                   &   &      &   &      &   &      &   & 0    & = & a + 1
            \end{array}
            \,\\
        \]

        Only for \(a = =1\) this system can be solved. A particular solution is
        \[
            \,\\
            \begin{bmatrix}
            x_1 \\
            x_2 \\
            x_3 \\
            x_4 \\
            x_5 
            \end{bmatrix}
            =
            \begin{bmatrix}
            2 \\
            0 \\
            -1 \\
            1 \\
            0 
            \end{bmatrix}
            \,\\
        \]

    From the row-echelon form, we can get free variables \(x_2, x_5\)
    \[
        \,\\
        \begin{array}{*{11}{r}} 
        x_1 & = & 2x_2 & + & 2x_5 \\
        x_2 & = & x_2  &   & \\
        x_3 & = &      & - & x_5 \\
        x_4 & = &      &   & 2x_5   \\
        x_5 & = &      &   & x_5 
        \end{array}
        \,\\
    \]

    Then, substitute them as \(x_2 = \lambda_1\) and \(x_5 = \lambda_2\)
    \[
        \,\\
        \begin{array}{*{11}{r}} 
        x_1 & = & 2\lambda_1 & + & 2\lambda_2 \\
        x_2 & = &  \lambda_1 &   & \\
        x_3 & = &      & - & \lambda_2 \\
        x_4 & = &      &   & 2\lambda_2   \\
        x_5 & = &      &   & \lambda_2
        \end{array}
        \,\\
    \]

    The general solution, which capture the set of all possible solutions, is
    \[
        \,\\
        \left\{
            x \in \mathbb{R}^5 :
            x = 
            \begin{bmatrix}
                2 \\
                0 \\
                -1 \\
                1 \\
                0
            \end{bmatrix}
            + \lambda_1
            \begin{bmatrix}
                2 \\
                1 \\
                0 \\
                0 \\
                0
            \end{bmatrix}
            + \lambda_2
            \begin{bmatrix}
                2 \\
                0 \\
                -1 \\
                2 \\
                1
            \end{bmatrix}
            ,\quad \lambda_1, \, \lambda_2 \in \mathbb{R}
        \right\}
        \,\\
    \]
    </div>
    
    <br><br>

    <strong><i>Remark</i></strong> (Povots and Staircase Structure).
    The leading coefficient of a row (first nonzero number from the left) is called the pivot and is always
    strictly to the right of the pivot of the row above it.
    Therefore, any equation system in row-echelon form always has a "staircase" structure.

    <br><br>

    <strong>Definition 2.6 (Row-Echelon Form)</strong>. 
    A matrix is in row-echeon form if
    <ul>
        <li>
            All rows that contain only zeros are at the bottom of the matrix;
            all rows that contain at least one nonzero element are on
            top of rows that contain only zeros.
        </li>
        <li>
            Looking at nonzero rows only, the first nonzero number from the left
            (also called the pivot or the leading coefficient) is always strictly to the
            right of the pivot of the row above it.
        </li>
    </ul>

    <br>

    <strong><i>Remark</i></strong> (Basic and Free Variables).
    The variables corresponding to the pivots in the row-echelon form are called basic variables and the other
    variables are free variables.

    <br><br>

    <strong><i>Remark</i></strong> (Obtaining a Particular Solution).
    We express the right-hand side of the equation system using the pivot
    columns, such that, \(b = \sum_{i=1}^P \lambda_i p_i  \), where \(p_i, \, i=1, ..., P\), are the pivot columns.
    
    In the previous example (2.6), we would try to find \(\lambda_1, \lambda_2, \lambda_3 \) so that
    \[
        \,\\
        \lambda_1
        \begin{bmatrix}
            1\\
            0\\
            0\\
            0
        \end{bmatrix}
        + \lambda_2
        \begin{bmatrix}
            1\\
            1\\
            0\\
            0
        \end{bmatrix}
        + \lambda_3
        \begin{bmatrix}
            -1\\
            -1\\
            1\\
            0
        \end{bmatrix}
        = 
        \begin{bmatrix}
            0\\
            -2\\
            1\\
            0
        \end{bmatrix}
        \,\\
    \]

    From here, we find relatively directly that \(\lambda_1 = 2, \lambda_2 = -1, \lambda_3 = 1\).
    When we put everything together, we must not forget the non-pivot columns
    for which we set the coefficients implicitly to 0, i.e., \([\lambda_1, 0, \lambda_2, \lambda_3, 0] \).
    Therefore, we get the particular solution \(x = [2, 0, -1, 1, 0]^{\top} \).

    <br><br>

    <strong><i>Remark</i></strong> (Reduced Row Echelon Form).
    An equation is in reduced row-echelon form if
    <ul>
        <li>
            It is in row-echelon form.
        </li>
        <li>
            Every pivot is 1.
        </li>
        <li>
            The pivot is the only nonzero entry in its column.
        </li>
    </ul>

    <br>

    <strong><i>Gaussian Elimination</i></strong>.
    Gaussian elimination is an algorithm that
    performs elementary transformations to bring a system of linear equations
    into reduced row-echelon form.

    <br><br>
    <div class="mml-example">
        <strong>Example 2.7</strong>
        <br>
        Verify that the following matrix is in reduced row-echelon form:

        \[
            \,\\
            A = 
            \begin{bmatrix}
            1 & 3 & 0 & 0 & 3 \\
            0 & 0 & 1 & 0 & 9 \\
            0 & 0 & 0 & 1 & -4 \\
            \end{bmatrix}
            \,\\
        \]

        The key idea for finding the solutions of \(Ax = 0\) is to look at the non-pivot columns,
        which we will need to express as a (linear) combination of the pivot columns. In other words,
        <br><br>
        \[
            \text{non-pivot column} = \text{some combination of pivot columns}
        \]
        <br>

        Returning to the example, we want to solve \(Ax = 0\).
        Here, we can define \(A\) as a collection of column vectors as \(A = [c_1, c_2, c_3, c_4, c_5]\),
        and \(Ax\) is to find all \(x\) for the equation below:
        \[
            \,\\
            \begin{align}
            x_1 c_1 + x_2 c_2 + x_3 c_3 + x_4 c_4 + x_5 c_5 &= 0
            \,\\
            \,\\
            x_1
            \begin{bmatrix}
                1\\
                0\\
                0
            \end{bmatrix}
            + x_2
            \begin{bmatrix}
                3\\
                0\\
                0
            \end{bmatrix}
            + x_3
            \begin{bmatrix}
                0\\
                1\\
                0
            \end{bmatrix}
            + x_4
            \begin{bmatrix}
                0\\
                0\\
                1
            \end{bmatrix}
            + x_5
            \begin{bmatrix}
                3\\
                9\\
                4
            \end{bmatrix}
            &= 0
            \end{align}
            \,\\
        \]

        In the matrix \(A\), the first non-pivot column \(c_2\) is
        \[
            \,\\
            c_2 = 
            \begin{bmatrix}
                3\\
                0\\
                0
            \end{bmatrix}
                = 3c_1
            \,\\
        \] 

        The above is the first dependency. The second non-pivot column \(c_5\) is the second:
        \[
            \,\\
            c_5 = 
            \begin{bmatrix}
                3\\
                9\\
                -4
            \end{bmatrix}
                = 3c_1 + 9c_3 - 4c_4
            \,\\
        \]

        From the relationship between vectors below, we can get the right expressions behind rightarrow

        \[
            \,\\
            \begin{align}
            \begin{aligned}
            3c_1 - c_2 + 0c_3 + 0c_4 + 0c_5  &= 0 \quad \Rightarrow \quad  3x_1 - x_2 + 0x_3 + 0x_4 + 0x_5 = 0 \\
            3c_1 + 0c_2 + 9c_3 - 4c_4 - c_5 &= 0 \quad \Rightarrow  \quad  3x_1 + 0x_2 + 9x_3 - 4x_4 - x_5 = 0
            \end{aligned}
            \end{align}
            \,\\
        \]

        To summarize, all solutions of \(Ax = 0, x\in \mathbb{R}^5 \) are given by
        \[
            \,\\
            \left\{
                x \in \mathbb{R}^5: x = \lambda_1
                \begin{bmatrix}
                    3\\
                    -1\\
                    0\\
                    0\\
                    0
                \end{bmatrix}
                + \lambda_2
                \begin{bmatrix}
                    3\\
                    0\\
                    9\\
                    -4\\
                    -1
                \end{bmatrix},
                \quad
                \lambda_1, \lambda_2 \in \mathbb{R}
            \right\}
            \,\\
        \]

        where the free variables \(x_2, x_5\) are  \(\lambda_1\) and \(\lambda_2\) respectively.
        
    </div>

    <br><br>

    <h4>2.3.3 The Minus-1 Trick</h4>
    We introduce a practical trick for reading out the solutions \(x\), where \(A \in \mathbb{R}^{k \times n}, x \in \mathbb{R}^n \) .
    To start, we assume that \(A\) is in reduced row-echelon form without any rows that just contain zeros.
    We extend the matrix \(A\) to an \(n \times n\)-matrix \(\tilde{A}\).

    <br><br>

    <div class="mml-example">
        <strong>Example 2.8 (Minus-1 Trick)</strong>
        <br>
        Let us revisit the matrix below, which is already in reduced REF:

        \[
            \,\\
            A = 
            \begin{bmatrix}
            1 & 3 & 0 & 0 & 3 \\
            0 & 0 & 1 & 0 & 9 \\
            0 & 0 & 0 & 1 & -4 \\
            \end{bmatrix}
            \,\\
        \]

        we now augment this matrix to a \(5 \times 5\) matrix by adding rows at the places where the pivots on the diagonal are missing
        and obtain

        \[
            \,\\
            \tilde{A} = 
            \begin{bmatrix}
            1 & 3 & 0 & 0 & 3 \\
            \color{blue}{0} & \color{blue}{-1} & \color{blue}{0} & \color{blue}{0} & \color{blue}{0} \\
            0 & 0 & 1 & 0 & 9 \\
            0 & 0 & 0 & 1 & -4 \\
            \color{blue}{0} & \color{blue}{0} & \color{blue}{0} & \color{blue}{0} & \color{blue}{-1}
            \end{bmatrix}
            \,\\
        \]

        From this form, we can immediately read out the solutions of \(Ax = 0\),
        which is the same as in the Example 2.7
        \[
            \,\\
            \left\{
                x \in \mathbb{R}^5: x = \lambda_1
                \begin{bmatrix}
                    3\\
                    -1\\
                    0\\
                    0\\
                    0
                \end{bmatrix}
                + \lambda_2
                \begin{bmatrix}
                    3\\
                    0\\
                    9\\
                    -4\\
                    -1
                \end{bmatrix},
                \quad
                \lambda_1, \lambda_2 \in \mathbb{R}
            \right\}
            \,\\
        \]
    </div>

    <br><br>
    Calculating the Inverse.
    To compute the inverse \(A^{-1}\) of \(A \in \mathbb{R}^{n\times n} \),
    we need to find a matrix \(X\) that satisfies \(AX = I_n\).
    Then \(X = A^{-1} \). We can write this down as a set of simultaneous linear equations \(AX = I_n\).
    If we bring the augmented equation system into row-echelon form,
    we can read out the inverse on the right-hand side of the equation system.

    <br><br>
    <div class="mml-example">
        <strong>Example 2.9 (Calculating an Inverse Matrix by Gaussian Elimination)</strong>
        <br>
        To determine the inverse of 
        \[
            \,\\
            A = 
            \begin{bmatrix}
                1 & 0 & 2 & 0 \\
                1 & 1 & 0 & 0 \\
                1 & 2 & 0 & 1 \\
                1 & 1 & 1 & 1 
            \end{bmatrix}
            \,\\
        \]

        we write down the augmented matrix
        \[
            \,\\
            \left[
            \begin{array}{cccc|cccc}
                 1 & 0 & 2 & 0 & 1 & 0 & 0 & 0 \\
                 1 & 1 & 0 & 0 & 0 & 1 & 0 & 0 \\
                 1 & 2 & 0 & 1 & 0 & 0 & 1 & 0 \\
                 1 & 1 & 1 & 1 & 0 & 0 & 0 & 1
            \end{array}
            \right]
            \,\\
        \]

        and use Gaussian elimination to bring it into reduced row-echelon form
        \[
            \,\\
            \begin{align}
            &\left[
            \begin{array}{cccc|cccc}
                 1 & 0 & 2 & 0 & 1 & 0 & 0 & 0 \\
                 0 & 1 & -2 & 0 & -1 & 1 & 0 & 0 \\
                 1 & 2 & 0 & 1 & 0 & 0 & 1 & 0 \\
                 1 & 1 & 1 & 1 & 0 & 0 & 0 & 1
            \end{array}
            \right]

            \rightarrow

            \left[
            \begin{array}{cccc|cccc}
                 1 & 0 & 2 & 0 & 1 & 0 & 0 & 0 \\
                 0 & 1 & -2 & 0 & -1 & 1 & 0 & 0 \\
                 0 & 1 & -1 & 0 & 0 & 0 & 1 & -1 \\
                 1 & 1 & 1 & 1 & 0 & 0 & 0 & 1
            \end{array}
            \right]

            \,\\
            \,\\

            \rightarrow
            
            &\left[
            \begin{array}{cccc|cccc}
                 1 & 0 &  2 & 0 &  1 &  0 & 0 & 0 \\
                 0 & 1 & -2 & 0 & -1 &  1 & 0 & 0 \\
                 0 & 0 &  1 & 0 &  1 & -1 & 1 & -1 \\
                 1 & 1 &  1 & 1 &  0 &  0 & 0 & 1
            \end{array}
            \right]

            \rightarrow 
            
            \left[
            \begin{array}{cccc|cccc}
                 1 & 0 &  2 & 0 &  1 &  0 & 0 & 0 \\
                 0 & 1 &  0 & 0 & 1 &  -1 & 2 & -2 \\
                 0 & 0 &  1 & 0 &  1 & -1 & 1 & -1 \\
                 1 & 1 &  1 & 1 &  0 &  0 & 0 & 1
            \end{array}
            \right]

            \,\\
            \,\\

            \rightarrow
            
            &\left[
            \begin{array}{cccc|cccc}
                1 & 0 &  0 & 0 &  -1 &  2 & -2 & 2 \\
                0 & 1 &  0 & 0 & 1 &  -1 & 2 & -2 \\
                0 & 0 &  1 & 0 &  1 & -1 & 1 & -1 \\
                1 & 1 &  1 & 1 &  0 &  0 & 0 & 1
            \end{array}
            \right]

            \rightarrow 
            
            \left[
            \begin{array}{cccc|cccc}
                1 & 0 &  0 & 0 &  -1 &  2 & -2 & 2 \\
                0 & 1 &  0 & 0 & 1 &  -1 & 2 & -2 \\
                0 & 0 &  1 & 0 &  1 & -1 & 1 & -1 \\
                0 & 0 &  0 & 1 &  -1 &  0 & -1 & 2
            \end{array}
            \right]

            \end{align}
            \,\\
        \]

        such that the desired inverse is given as its right-hand side:
        \[
            \,\\
            A^{-1} = 
            \left[
            \begin{array}{cccc}
                 -1 &  2 & -2 & 2 \\
                1 &  -1 & 2 & -2 \\
                 1 & -1 & 1 & -1 \\
                 -1 &  0 & -1 & 2
            \end{array}
            \right]

            \,\\
        \]
    </div>

    <br><br>

    Each elementary row operation \(E\) can be represented as a left multiplication of a matrix \(A\), that is \(EA\).
    Performing multiple elementary row operations \(E_1, E_2, ..., E_k\) is equivalent to 
    \[
        \,\\
        E_k\, E_{k-1} \cdots E_1 \, A = I
        \,\\
    \]
    
    In other words, transforming \(A\) into the identity matrix \(I\) by using elementary transformations is 
    equivalent to multiplying \(A\) by the matrix \(E\), where \(E\) is defined as
    \[
        \,\\
        E = E_k\, E_{k-1} \cdots E_1 \
        \,\\
    \]

    Therefore, 
    \[
        \,\\
        \begin{align}
            EA &= I \\
            E  &= A^{-1}
        \end{align}

        \,\\
    \]

    <br>
    
    <h4>2.3.4 Algorithms for Solving a System off Linear Equations</h4>
    Gaussian elimination plays an important role when 
    1) computing determinants,
    2) checking whether a set of vectors is linearly independent,
    3) computing the inverse of a matrix,
    4) computing the rank of a matrix,
    and 4) determining a basis of a vector space.

    <br><br>

    <h4>2.4 Vector Spaces</h4>
    In the following,
    we will have a closer look at vector spaces, i.e., a structured space in which
    vectors live.

    <br><br>
    In the beginning of this chapter, we informally characterized vectors as objects
    that can be added together and multiplied by a scalar, and they remain objects of the same type
    Now, we are ready to formalize this, and we will start by introducing the concept of a group, 
    which is a set of elements and an operation defined on these elements that keeps some structure of the set intact.

    <br><br>
    <h4>2.4.1 Groups</h4>
    <strong>Definition 2.7 (Group)</strong>. 
    Consider a set \(\mathcal{G}\) and an operation \(\otimes : \mathcal{G} \times \mathcal{G} \rightarrow \mathcal{G} \)
    defined on \(\mathcal{G}\). Then \(G := (\mathcal{G}, \otimes)\) is called a group if the following hold:
    <ol>
        <li>
            Closure of \(\mathcal{G}\) under \(\otimes : \forall x, y \in \mathcal{G}: x \otimes y \in \mathcal{G} \)
        </li>
        <li>
            Associativity: \(\forall x, y, z \in \mathcal{G}: (x \otimes y) \otimes z = x \otimes (y \otimes z) \)
        </li>
        <li>
            Neutral element (Identity element): 
            \(\exists e \in \mathcal{G} \, \forall x \in \mathcal{G}: x \otimes e = x\) and \(e \otimes x = x \)
        </li>
        <li>
            Inverse element: \(\forall x \in \mathcal{G} \, \exists y \in \mathcal{G}: x \otimes y = e \) and \(y \otimes x = e \)
            where \(e\) is the neutral element (identity element).
            We often write \(x^{-1}\) to denote the inverse element of \(x\).
        </li>
    </ol>

    <br>

    <strong><i>Remark</i></strong>. The inverse element is defined with respect to the operation \(\otimes\) and does not 
    necessarily mean \(\frac{1}{x}\). If additionally \(\forall x, y \in \mathcal{G}: x \otimes y = y \otimes x \), then
    \(G = (\mathcal{G}, \otimes)\) is an Abelian group (commutative).

    <br><br>

    <div class="mml-example">
        <strong>Example 2.10 (Groups)</strong>
        <br>
        Let us have a look at some examples of sets with associated operations
        and see whether they are groups:
        <ul>
            <li>
                \( (\mathbb{Z}, +) \) is an Abelian group.
            </li>
            <li>
                \( (\mathbb{N}_0, +) \) is not a group, where \(\mathbb{N}_0 := \mathbb{N} \cup \{0\} \):
                Although \( (\mathbb{N}_0, +) \) possesses a neutral element \((0)\), the inverse elements are missing.
            </li>
            <li>
                \( (\mathbb{Z}, \cdot) \) is not a group: Although \( (\mathbb{Z}, \cdot) \) contains a neutral element
                \((1)\), the inverse elements for any \(z \in \mathbb{Z} \setminus \{\pm 1\} \) are missing.
                In addition, \(0 \in \mathbb{Z} \) cannot have an inverse element since 
                the equation \(0 \cdot x = 1 \) cannot be satisfied for any \(x \in \mathbb{Z}\).
            </li>
            <li>
                \( (\mathbb{R}, \cdot) \) is not a group since \(0\) does not possess an inverse element.
            </li>
            <li>
                \( (\mathbb{R} \setminus \{0\}, \cdot) \) is Abelian.
            </li>
            <li>
                \( (\mathbb{R}^n, +), (\mathbb{Z}^n, + ), n \in \mathbb{N} \) are Abelian if \(+\)
                is defined componentwise, i.e.,
                \[
                    \,\\
                    (x_1, \cdots, x_n) + (y_1, \cdots, y_n) = (x_1 + y_1, \cdots, x_n + y_n)
                    \,\\
                \]
                Then, \( (x_1, \cdots, x_n)^{-1} := (-x_1, \cdots, -x_n) \) is the inverse element and
                \(e = (0, \cdots, 0) \) is the neutral element. 
                For example, if \( x = (1, 2, 3, 4, 5) \), then its inverse is
                \[
                    \,\\
                    x^{-1}  = (-1, -2, -3, -4, -5)
                    \,\\
                \]

                and by adding them componentwise, we obtain the neutral element:
                \[
                    \,\\
                    (1-1, \,\, 2-2, \,\, 3-3, \,\, 4-4, \,\, 5-5) = (0, \,\, 0, \,\, 0, \,\, 0, \,\, 0).
                    \,\\
                \]
            </li>
            <li>
                \( (\mathbb{R}^{m \times n}, +) \), the set of \(m \times n\)-matrices is Abelian (with componentwise addition)
            </li>
            <li>
                \( (\mathbb{R}^{n \times n}, \cdot ) \), the set of \(n \times n\)-matrices with matrix multiplication
                <ul>
                    <li>
                        Closure and associativity follow directly from the definition of matrix multiplication
                        \[
                            \,\\
                                (A \cdot B) \cdot C = A\cdot (B \cdot C)
                            \,\\
                        \]
                    </li>
                    <li>
                        The identity matrix \(I_n\) is the neutral element
                        \[
                            \,\\
                            I_n \cdot A = A \cdot I_n = A
                            \,\\
                        \]
                    </li>
                    <li>
                        For a regular matrix \( A \in \mathbb{R}^{n \times n} \), the inverse element \( A^{-1} \) exists. 
                        If \( A \) is singular, it does not have an inverse element.
                        <br><br>
                        The set of all regular (invertible) matrices forms a group under matrix multiplication,
                        denoted by \( GL(n, \mathbb{R}) = \{ A \in \mathbb{R}^{n \times n} \mid \det(A) \neq 0 \} \),
                        which is called the General Linear Group. 
                        <br><br>
                        Therefore, \( (GL(n, \mathbb{R}), \cdot) \) is a group. 
                        However, since matrix multiplication is not commutative, the group is not Abelian.
                    </li>
                </ul>
            </li>
        </ul>
    </div>

    <br><br>

    <h4>2.4.2 Vector Spaces</h4>
    <strong>Definition 2.9 (Vector Space)</strong>. A real-valued vector space \(V = \mathcal{V, +, \cdot}\) is
    a set \(\mathcal{V} \) with two operations
    \[
        \,\\
        \begin{align}
        +     &: \mathcal{V} \times \mathcal{V} \rightarrow \mathcal{V} \\
        \cdot &: \mathbb{R}  \times \mathcal{V} \rightarrow \mathcal{V}
        \end{align}
        \,\\
    \]

    where \( (\mathcal{V}, +) \) is an abelian group.
    The elements \(x \in V\) are called vectors. The neutral element of \( (\mathcal{V}, +) \)
    is the ero vector \( [0, ..., 0]^{\top} \), and the operation \(+\) is called vector addition.
    The elements \(\lambda \in \mathbb{R} \) are called scalars and the operation \( \cdot \) is a multiplication by scalars. 
    
    <br><br>

    <strong><i>Remark</i></strong>. A vector multiplication \(ab, \,\, a, b \in \mathbb{R}^n\), is not defined.
    By treating vectors as \(n \times 1\) matrices, we can use the matrix multiplication.
    Only the following multiplications for vectors are defined: \(ab^{\top} \in \mathbb{R}^{n\times n}\) (outer product),
    \( a^{\top}b \in \mathbb{R} \) (inner/scalar/dot product). 
    \[
        \,\\
        a = 
        \begin{bmatrix}
            0\\
            1\\
            2
        \end{bmatrix}, \,\,
        b = 
        \begin{bmatrix}
            3\\
            4\\
            5
        \end{bmatrix}

        \,\\
        \,\\

        \begin{align}
        ab^{\top} &= 
        \begin{bmatrix}
            0\\
            1\\
            2
        \end{bmatrix}
        \begin{bmatrix}
            3&4&5
        \end{bmatrix}
        = 
        \begin{bmatrix}
            0 & 0 & 0 \\
            3 & 4 & 5 \\
            6 & 8 & 10 
        \end{bmatrix}

        \,\\
        \,\\

        a^{\top}b &= 
        \begin{bmatrix}
            0&1&2
        \end{bmatrix}
        \begin{bmatrix}
            3\\
            4\\
            5
        \end{bmatrix}
        = 14
        \end{align}
        \,\\
    \]

    <h4>2.4.3 Vector Subspaces</h4>
    Intuitively, they are
    sets contained in the original vector space with the property that when
    we perform vector space operations on elements within this subspace, we
    will never leave it. In this sense, they are "closed".

    <br><br>

    <strong>Definition 2.10 (Vector Subspace)</strong>. 
    Let \(V = (\mathcal{V}, +, \cdot) \) be a vector space and
    \(\mathcal{U} \subseteq \mathcal{V}, \mathcal{U} \neq \emptyset \).
    Then \(U = (\mathcal{U}, +, \cdot) \) is called vector subspace of \(V\) (or linear subspace)
    if \(U\) is a vector space with the vector space operations \(+\) and \(\cdot\)
    restricted to \(\mathcal{U} \times \mathcal{U} \) and \( \mathbb{R} \times \mathcal{U} \).
    We write \( U \subseteq V \) to denote a subspace \(U\) of \(V\).

    <br><br>

    If \(\mathcal{U} \subseteq \mathcal{V} \) and \(V\) is a vector space, then \(U\) naturally inherits 
    the operations of \(V\), that is,
    \[
        \,\\
        \begin{align}
        +     &: \mathcal{U} \times \mathcal{U} \rightarrow \mathcal{U} \\
        \cdot &: \mathbb{R}  \times \mathcal{U} \rightarrow \mathcal{U}
        \end{align}
        \,\\
    \]

    For \(U\) to be a subspace of \(V\), the following conditions must hold:
    <ol>
        <li>
            neutral element (zero vector): 
            \[
                \mathcal{U} \neq \emptyset, \, \boldsymbol{0} \in \mathcal{U}
            \]
        </li>
        <li>
            closure:
            \[
                \begin{align}
                &\forall \lambda \in \mathbb{R} \, \forall x \in \mathcal{U} : \lambda x \in \mathcal{U}
                \,\\
                &\forall x, y \in \mathcal{U} : x + y \in \mathcal{U}
                \end{align}
            \]
        </li>
    </ol>

    <br>
    
    <div class="mml-example">
        <strong>Example 2.12 (Vector Subspaces)</strong>
        <ul>
            <li>
                For every vector space \(V\), the trivial subspaces are \(V\) itself and \( \{\boldsymbol{0}\}\).
            </li>
            <li>
                Only example \(D\) in the figure below is a subspace of \(\mathbb{R}^2 \).
                In \(A\) and \(C\), the closure property is violated; \(B\) does not contain \(\boldsymbol{0}\).
            </li>
            <figure>
                <img src="/img/mathematics-for-machine-learning/mathematics-for-machine-learning-4.png" width="100%" onerror=handle_image_error(this)>
                <figcaption>
                    Only \(D\) is a subspace
                </figcaption>
            </figure>
        </ul>

    </div>

    <br><br>

    <strong><i>Remark</i></strong>. Every subspace \(U \subseteq (\mathbb{R}^n, +, \cdot) \) is the solution
    space of a homogeneous system of linear equations \(Ax = 0\) for \(x \in \mathbb{R}^n\).
    \[
        \,\\
        A(\boldsymbol{0}) = \boldsymbol{0}
        \,\\
        \,\\
        A(x_1) = \boldsymbol{0}, \,\, A(x_2) = \boldsymbol{0} 
        \,\\
        \,\\
        A(x_1 + x_2) = A(x_1) + A(x_2) = \boldsymbol{0} + \boldsymbol{0} = \boldsymbol{0}
        \,\\
        \,\\
        A(\lambda x_1) = \lambda A(X_1) = \lambda \cdot \boldsymbol{0} = \boldsymbol{0}
        \,\\
    \]

    <h4>2.5 Linear Independence</h4>
    The closure property guarantees that we
    end up with another vector in the same vector space. It is possible to find
    a set of vectors with which we can represent every vector in the vector
    space by adding them together and scaling them. 
    This set of vectors is a basis. Before we get concepts of the basis,
    we will need to introduce the concepts of linear combinations and linear independence.
    
    <br><br>

    <strong>Definition 2.11 (Linear Combination)</strong>.
    Consider a vector space \(V\) and a finite number of vectors \(x_1, ..., x_k \in V \).
    Then, every \(v \in V\) of the form
    \[
        \,\\
        v = \lambda_1 x_1 + \cdots + \lambda_k x_k = \sum_{i=1}^k \lambda_i x_i \in V
        \,\\
    \]

    with \(\lambda_1, ..., \lambda_k \in \mathbb{R} \) is a linear combination of the vectors \(x_1, ..., x_k \).

    <br><br>

    The \(\boldsymbol{0} \)-vector can always be written as the linear combination of \(k\) vectors \(x_1, ..., x_k \)
    because \(\boldsymbol{0} = \sum_{i=1}^k 0x_i \) is always true, and is trivial.
        \[
            \,\\
            \boldsymbol{0} = 0x_1 + \cdots + 0x_k
            \,\\
        \]

    In the following, we are interested in non-trivial linear combinations of a set of vectors to represet \(\boldsymbol{0}\),
    i.e., linear combinations of vectors \(x_1, ..., x_k \), where not all coefficients \(\lambda_i\)in are \(0\).

    <br><br>

    <strong>Definition 2.12 (Linear (In)dependence)</strong>.
    Let us consider a vector space \(V\) with \(k\in \mathbb{N}\) and \(x_1, ..., x_k \in V  \).
    If there is a non-trivial linear combination, such that \(\boldsymbol{0} = \sum_{i=1}^k \lambda_i x_i  \)
    with at least one \(\lambda_i \neq 0\), the vectors are linearly dependent.
    \[
        \,\\
        \begin{align}
        x_3 &= 2x_1 - 3x_2 \\
        0   &= 2x_1 - 3x_2 - x_3
        \end{align}
        \,\\
    \]
    
    In contrast,
    \(\boldsymbol{0} = \lambda_1 x_1 + \cdots + \lambda_k x_k\),
    only the trivial solution exists, i.e.,
    \(\lambda_1 = \cdots = \lambda_k = 0\),
    the vectors \(x_1, \dots, x_k\) are said to be linearly independent.
    \[
        \,\\
        x_1 = 
        \begin{bmatrix}
            1\\
            0\\
        \end{bmatrix},
        \,\, 
        x_2 = 
        \begin{bmatrix}
            0\\
            1\\
        \end{bmatrix}
        \,\\
        \,\\
        \lambda_1 
        \begin{bmatrix}
            1\\
            0\\
        \end{bmatrix}
        + \lambda_2 
        \begin{bmatrix}
            0\\
            1\\
        \end{bmatrix} = 
        0 
        \begin{bmatrix}
            1\\
            0\\
        \end{bmatrix}
        + 0 
        \begin{bmatrix}
            0\\
            1\\
        \end{bmatrix} = 
        
        \boldsymbol{0}
        \,\\
    \]

    <strong><i>Remark</i></strong>. 
    The following properties are useful to find out whether vectors are linearly independent:
    <ul>
        <li>
            \(k\) vectors are either linearly dependent or independent. There is no third option.
        </li>
        <li>
            If at least one of the vectors \(x_1, ..., x_k \) is \(\boldsymbol{0}\) then they are linearly dependent.
            The same vectors exist, they are linearly dependent.
        </li>
        <li>
            The vectors \( \{ x_1, ..., x_k : x_i \neq 0, i = 1, ..., k \}, k \ge 2 \), are linearly dependent
            if and only if (at least) one of them is a linear combination of the others.
            In particular, if one vector is a multiple of another vector, i.e., \(x_i = \lambda x_j, \lambda \in \mathbb{R} \ \)
            then the vectors linearly dependent.
        </li>
        <li>
            A practical way of checking whether vectors are linearly independent
            is to use Gaussian elimination: Write all vectors as columns
            of a matrix \(A\) and perform Gaussian elimination until the matrix is in
            row echelon form (the reduced row-echelon form is unnecessary here).
        </li>
        <li>
            The non-pivot columns can be expressed as linear combinations of the pivot columns.
            For example, the row-echelon form
            
            \[
                x_1 = 
                \begin{bmatrix}
                    1\\
                    0\\
                    0
                \end{bmatrix}, \quad
                x_2 = 
                \begin{bmatrix}
                    2\\
                    1\\
                    0
                \end{bmatrix}, \quad
                x_3 = 
                \begin{bmatrix}
                    3\\
                    1\\
                    0
                \end{bmatrix}

                \,\\
                \,\\
                
                A = 
                \begin{bmatrix}
                    1 & 2 & 3 \\
                    0 & 1 & 1 \\
                    0 & 0 & 0
                \end{bmatrix}
            \]

            tells us that the first and the second columns are pivot columns.
            The third column is a non-pivot column because it can be expressed \(x_1 + x_2\). 
        </li>
        <li>
            All column vectors are linearly independent if and only if all columns are pivot columns.
        </li>
    </ul>

    <br>

    <div class="mml-example">
        <strong>Example 2.14</strong>
        <br>
        Consider \(\mathbb{R}^4\) with
        \[
            \,\\
            x_1 = 
            \begin{bmatrix}
                1 \\
                2 \\
                -3 \\
                4 \\
            \end{bmatrix}, \quad
            x_2 = 
            \begin{bmatrix}
                1 \\
                1 \\
                0 \\
                2 \\
            \end{bmatrix}, \quad
            x_3 = 
            \begin{bmatrix}
                -1 \\
                -2 \\
                1 \\
                1 \\
            \end{bmatrix}, \quad
            
            A = 
            \begin{bmatrix}
                1 & 1 & -1 \\
                2 & 1 & -2 \\
                -3 & 0 & 1 \\
                4 & 2 & 1
            \end{bmatrix}
            \,\\
        \]

        To check whether they are linearly independent, perform elementary row operations
        until we identify the pivot columns:
        \[
            \,\\
            \begin{align}
            A &=
            \begin{bmatrix}
                1 & 1 & -1 \\
                2 & 1 & -2 \\
                -3 & 0 & 1 \\
                4 & 2 & 1
            \end{bmatrix}
            \,\\
            \,\\
            & \Rightarrow
            \begin{bmatrix}
                1 & 1 & -1 \\
                0 & 1 & 0 \\
                -3 & 0 & 1 \\
                4 & 2 & 1
            \end{bmatrix}
            \Rightarrow
            \begin{bmatrix}
                1 & 1 & -1 \\
                0 & 1 & 0 \\
                0 & 3 & -2 \\
                0 & 2 & -5
            \end{bmatrix}
            \Rightarrow
            \begin{bmatrix}
                1 & 1 & -1 \\
                0 & 1 & 0 \\
                0 & 0 & 1 \\
                0 & 0 & 5
            \end{bmatrix}
            \,\\
            \,\\
            &\Rightarrow
            \begin{bmatrix}
                1 & 1 & -1 \\
                0 & 1 & 0 \\
                0 & 0 & 1 \\
                0 & 0 & 0
            \end{bmatrix}
            \end{align}
            \,\\
        \]

        Here, every column of the matrix is a pivot column.
        Therefore, there is no non-trivial solution, and we require \(\lambda_1 = 0, \lambda_2 = 0, \lambda_3 = 0 \)
        to solve the equation system. Hence, the vectors \(x_1, x_2, x_3 \) are linearly independent.
    </div>

    <br><br>

    <!-- <strong><i>Remark</i></strong>. Consider a vector space \(V\) with \(k\) linearly independent vectors
    \(b_1, ..., b_k\) and \(m\) linear combinations
    \[
        \,\\
        x_1 = \sum_{i=1}^{k} \lambda_{i1} b_i
        \,\\
        \,\\
        \vdots
        \,\\
        \,\\
        x_m = \sum_{i=1}^{k} \lambda_{im} b_i
        \,\\
    \]

    Defining \(\boldsymbol{B} = [b_1, ..., b_k] \) as the matrix whose columns are the linearly independent
    vectors, we can write
    \[
        \,\\
        x_j = \boldsymbol{B} \boldsymbol{\lambda}_j, 
        \quad \boldsymbol{\lambda}_j = 
        \begin{bmatrix}
            \lambda_{1j} \\
            \vdots \\
            \lambda_{kj}
        \end{bmatrix}
        , \quad
        j = 1, ..., n
        \,\\
    \]

    <br><br> -->

    <strong><i>Remark</i></strong>. In a vector space \(V\), \(m\) linear combinations of \(k\) vectors are
    linearly dependent if \(m > k\).

    <br><br>

    <h4>2.6 Basis and Rank</h4>
    In a vector space \(V\) , we are particularly interested in sets of vectors \(\mathcal{A}\) that
    possess the property that any vector \(v \in V\) can be obtained by a linear
    combination of vectors in \(\mathcal{A}\).

    <br><br>

    <h4>2.6.1 Generating Set and Basis</h4>
    <strong>Definition 2.13 (Generating Set and Span)</strong>.
    Consider a vector space \(V - (\mathcal{V}, +, \cdot) \) and set of vectors
    \(\mathcal{A} = \{ a_1, ..., a_k\} \subseteq \mathcal{V} \).
    If every vector \(v \in \mathcal{V} \) can be expressed as a linear combination of \(a_1, ..., a_k \),
    \(\mathcal{A}\) is called a generating set of \(V\).
    The set of all linear combinations of vectors in \(\mathcal{A} \) is called the span of \(\mathcal{A}\).
    If \(\mathcal{A}\) spans the vector space \(V\), we write
    \[
        \,\\
        \begin{align}
        V &= \text{span}[\mathcal{A}] \\
          &= \text{span}[a_1, ..., a_k]
        \end{align}
        \,\\
    \]

    <strong>Definition 2.14 (Basis)</strong>. 
    Consider a vector space \(V = (\mathcal{V}, +, \cdot) \) and \(\mathcal{A} \subseteq \mathcal{V}\).
    A generating set \(\mathcal{A}\) of \(V\) is called minimal if there exists no smaller set
    \(\tilde{\mathcal{A}} \subsetneq \mathcal{A} \subseteq \mathcal{V}\) that spans \(V\).
    Every linearly independent generating set of \(V\) is minimal and is called a basis of \(V\).
    For example, consider the following two generating sets in \(\mathbb{R}^2\).
    \[
        \,\\
        \mathcal{A}_1 = 
        \left\{ 
            \begin{bmatrix}1 \\ 0 \end{bmatrix}, \begin{bmatrix}0 \\ 1 \end{bmatrix}
        \right\}, 
        \quad 
        \mathcal{A}_2 = 
        \left\{
            \begin{bmatrix}1 \\ 0 \end{bmatrix}, \begin{bmatrix}0 \\ 1 \end{bmatrix}, \begin{bmatrix}1 \\ 1 \end{bmatrix}
        \right\}
        \,\\
    \]

    However, there is an unnecessary vector \( [1, 1]^{\top} \in \mathcal{A}_2\)
    since \( [1, 1]^{\top} = [1, 0]^{\top} + [0, 1]^{\top} \). 
    Therefore, even if we remove \([1, 1]^{\top}\) from \(\mathcal{A}_2\),
    the remaining vectors can still generate \(\mathbb{R}^2\)
    Hence, \(\mathcal{A}_2\) is a generating set but not a minimal,
    while \(\mathcal{A}_1\) is a minimal generating set, that is a basis of \(\mathbb{R}^2\).

    <br><br>

    Let \(V = (\mathcal{V}, +, \cdot) \) be a vector space and \(\mathcal{B} \subseteq \mathcal{V}, \mathcal{B} \neq \emptyset \).
    Then, the following statements are equivalent:
    <ul>
        <li>
            \(\mathcal{B}\) is a basis of \(V\).
        </li>
        <li>
            \(\mathcal{B}\) is a minimal generating set.
        </li>
        <li>
            \(\mathcal{B}\) is a maximal linearly independent set of vectors in \(V\),
            i.e., adding any other vector to this set will make it linearly dependent.
        </li>
        <li>
            Every vector \(x \in V \) is a linear combination of vectors from \(\mathcal{B}\),
            and every linear combination is unique.
        </li>
    </ul>

    <br>

    <div class="mml-example">
        <strong>Example 2.16</strong>
        <ul>
            <li>
                In \(\mathbb{R}^n\), the standard basis is
                \[
                    \mathcal{B} =
                    \left\{
                        \begin{bmatrix}
                            1 \\
                            0 \\
                            \vdots \\
                            0
                        \end{bmatrix},
                        \begin{bmatrix}
                            0 \\
                            1 \\
                            \vdots \\
                            0
                        \end{bmatrix},
                        \ldots,
                        \begin{bmatrix}
                            0 \\
                            0 \\
                            \vdots \\
                            1
                        \end{bmatrix}
                    \right\}
                \]
            </li>
            <li>
                in \(\mathbb{R}^3\), the standard basis is
                \[
                \mathcal{B} =
                \left\{
                    \begin{bmatrix}
                        1 \\
                        0 \\
                        0
                    \end{bmatrix},
                    \begin{bmatrix}
                        0 \\
                        1 \\
                        0
                    \end{bmatrix},
                    \begin{bmatrix}
                        0 \\
                        0 \\
                        1
                    \end{bmatrix}
                \right\}
            \]
            </li>
            <li>
                Different bases in \(\mathbb{R}^3\) are
                \[
                    \mathcal{B}_1 =
                    \left\{
                        \begin{bmatrix}
                            1 \\
                            0 \\
                            0 \\
                        \end{bmatrix},
                        \begin{bmatrix}
                            1 \\
                            1 \\
                            0 \\
                        \end{bmatrix},
                        \begin{bmatrix}
                            1 \\
                            1 \\
                            1 \\
                        \end{bmatrix}
                    \right\}, \quad
                    \mathcal{B}_2 = \left\{
                        \begin{bmatrix}
                            0.5 \\
                            0.8 \\
                            0.4 \\
                        \end{bmatrix},
                        \begin{bmatrix}
                            1.8 \\
                            0.3 \\
                            0.3 \\
                        \end{bmatrix},
                        \begin{bmatrix}
                            -2.2 \\
                            -1.3 \\
                            3.5 \\
                        \end{bmatrix}
                    \right\}, \quad

                \]
            </li>
            <li>
                The set 
                \[
                    \mathcal{A} = 
                    \left\{
                        \begin{bmatrix}
                            1 \\
                            2 \\
                            3 \\
                            4 \\
                        \end{bmatrix},
                        \begin{bmatrix}
                            2 \\
                            -1 \\
                            0 \\
                            2 \\
                        \end{bmatrix},
                        \begin{bmatrix}
                            1 \\
                            1 \\
                            0 \\
                            4 \\
                        \end{bmatrix}
                    \right\}
                \]

                is linearly independent, but not a generating set (and no basis) of \(\mathbb{R}^4\):
                For instance, the vector \([0,0,0,1]^{\top} \) cannot be obtained by a linear combination of elements in \(\mathcal{A}\).
            </li>
        </ul>
    </div>
       
    <br><br>

    <strong><i>Remark</i></strong>. Every vector space \(V\) possesses a basis \(\mathcal{B}\).
    The Example 2.16 shows that there can be many bases of a vector space \(V\), i.e., there is no unique basis. 
    For example, the following are all bases of \(\mathbb{R}^2\):
    \[
        \,\\
            \mathcal{B}_1 = 
            \left\{
                \begin{bmatrix}
                    1 \\
                    0
                \end{bmatrix},
                \begin{bmatrix}
                    0 \\
                    1
                \end{bmatrix}
            \right\}, \quad
            \mathcal{B}_2 = 
            \left\{
                \begin{bmatrix}
                    1 \\
                    1
                \end{bmatrix},
                \begin{bmatrix}
                    1 \\
                    -1
                \end{bmatrix}
            \right\}
        \,\\
    \]

    However, all bases possess the same number of elements, the basis vectors.
    In other words, in \(\mathbb{R}^n\) space, every basis consists of exactly
    \(n\) linearly independent vectors.
    This means that any set of \(n\) linearly independent vectors in \(\mathbb{R}^n\)
    automatically forms a basis, and no smaller or larger set of vectors can do so.
    Therefore, all possible bases of \(\mathbb{R}^n\) contain exactly \(n\) basis vectors.

    <br><br>

    We only consider finite-dimensional vector spaces \(V\).
    In this case, the dimension of \(V\) is the numer of basis vectors of \(V\),
    and we write \(\dim(V)\).
    \[
        \mathcal{B} = 
        \left\{
            \begin{bmatrix}
                1 \\
                0 \\
                0 \\
            \end{bmatrix}, 
            \begin{bmatrix}
                0 \\
                1 \\
                0 \\
            \end{bmatrix}, 
            \begin{bmatrix}
                0 \\
                0 \\
                1 \\
            \end{bmatrix} 
        \right\}, \quad

        \dim(\mathcal{V}) = 3
    \]
        
    If \(U \subseteq V\) is a subspace of \(V\), then \(\dim(U) \le \dim(V) \)
    and \(\dim(U) = \dim(V) \) if and only if \(U = V\).
    Intuitively, the dimension of a vector space can be thought of as the number of independent directions
    in this vector space.

    <br><br>

    <strong><i>Remark</i></strong>.
    A basis of a subspace \(U = \text{span}[x_1, ..., x_m] \subseteq \mathbb{R}^n\) can be found
    by executing the following steps:
    <ol>
        <li>
            Write the spanning vectors as columns of a matrix \(A\).
        </li>
        <li>
            Determine the row-echelon form of \(A\).
        </li>
        <li>
            The spanning vectors associated with the pivot columns are a basis of \(U\).
        </li>
    </ol>

    <br>

    <div class="mml-example">
        <strong>Example 2.17 (Determining a Basis)</strong>
        <br>
        Let \(U \subseteq \mathbb{R}^3\) be spanned by four vectors
        \[
            \,\\
                x_1 =
                \begin{bmatrix} 
                    1\\
                    0\\
                    0\\
                \end{bmatrix} 
                , \quad
                x_2 = 
                \begin{bmatrix} 
                    2\\
                    0\\
                    0\\
                \end{bmatrix}
                , \quad
                x_3 = 
                \begin{bmatrix}
                    0\\
                    1\\
                    1\\ 
                \end{bmatrix}
                , \quad
                x_4 = 
                \begin{bmatrix} 
                    0\\
                    2\\
                    2\\
                \end{bmatrix}
            \,\\
        \]

        we are interested in finding out wich vectors can form a basis for \(U\).        
        For this, we need to check whether \(x_1, ..., x_4\) are linearly independent.
        \[
            \,\\
            [x_1, x_2, x_3, x_4] = 
            \begin{bmatrix}
                1 & 2 & 0 & 0\\
                0 & 0 & 1 & 2\\
                0 & 0 & 1 & 2\\
            \end{bmatrix}
            \,\,
            \xrightarrow{\text{RREF}}
            \,\,
            \begin{bmatrix}
                1 & 2 & 0 & 0 \\
                0 & 0 & 1 & 2 \\
                0 & 0 & 0 & 0 \\
            \end{bmatrix}
            \,\\
        \]

        There are two pivot columns (\(x_1, x_3 \)). Since the pivot columns indicate
        which set of vectors is linearly independent, we see form the row-echelon form that \(x_1, x_3\)
        are linearly independent (because the system of linear equations \(\lambda_1 x_1 + \lambda_3 x_3 = 0\) can be 
        only solved with \(\lambda_1 = \lambda_3 = 0\)).
        Therefore \(\{ x_1, x_3\} \) is a basis of \(U\).
    </div>

    <br><br>
    <h4>2.6.2 Rank</h4>
    The number of linearly independent columns of a matrix \(A \in \mathbb{R}^{n \times n} \)
    equals to the number of linearly independent rows. 
    It is called the rank of \(A\) and is denoted by
    \[
        \,\\
        \text{rk}(A)
        \,\\
    \]

    <strong><i>Remark</i></strong>. The rank of a matrix has some important properties:
    <ul>
        <li>
            \(\text{rk}(A) = \text{rk}(A^{\top}) \), i.e., the column rank equals to the row rank.
        </li>
        <li>
            The columns of \(A \in \mathbb{R}^{m \times n}  \) span a subspace \(U \subseteq \mathbb{R}^{m}  \)
            with \(\dim(U) =\text{rk}(A)\).
            Later we will call this subspace the image or range.
            A basis of \(U\) can be found by applying Gaussian elimination to \(A\) to identify the pivot columns.
        </li>
        <li>
            The rows of \(A\in \mathbb{R}^{m\times n} \) span subspace \(W \subseteq \mathbb{R}^n \)
            with \(\dim(W) = \text{rk}(A) \). A basis of \(W\) can be found by applying Gaussian elimination to \(A^{\top}\).
        </li>
        <li>
            A matrix \(A \in \mathbb{R}^{m \times n} \) has full rank
            if its rank equals the largeest possible rank for a matrix of the same dimensions.
            \[
                \text{rk}(A) = \min (m, n)
            \]
            
            Otherwise \(A\) is rank deficient.
            Let \(B \in \mathbb{R}^{4\times3} \)
            \[
                B =
                \begin{bmatrix}
                    1 & 0 & 0 \\
                    0 & 1 & 0 \\
                    0 & 0 & 1 \\
                    1 & 1 & 1 \\
                \end{bmatrix}
                \,\,
                \xrightarrow{R4-R3-R2-R1}
                \,\,
                \begin{bmatrix}
                    1 & 0 & 0 \\
                    0 & 1 & 0 \\
                    0 & 0 & 1 \\
                    0 & 0 & 0 \\
                \end{bmatrix}
            \]
            The rank of matrix \(B\) is \(\text{rk}(B) = 3\), \(\text{rk}(B) = 3 = \min(4, 3) \).
            Thus \(B\) is full rank.
        </li>
    </ul>
    <br><br>

    <h4>2.7 Linear Mappings</h4>
    We will study mappings on vector spaces that preserve their structure,
    which will allow us to define the concept of a coordinate.
    In the beginning of the chapter, we said that vectors are objects that can be
    added together and multiplied by a scalar, and the resulting object is still
    a vector. We wish to preserve this property when applying the mapping:
    \[
        \,\\
        \begin{align}
        \begin{aligned}
        \Phi (x + y) &= \Phi(x) + \Phi(y) \quad  &\text{Additivity} \\
        \Phi(\lambda x) &= \lambda \Phi(x) \quad &\text{Homogeneity}
        \end{aligned}
        \end{align}
        \,\\
    \]

    The mapping \(\Phi\) is linear if both conditions are satisfied.

    <br><br>

    <strong>Definition 2.15 (Linear Mapping)</strong>.
    For vector spaces \(V, W\), a mapping \(\Phi : V \rightarrow W \) is called a linear mapping 
    (or vector space homomorphism / linear transformation) if 
    \[
        \,\\
        \forall x, y \in V \, \forall \lambda, \psi \in \mathbb{R} : 
        \Phi(\lambda x + \psi y) = \lambda (\Phi(x)) + \psi (\Phi(y))
        \,\\
    \]

    We can represent linear mappings as matrices.

    <br><br>

    <strong>Definition 2.16 (Injective, Surjective, Bijective)</strong>.
    Consider a mapping \(\Phi : \mathcal{V} \rightarrow \mathcal{W} \), where \(\mathcal{V}, \mathcal{W}\)
    can be arbitarary sets. Then \(\Phi\) is called
    <ul>
        <li>
            Injective if \(\forall x, y \in \mathcal{V} : \Phi(x) = \Phi(y) \Rightarrow x = y \).
        </li>
        <li>
            Surjective if \(\Phi(\mathcal{V}) = \mathcal{W}  \).
        </li>
        <li>
            Bijective if it is injective and surjective.
        </li>
    </ul>

    A is bijective \(\Phi\) can be undone, i.e. there exists a mapping \(\Psi : \mathcal{W} \rightarrow \mathcal{V} \)
    so that \(\Psi \circ \Phi(x) = x \). This mapping \(\Psi \) is then called the inverse of \(\Phi\) and normally denoted by \(\Phi^{-1}\).
    <ul>
        <li>
            Isomorphism:
            \(\Phi : V \rightarrow W\) is called an isomorphism if \(\Phi\) is linear and bijective.
        </li>
        <li>
            Endomorphism:
            \(\Phi : V \rightarrow V\) is called an endomorphism if \(\Phi\) is linear.
        </li>
        <li>
            Automorphism:
            \(\Phi : V \rightarrow V\) is called an automorphism if \(\Phi\) is linear and bijective.
        </li>
        <li>
            We define \(\text{id}_V: V \rightarrow V, x \mapsto x \) as the identity mapping or identity automorphism in \(V\).
        </li>
    </ul>

    <br>

    <strong>Theorem 2.17</strong>. Finite-dimensional vector spaces \(V\) and \(W\) are isomorphic
    if and only if \(\dim(V) = \dim(W)\).
    Intuitively, this means
    that vector spaces of the same dimension are kind of the same thing, as
    they can be transformed into each other without incurring any loss.

    <br><br>

    <strong><i>Remark</i></strong>. Consider vector spaces \(V, W, X\). Then:
    <ul>
        <li>
            For linear mappings \(\Phi : V \rightarrow W \) and \(\Psi : W \rightarrow X \),
            the mapping \(\Psi \circ \Phi : V \rightarrow W \) is also linear.
        </li>
        <li>
            If \(\Phi : V \rightarrow W\) is an isomorphism, then \(\Phi^{-1} : W \rightarrow V \) is an isomorphism too.
        </li>
    </ul>

    <br>
    <h4>2.7.1 Matrix Representation of Linear Mappings</h4>
    Any \(n\)-dimensional vector space is isomorphic to \(\mathbb{R}^n\) (Therorem 2.17).
    In the following, the order of the basis vectors will be important. Therefore, we write
    \[
        \,\\
        B = (b_1, ..., b_n)
        \,\\
    \]

    and call ordered basis of \(V\).

    <br><br>

    <strong>Definition 2.18 (Coordinates)</strong>.
    Consider a vector space \(V\) and an ordered basis \(B = (b_1, ..., b_n) \) of \(V\).
    For any \(x \in V\), we obtain a unique representation (linear combination)
    \[
        \,\\
        x = \alpha_1 b_1 + \cdots + \alpha_n b_n
        \,\\
    \]

    of \(x\) with respect to \(B\). Then \(\alpha_1, ..., \alpha_n\) are the coordinates of \(x\) with respect to \(B\), and the vector
    \[
        \,\\
        \boldsymbol{\alpha} = 
        \begin{bmatrix}
            \alpha_1 \\
            \vdots \\
            \alpha_n
        \end{bmatrix}
        \in \mathbb{R}^n
        \,\\
    \]

    is the coordinate vector (coordinate representation) of \(x\) with respect to the ordered basis \(B\).
    A basis effectively defines a coordinate system. We are familiar with the
    Cartesian coordinate system in two dimensions,
    which is spanned by the standard basis vectors \(e_1\), \(e_2\). 
    In this coordinate system, a vector \(x \in \mathbb{R}^2\)
    has a representation that tells us how to linearly combine \(e_1\) and \(e_2\) to
    obtain \(x\). However, any basis of \(\mathbb{R}^2\) defines a valid coordinate system,
    and the same vector \(x\) from before may have a different coordinate representation in the \((b_1, b_2)\) basis.


    <figure>
        <img src="/img/mathematics-for-machine-learning/mathematics-for-machine-learning-5.png" width="75%" onerror=handle_image_error(this)>
        <figcaption>
            Two different coordinate systems defined by two sets of basis vectors
        </figcaption>
    </figure>

    <br>

    In the figure above, the coordinates of \(x\) with
    respect to the standard basis \(e_1, e_2\) is \([2, 2]^{\top}\). However, with respect to
    the basis \(b_1, b_2\) the same vector \(x\) is represented as \([1.09, 0.72]^{\top}\), 
    i.e., \(x = 1.09b_1 + 0.72b_2\).

    <br><br>

    <div class="mml-example">
        <strong>Example 2.20</strong>
        <br>
        Consider the vector \(x \in \mathbb{R}^2\) with coordinates \( [2, 3]^{\top} \) 
        with respect to the standard basis \((e_1, e_2)\) of \(\mathbb{R}^2\). 
        This standard basis allows us to write \(\color{blue}{x = 2e_1 + 3e_2}\).
        However, we do not have to choose the standard basis to represent the vector \(x\).

        <br><br>

        <figure>
            <img src="/img/mathematics-for-machine-learning/mathematics-for-machine-learning-6.png" width="20%" onerror=handle_image_error(this)>
            <figcaption>
                Different coordinate
                representations of a
                vector \(x\), depending
                on the choice of
                basis
            </figcaption>
        </figure>
        <br>
        If we use \( b_1 = [1, -1]^{\top},\ b_2 = [1, 1]^{\top} \) as the basis,
        the vector \(x\) can be represented as \(\color{orange}{x = -\frac{1}{2} b_1 + \frac{5}{2} b_2}\).
        </div>

        <br><br>
        <strong>Definition 2.19 (Transformation Matrix)</strong>.
        Consider vector spaces \(V, W\) with corresponding ordered bases \(B = (b_1, ..., b_n) \)
        and \(C = (c_1, ..., c_m)\).
        Moreover, we consider a linear mapping \(\Phi : V \rightarrow W  \). For \(j\in \{1,...,n\}\)
        \[
            \,\\
                \Phi(b_j) = \alpha_{1j} c_1 + \cdots + \alpha_{mj} c_m = \sum_{i=1}^m \alpha{ij}c_i
            \,\\
        \]
        is the unique representation of \(\Phi(b_j)\) with respect to \(C\).
        where, \(\alpha_{ij}\) is defined as
        \[
            \,\\
            A_{\Phi}(i, j) = \alpha_{ij}
            \,\\
        \]    
        For example, consider the bases of \(V\) and \(W\) as follows:
        \[
            \,\\
                \begin{align}
                B = 
                \begin{bmatrix}
                    1 & 0\\
                    0 & 1\\
                \end{bmatrix} \quad &\Rightarrow \quad
                b_1 = 
                \begin{bmatrix}
                    1 \\
                    0
                \end{bmatrix}, \,\,
                b_2 = 
                \begin{bmatrix}
                    0 \\
                    1
                \end{bmatrix}

                \,\\
                \,\\

                C = 
                \begin{bmatrix} 
                    1 & 2 \\
                    3 & 0 \\
                    0 & -1 \\
                \end{bmatrix} \quad &\Rightarrow \quad
                c_1 = 
                \begin{bmatrix}
                    1 \\
                    3 \\
                    0
                \end{bmatrix}, \,\,
                c_2 = 
                \begin{bmatrix}
                    2 \\
                    0 \\
                    -1
                \end{bmatrix}
                \end{align}
            \,\\
        \]

        Here, the unique representation of \(\Phi(b_1)\) is
        \[
            \,\\
                \begin{align}
                \Phi(b_1) &= 
                \begin{bmatrix}
                    1 & 2\\
                    3 & 0\\
                    0 & -1
                \end{bmatrix}
                \begin{bmatrix}
                    1\\
                    0
                \end{bmatrix}
                = 
                \begin{bmatrix}
                    1\\
                    3\\
                    0
                \end{bmatrix}
                \,\\
                \,\\

                &= 1 \cdot c_1 + 0 \cdot c_2 = c_1
                \end{align}
            \,\\
        \]

        The coordinates of \(\Phi(b_j)\) with respect to the ordered basis \(C\) of \(W\)
        are the \(j\)-th column of \(A_{\Phi}\).
        Consider vector spaces \(V, W\) with ordered bases \(B, C\) and a linear mapping \(\Phi: V \rightarrow W\)
        with transformation matrix \(A_{\Phi}\).
        If \(\hat{x}\) is the coordinate vector of \(x\in V\) with respect to \(B\) and \(\hat{y}\) the coordinate vector
        of \(y = \Phi(x) \in W \) with respect to \(C\), then
        \[
            \,\\
                \hat{y} = A_{\Phi}\hat{x}
            \,\\
        \]

        This means that the transformation matrix can be used to map coordinates
        with respect to an ordered basis in \(V\) to coordinates with respect to an
        ordered basis in \(W\).

        <br><br>

        <div class="mml-example">
            <strong>Example 2.22 (Linear Transformations of Vectors)</strong>
            <br>

            We consider three linear transformations of a set of vectors in \(\mathbb{R}^2\) with 
            \[
                \,\\
                A_1 =
                \begin{bmatrix} 
                    \cos(\frac{\pi}{4}) & -\sin(\frac{\pi}{4}) \\ 
                    \sin(\frac{\pi}{4}) & \cos(\frac{\pi}{4})
                \end{bmatrix}, \quad
                A_2 = 
                \begin{bmatrix} 
                    2 & 0 \\
                    0 & 1
                \end{bmatrix}, \quad
                A_3 = \frac{1}{2} 
                \begin{bmatrix} 
                    3 & -1 \\
                    1 & -1
                \end{bmatrix}
                \,\\
            \]

            The figure below shows three examples of linear transformations applied to a set of vectors.
            In the figure, the original data on the far left shows the vectors in \(\mathbb{R}^2\).
            The vectors are arranged in a square, with each dot corresponding to \((x_1, x_2)\)-coordinates.
            When we use matrix \(A_1\) to apply linear transformations to the original vectors, we obtain the second vectors,
            which is rotated by \(45^{\small{\circ}}\).
            If we apply \(A_2\), the data will be stretched along the x-axis by a factor of \(2\).

            <figure>
                <img src="/img/mathematics-for-machine-learning/mathematics-for-machine-learning-7.png" onerror=handle_image_error(this)>
                <figcaption>
                    From the left, <br>
                    Original data ·
                    Rotation by 45° ·
                    Stretch along the horizontal axis ·
                    General linear mapping
                </figcaption>
            </figure>
        </div>
        
        <br><br>

        <h4>2.7.2 Basis Change</h4> 
        <strong><i>Remark</i></strong>.
        In the following, we will have a closer look at how transformation matrices
        of a linear mapping \(\Phi: V \rightarrow W \) change if we change the bases in \(V\) and \(W\).

        We effectively get different coordinate representations of the
        identity mapping \(\text{id}_{V}\).
        In the context of Figure 4, this would mean to
        map coordinates with respect to \((e_1, e_2)\) onto coordinates with respect to
        \((b1, b2)\) without changing the vector \(x\).
        By changing the basis and correspondingly the representation of vectors, the transformation matrix with
        respect to this new basis can have a particularly simple form that allows
        for straightforward computation.

        <br><br>

        Consider a vector \(x\) with coordinates \((2, 3)\) in the standard basis of \(\mathbb{R}^2\).
        In this basis, the vector \(x\) is represented as \(2 e_1 + 3 e_2\).
        If we choose the basis \(a = [1, 0]^{\top} = c_1\) and \(b = [1, 3]^{\top} = c_2\), \(C = \{[c_1, c_2]\}\) then \(x\) can be represented as \(c_1 + c_2\).

        <figure>
            <img src="/img/mathematics-for-machine-learning/mathematics-for-machine-learning-8.png" width="50%" onerror=handle_image_error(this)>
            <figcaption>
                <a href="https://plusthemath.tistory.com/251">Basis Change</a>. <br>
                Different coordinate representations
                \[
                    \,\\
                    \left[
                        x
                    \right]_E = 
                    \begin{bmatrix}
                        2 \\ 
                        3
                    \end{bmatrix}, \quad
                    \left[
                        x
                    \right]_C = 
                    \begin{bmatrix}
                        1 \\ 
                        1
                    \end{bmatrix}
                    \,\\
                \]
            </figcaption>
        </figure>

        <br>

        <strong><i>Remark</i></strong> (Basis Change Transformation Matrix).
        A basis change transformation matrix is a matrix that converts the coordinate representation of a vector
        \(x\) from one basis \(B = (b_1, b_2)\) to another basis \(\hat{B} = (\hat{b_1}, \hat{b_2})\).
        In general the vector \(x\) can be written as
        \[
            \,\\
            x = b_1 x  + b_2 x  = \hat{b_1} \hat{x}  + \hat{b_2} \hat{x} 
            \,\\
        \]

        If each vector of \(B\) is expressed in terms of \(\hat{B}\) as

        \[
            \,\\
                b_1 = p \hat{b_1} + r \hat{b_2}
                \\
                b_2 = q \hat{b_1} + s \hat{b_2}
            \,\\
        \]

        then

        \[
            \,\\
            [x]_{\hat{B}}
            =
            \begin{bmatrix}
                p & q \\
                r & s
            \end{bmatrix}^{-1}
            [x]_{B}
            \,\\
        \]

        The matrix 
        \[
            \,\\
            [P]^{B}_{\hat{B}}
            =
            \begin{bmatrix}
                p & q \\
                r & s
            \end{bmatrix}^{-1}
            \,\\
        \]

        is called the basis change transformation matrix from \(B\) to \(\hat{B}\).
        For example, consider the coordinates \( (x, y) \) in the standard basis \(E\) of \(\mathbb{R}^2\),
        and the coordinates \(\hat{x}, \hat{y}\) in the basis \(C = \{ [1, 0]^{\top}, [1, 3]^{\top} \}\).
        \[
            \,\\
            \begin{align}
            x 
            \begin{bmatrix}
                1\\
                0
            \end{bmatrix}
            + 
            y
            \begin{bmatrix}
                0\\
                1
            \end{bmatrix}
            &=
            \hat{x}
            \begin{bmatrix}
                1\\
                0
            \end{bmatrix}
            +
            \hat{y}
            \begin{bmatrix}
                1\\
                3
            \end{bmatrix}

            \,\\
            \,\\
            
            \Rightarrow
            \begin{bmatrix}
                1 & 0\\
                0 & 1
            \end{bmatrix}
            \begin{bmatrix}
                x \\
                y 
            \end{bmatrix}
            &=
            \begin{bmatrix}
                1 & 1 \\
                0 & 3
            \end{bmatrix}
            \begin{bmatrix}
                \hat{x} \\
                \hat{y}
            \end{bmatrix}
            \end{align}
            \,\\
        \]

        Therefore, 

        \[
            \,\\
            \begin{align}
            \begin{bmatrix}
                \hat{x} \\
                \hat{y}
            \end{bmatrix}
            &= 
            \begin{bmatrix}
                1 & 0 \\
                0 & 1 \\
            \end{bmatrix}
            \begin{bmatrix}
                1 & 1 \\
                0 & 3 \\
            \end{bmatrix}^{-1}
            \begin{bmatrix}
                x \\
                y
            \end{bmatrix}

            \,\\
            \,\\

            &= 

            \frac{1}{3}
            \begin{bmatrix}
                3 & -1 \\
                0 & 1
            \end{bmatrix}
            \begin{bmatrix}
                x \\
                y
            \end{bmatrix}

            \end{align}
            \,\\
        \]

        Here, \( \frac{1}{3}\begin{bmatrix} 3 & -1 \\ 0 & 1 \end{bmatrix} \) is the basis change transformation matrix,
        denoted by \( [P]_E^C \). In other words, it is the matrix
        that maps the coordinates in \(E\) to those in \(C\).
        To express the vector \((2, 3)\) in the standard basis \(E\) (Fig. 7) as coordinates relative to the basis \(C\),
        we can multiply it by \([P]_E^C\) as obtained above to get its representation in basis \(B\):
        \[
            \,\\
            \begin{align}
            [P]_E^C 
            \begin{bmatrix}
                2 \\
                3
            \end{bmatrix}
            &= 
            \frac{1}{3}
            \begin{bmatrix}
                3 & -1 \\
                0 & 1
            \end{bmatrix}
            \begin{bmatrix}
                2 \\
                3
            \end{bmatrix}
            \,\\
            \,\\
            &=
            \begin{bmatrix}
                1 \\
                1
            \end{bmatrix}
            \end{align}
            \,\\
        \]

        <h4>2.7.3 Image and Kernel</h4>
        The image and kernel of a linear mapping are vector subspaces with certain important properties.
        
        
        <br><br>

        <strong>Definition 2.23</strong> (Image and Kernel).

        <br><br>

        For \(\Phi : V \rightarrow W\), we define the kernel/nullspace
        \[
            \,\\
            \ker(\Phi) := \Phi^{-1}(0w) = \{ v \in V: \Phi(v) = 0w \}
            \,\\
        \]

        and the image/range
        \[
            \,\\
            \text{Im}(\Phi) := \Phi(V) = \{ w \in W | \exists v \in V: \Phi(v) = w \}
            \,\\
        \]


        Intuitively, the kernel is the set of vectors \(v \in V\) that \(\Phi\) maps onto the neutral element \(\boldsymbol{0}_W \in W\).
        The image is the set of vectors \(w \in W\) that can be reached by \(\Phi\) from any vector in \(V\).

        <br><br>
        <figure>
            <img src="/img/mathematics-for-machine-learning/mathematics-for-machine-learning-9.png" width="70%" onerror=handle_image_error(this)>
            <figcaption>
                Kernel and image of a linear mapping
            </figcaption>
        </figure>
        <br><br>

        <strong><i>Remark</i></strong>.
        Consider a linear mapping \(\Phi : V \rightarrow W \), where \(V, W\) are vector spaces:
        <ul>
            <li>
                It always holds that \(\Phi(\boldsymbol{0}_V) = \boldsymbol{0}_W \) and,
                therefore, \(\boldsymbol{0}_V \in \ker (\Phi) \).
                In particular, the null space is never empty.
            </li>
            <li>
                \(\text{Im}(\Phi) \subseteq W\) is a subspace of \(W\), 
                and \(\ker(\Phi) \subseteq V\) is a subspace of \(V\).
            </li>
            <li>
                \(\Phi\) is injective (one to one) if and only if \(\ker(\Phi) = \{\boldsymbol{0}\}\).
            </li>
            <li>
                \(\text{rk}(A) = \dim(\text{Im}(\Phi)) \)
            </li>
            <li>
                The kernel/null space \(\ker(\Phi)\) is the general solution to the homogeneous system of 
                inear equations \(\boldsymbol{A}x = \boldsymbol{0}\)
            </li>
            <li>
                \(\dim(\ker(\Phi)) + \dim(\text{Im})(\Phi) = \dim(V) \)
            </li>
            <li>
                If \(\dim(V) = \dim(W)\), the the three-way equivalence
                \[
                    \Phi \text{ is injective} \Longleftrightarrow \Phi \text{ is surjective} \Longleftrightarrow \Phi \text{ is bijective}
                \]
            </li>
        </ul>

    </div>


</div><br><br>

<h3>3. Analytic Geometry</h3>
<div class="article">

    In this chapter, we
    will look at geometric vectors and compute their lengths and distances
    or angles between two vectors.
    Inner products and their corresponding norms and metrics capture
    the intuitive notions of similarity and distances.

    <br><br>

    <h4>3.1 Norms</h4>
    <strong>Definition 3.1</strong> (Norm).
    A norm on a vector space \(V\) is a function
    \[
        \,\\
        \begin{align}
        \| \cdot \| : V &\rightarrow \mathbb{R}, \\
                      x &\mapsto \| x \|
        \end{align}
        \,\\
    \]

    which assigns each vector \(x\) its length \(\| x \| \in \mathbb{R} \),
    such that for all \(\lambda \in \mathbb{R}\) and \(x, y \in V\) the following hold:
    <ul>
        <li>
            Absolutely homogeneous: \(\| \lambda x \| = |\lambda| \| x \| \)
        </li>
        <li>
            Triangle inequality: \( \| x + y \| \leq \| x \| + \| y \| \)
        </li>
        <li>
            Positive definite: \( \| x \| \ge 0 \) and \(\| x \| = 0 \Longleftrightarrow x = \boldsymbol{0} \)
        </li>
    </ul>

    For any triangle, the sum of the lengths of any two sides must be greater than or equal to the length
    of the remaining side.

    <br><br>
    <div class="mml-example">
        <strong>Example 3.1 (Manhattan Norm)</strong>
        <br>
        The Manhattan norm on \(\mathbb{R}^n\) is defined for \(x \in \mathbb{R}^n \) as
        \[
            \,\\
                \| x \|_1 := \sum_{i=1}^n | x_i |
            \,\\
        \]

        where \( | \cdot | \) is the absolute value, \(x_i\) is the \(i^{th} \) element of the vector \(x\).
        The left side of Fig.9 below shows all vectors \(x \in \mathbb{R}^2\) with \(\|x \|_1 = 1\).
        The Manhattan norm is also called L1 norm.

        <br><br><br>

        <strong>Example 3.2 (Euclidean Norm)</strong>
        <br>
        The Euclidean norm of \(x \in \mathbb{R}^n \) is defined as
        \[
            \,\\
            \| x \|_2 := \sqrt{ \sum_{i=1}^n x_i^2 } = \sqrt{x^{\top}x}
            \,\\
        \]

        and computes the Euclidean distance of \(x\) from the origin.
        The right side of Fig.9 shows all vectors \(x \in \mathbb{R}^2 \) with \( \|x\|_2 = 1 \).
        The Euclidean norm is also called L2 norm.

        <br><br>
        <figure>
            <img src="/img/mathematics-for-machine-learning/mathematics-for-machine-learning-10.png" width="60%" onerror=handle_image_error(this)>
            <figcaption>
                From the left, Manhattan norm · Euclidean norm
                <br>
                The red lines indiceate the set of vectors with norm 1
            </figcaption>
        </figure>
    </div>

    <br><br>

    <h4>3.2 Inner Products</h4>
    A major purpose of inner products is to determine whether
    vectors are orthogonal to each other.

    <br><br>

    <h4>3.2.1 Dot Product</h4>
    We may already be familiar with a particular type of inner product,
    the scalar product/dot product in \(\mathbb{R}^n\), which is given by
    \[
        \,\\
            \boldsymbol{x}^{\top}\boldsymbol{y} = \sum_{i=1}^nx_i y_i
        \,\\
    \]

    <h4>3.2.2 General Inner Products</h4>
    <strong>Definition 3.2.</strong> Let \(V\) be a vector space and \(\Omega : V \times V \rightarrow \mathbb{R} \)
    be a mapping that takes two vectors and maps them onto a real number. Then
    <ul>
        <li>
            \(\Omega\) is called symmetric if \(\Omega(x, y) = \Omega(y, x), \forall x, y \in V  \).
        </li>
        <li>
            \(\Omega\) is called positive definite if
            \[
                \forall x \in V \setminus \{\boldsymbol{0}\} : \Omega(x, x) > 0, \quad \Omega(\boldsymbol{0}, \boldsymbol{0}) = 0.
            \]
        </li>
        <!-- <li>
            Typically write \( \langle x, y \rangle \) instead of \(\Omega(x, y)\).
            \[
                \langle x, y \rangle = x_1 y_1 + x_2 y_2 + \cdots + x_n y_n
            \]
        </li> -->
    </ul>

    <br>

    <h4>3.2.3 Symmetric, Positive Definite Matrices</h4>
    Consider an \(n\)-dimensional vector space \(V\) with an inner product \( \langle \cdot, \cdot \rangle : V \times V \rightarrow \mathbb{R} \)
    and an ordered basis \(B = b_1, \cdots, b_n \) of \(V\).
    Any vectors \(x, y \in V \) can be written as linear combinations of the basis vectors
    so that \(x = \sum_{i=1}^n \psi_i b_i \in V \) and \( y = \sum_{j=1}^n \lambda_j b_j \in V \) for suitable \(\psi_i, \lambda_j \in \mathbb{R} \).
    Due to the bilinearity of the inner product, it holds for all \(x, y \in V\) that
    \[
        \,\\
            \langle x, y \rangle = 
            \left\langle \sum_{i=1}^n \psi_i b_i,\,\, \sum_{j=1}^n \lambda_j b_j \right\rangle =
            \sum_{i=1}^n \sum_{j=1}^n \psi_i \langle b_i, b_j \rangle \lambda_j
        \,\\    
    \]

    The mid expression expanded by bilinearity as
    \[
        \,\\
        \left\langle \sum_{i=1}^n \psi_i b_i,\,\, \sum_{j=1}^n \lambda_j b_j \right\rangle = 
        \sum_{i=1}^n \psi_i \left\langle b_i, \sum_{j=1}^n \lambda_j b_j \right\rangle

        \,\\
        \,\\

        \left\langle b_i, \sum_{j=1}^n \lambda_j b_j \right\rangle = \sum_{j=1}^n \lambda_j \langle b_i, b_j \rangle

        \,\\
        \,\\

        \left\langle \sum_{i=1}^n \psi_i b_i,\,\, \sum_{j=1}^n \lambda_j b_j \right\rangle =
        \sum_{i=1}^n \sum_{j=1}^n \psi_i \langle b_i, b_j \rangle \lambda_j

        \,\\
    \]

    In the matrix form, 
    where
    \(\hat{x} = (\psi_1, \cdots, \psi_n)^\top\),
    \(\hat{y} = (\lambda_1, \cdots, \lambda_n)^\top\),

    \[
        \,\\
        \langle x, y \rangle = \hat{x}^{\top}A\hat{y} = \sum_{i, j} \psi_i A_{ij} \lambda_j
        \,\\
    \]

    where, \(\hat{x}, \hat{y}\) are the coordinates of \(x\) and \(y\) with respect to the basis \(B\).
    \(A_{ij}\) is \(\langle b_i, b_j \rangle \):
    \[
        \,\\
        A = 
        \begin{bmatrix}
        \langle b_1, b_1 \rangle & \langle b_1, b_2 \rangle & \cdots \\
        \langle b_2, b_1 \rangle & \langle b_2, b_2 \rangle & \cdots \\
        \vdots & \vdots & \ddots
        \end{bmatrix}
        \,\\
    \]

    The inner product \( \langle \cdot, \cdot \rangle \)
    is uniquely determined through \(A\).
    The symmetry of the inner product also means that \(A\) is symmetric.
    Furthermore, the positive definiteness of the inner product implies that
    \[
        \,\\
            \forall x \in V \setminus \{ 0 \} : x^\top Ax > 0
        \,\\
    \]

    <br>

    <strong>Definition 3.4</strong> (Symmetric, Positive Definite Matrix).
    A symmetric matrix \(A \in \mathbb{R}^{n \times n} \) that satisfies the positive definiteness above is called
    symmetric, positive definite, or just positive definite.
    If \(x^\top Ax \ge 0\), then \(A\) is called symmetric, positive semidefinite. 

    <br><br>

    <div class="mml-example">
        <strong>Example 3.4 (Geometric Intuition of Symmetric, Positive Definite Matrices)</strong>
        <br>
        Consider the matrices, and \(x\)
        \[
            \,\\
            A = 
            \begin{bmatrix}
                3 & -1 \\
                -1 & 2
            \end{bmatrix}
            , \quad
            B = 
            \begin{bmatrix}
                -3 & -1 \\
                -1 & 4
            \end{bmatrix}
            , \quad
            x = 
            \begin{bmatrix}
                1 \\
                1
            \end{bmatrix}
            \,\\
        \]

        Results of the \(x^\top Ax\) and \(x^\top Bx\) are as follows respectively 
        \[
            \,\\
            x^\top Ax = 
            \begin{bmatrix}
                1 & 1
            \end{bmatrix}
            \begin{bmatrix}
                3 & -1 \\
                -1 & 2
            \end{bmatrix}
            \begin{bmatrix}
                1 \\ 1
            \end{bmatrix} = 3

            \,\\
            \,\\
            x^\top Bx = 
            \begin{bmatrix}
                1 & 1
            \end{bmatrix}
            \begin{bmatrix}
                3 & -1 \\
                -1 & 4
            \end{bmatrix}
            \begin{bmatrix}
                1 \\ 1
            \end{bmatrix} = -1
            \,\\
        \]

        In a geometric perspective, positive definite means that
        the angle between any vector \(x\) and the vector transformed by the matrix (here, \(Ax\) or \(Bx\))
        remains between -90 and 90 degrees.
        The figure below intuitively illustrates the concept of positive definite.


        <figure>
            <img src="/img/mathematics-for-machine-learning/mathematics-for-machine-learning-20.png" width="70%" onerror=handle_image_error(this)>
            <figcaption>Geometric intuition of Positive Definite</figcaption>
        </figure>

    </div>

    <br><br>

    The following properties hold if \(A\in\mathbb{R}^{n \times n}\) is symmetric and positive definite:
    <ul>
        <li>
            The null space (kernel) of \(A\) consists only of \(\boldsymbol{0}\) because
            \(x^\top Ax > 0, \) for all \(x \neq \boldsymbol{0}\). Therefore 
            \[
                \ker(A) = \{\boldsymbol{0}\}
            \]
        </li>
    </ul>

    <br>

    <h4>3.3 Lengths and Distances</h4>
    <strong><i>Remark</i></strong> <a href="https://en.wikipedia.org/wiki/Cauchy%E2%80%93Schwarz_inequality">(Cauchy-Schwarz Inequality)</a>.
    For an inner product vector space \((V, \langle \cdot, \cdot \rangle) \)
    the induced norm \(\| \cdot \|\) satisfies the Cauchy-Schwarz inequality
    \[
        \,\\

        | \langle x, y \rangle |^2 \le \langle x, x \rangle  \langle y, y \rangle

        \Longleftrightarrow

        | \langle x, y \rangle | \le \sqrt{ \langle x, x \rangle  \langle y, y \rangle } = \|x\| \|y\|
        \,\\
    \]

    Consider two vectors \(x, y \in \mathbb{R}^2 \)
    \[
        \,\\
        x = [1, 2], \quad y = [3, 4].
        \,\\
    \]

    First, compute the inner product:
    \[
        \,\\
        \begin{align}
        \langle x, y \rangle &= 1 \times 3 + 2 \times 4 \\
                             &= 11
        \end{align}
        \,\\
    \]

    Then, compute the norms:
    \[
        \,\\
        \begin{align}
        \|x\| = \sqrt{1^2 + 2^2} &= \sqrt{5} \approx 2.23606797749979 \\
        \|y\| = \sqrt{3^2 + 4^2} &= 5 \\
        \end{align}
        \,\\
    \]

    Now compare both sides of the Cauchy-Schwarz inequality:
    \[
        \,\\
        | \langle x, y \rangle | = 11, \quad \|x\| \|y\| = 5\sqrt{5} \approx 11.180339887498949
        \,\\
    \]

    Therefore, 
    \[
        \,\\
        | \langle x, y \rangle | = 11 \le 11.180339887498949 = \|x\| \|y\|
        \,\\
    \]

    <br><br>
    
    <strong>Definition 3.6</strong> (Distance and Metric).
    Consider an inner product space \( (V, \langle \cdot, \cdot \rangle) \). Then
    \[
        \,\\
        d(x, y) := \|x - y\| = \sqrt{ \langle x-y , x-y \rangle }
        \,\\
    \]

    is called the distance between \(x\) and \(y\) for \(x, y \in V\).
    If we use the dot product as the inner product, then the distance is called Euclidean distance.

    <br><br>

    The mapping \(d\) is called a metric
    \[
        \,\\
        \begin{align}
        d : V \times V &\rightarrow \mathbb{R} \\
                (x, y) &\mapsto d(x, y)
        \end{align}
        \,\\
    \]

    A metric \(d\) satisfies the following:
    <ol>
        <li>
            \(d\) is positive definite, i.e., \(d(x, y) \ge 0 \) for all \(x, y \in V\) and \(d(x, y) = 0 \Longleftrightarrow x = y \).
        </li>
        <li>
            \(d\) is symmetric, i.e., \(d(x, y) = d(y, x) \) for all \(x, y \in V\).
        </li>
        <li>
            Triangle inequality: \(d(x,z) \le d(x,y) + d(y,z) \) for all \(x, y, z \in V\).
        </li>
    </ol>

    <strong><i>Remark</i></strong>.
    We observe that \(\langle x, y \rangle\ \) and \(d(x, y) \) behave in opposite directions.
    Very similar \(x\) and \(y\) will result in a large value for the inner product and a small value for the metric.

    <br><br>

    <h4>3.4 Angles and Orthogonality</h4>
    We use the Cauchy-Schwarz inequality to define angles \(\theta\) in inner product spaces between two vectors \(x, y\).
    Assume that \(x \neq 0, y \neq 0\).

    First, divide both sides of the Cauchy-Schwarz inequality by \( \|x\| \|y\| \).
    Two vectors are not zero vectors. therefore, \(\|x\| \ge 0, \|y\| \ge 0 \), and the direction of inequality are not changed.

    \[
        \,\\
        |\langle x, y \rangle| \le \|x\| \|y\|
        \Longleftrightarrow
        \frac{ | \langle x, y \rangle | }{ \|x\|\|y\| } \le 1
        \,\\
    \]

    Based on the definition of absolute value \( |a| \le 1 \Longleftrightarrow -1 \le a \le 1 \), we get
    \[
        \,\\
        -1 \le \frac{ \langle x, y \rangle }{ \|x\| \|y\| } \le 1
        \,\\
    \]

    Therefore, there exists a unique \(\theta \in [0, \pi] \).
    The number \(\theta\) is the angle between the two vectors \(x, y\).
    \[
        \,\\
        \cos \theta = \frac{ \langle x, y \rangle }{ \|x\| \|y\| }
        \,\\
    \]

    Intuitively, the angle between two vectors tells us how similar their orientations are.
    Based on the above, the inner product can be rewritten as
    \[
        \,\\
        \langle x, y \rangle = \|x\| \|y\| \cos \theta
        \,\\
    \]

    <br>

    <div class="mml-example">
        <strong>Example 3.6 (Angle between Vectors)</strong>
        <br>
        Let us compute the angle between \(x = [1, 1]^\top \in \mathbb{R}^2 \) and \( y = [-1, 1]^\top \in \mathbb{R}^2 \).
        We use the dot product as the inner product. Then we get
        \[
            \,\\
            \begin{align}
            \cos \theta &= \frac{ \langle x, y \rangle }{ \|x\| \|y\| } \\
                        &= \frac{ 1 \times (-1) + 1 \times 1 }{ \sqrt{1^2 + 1^2} \times \sqrt{(-1)^2 + 1^2} } \\
                        &= \frac{0}{2} \\
                        &= 0
            \end{align}
            \,\\
        \]

        and the angle between the two vectors is \(\arccos (0) = \frac{\pi}{2} = 90^{\small{\circ}}\)
    </div>

    <br><br>
    A key feature of the inner product is that it allows us to characterize vectors that are orthogonal.

    <br><br>
    <strong>Definition 3.7</strong> (Orthogonality).
    Two vectors \(x\) and \(y\) are orthogonoal if and only if \( \langle x, y, \rangle = 0\) and we write \(x \perp y \).
    If \( \| x \| = \| y \| = 1 \), i.e., the vectors are unit vectors, then \(x\) and \(y\) are orthonormal.
    An implication of this definition is that the \(\boldsymbol{0}\)-vector is orthogonal to every vector in the vector space.

    <br><br>

    <strong><i>Remark</i></strong>. Geometrically we can think of orthogonal vectors as having a right angle with respect to a specific inner product.

    <br><br>

    <strong>Definition 3.8</strong> (Orthogonoal Matrix).
    A square matrix \(A \in \mathbb{R}^{n \times n} \) is an orthogonal matrix
    if and only if its columns are orthonormal so that
    \[
        \,\\
        AA^\top = I = A^\top A
        \,\\
    \]

    which implies that 
    \[
        \,\\
        A^\top = A^{-1}
        \,\\
    \]
    i.e., the inverse is obtained by simply transposing the matrix.

    <br><br>

    Transformations by orthogonal matrices are special because the length of a vector \(x\) is not changed
    when transforming it using an orthogonal matrix \(A\).
    Moreover, the angle  between any two vectors \(x, y\) is also unchanged when transforming both of them using
    an orthogonal matrix \(A\). Assuming the dot product as the inner product,
    the angle between \(Ax\) and \(Ay\) is given as
    \[
        \,\\
        \begin{align}
        \cos \theta &= \frac{ ( Ax )^\top ( Ay ) }{ \| Ax \| \| Ay \| }
        \,\\
        \,\\
                    &= \frac{ x^\top A^\top Ay }{ \sqrt{ (Ax)^\top Ax  (Ay)^\top Ay } }
        \,\\
        \,\\
                    &= \frac{ x^\top A^\top Ay }{ \sqrt{ (x^\top A^\top A x) (y^\top A^\top A y) }}
        \,\\
        \,\\
                    &= \frac{ x^\top y }{ \sqrt{ (x^\top x) (y^\top y) } }
        \,\\
        \,\\
                    &= \frac{ x^\top y }{ \|x\| \|y\|  }
        \end{align}
        \,\\
    \]

    which gives exactly the angle between \(x\) and \(y\).
    This means that ortohonal matrices \(A\) with \( A^\top = A^{-1} \) preserve both angles and distances.
    It turns out that orthogonal matrices define transformations that are rotations.

    <br><br>

    <h4>3.5 Orthonormal Basis</h4>
    We will discuss the special case where the basis
    vectors are orthogonal to each other and where the length of each basis
    vector is 1. We will call this basis then an orthonormal basis.

    <br><br>

    <strong>Definition 3.9</strong> (Orthonormal Basis).
    Consider an \(n\)-dimensional vector space \(V\) and a basis \( \{ \boldsymbol{b}_1, \cdots, \boldsymbol{b}_n \} \) of \(V\).
    \[
        \,\\
        \begin{align}
        \langle \boldsymbol{b}_i, \boldsymbol{b}_j \rangle &= 0 \quad \text{for} \,\, i \neq j \\
        \langle \boldsymbol{b}_i, \boldsymbol{b}_i \rangle &= 1
        \end{align}
        \,\\
    \]

    for all \(i, j = 1, \cdots, n\) then the basis is called an orthonormal basis (ONB).

    <br><br>

    <div class="mml-example">
        <strong>Exampe 3.8 (Orthonormal Basis)</strong>
        <br>
        The standard basis for a Euclidean vector space \( \mathbb{R}^n \) is an
        orthonormal basis, where the inner product is the dot product of vectors.
        In \(\mathbb{R}^2\), consider the orthogonal vectors \(b_1, b_2\)
        \[
            \,\\
            b_1 = [1, 1]^\top, \quad b_2 = [1, -1]^\top
            \,\\
        \]

        To make \(b_1\) and \(b_2\) orthonormal, divide \(b_1\) by \( \| b_1 \| \),
        and divdie \( b_2 \) by \( \|b_1\| \).
        \[
            \,\\
            \begin{align}
            \tilde{b}_1 &= \frac{1}{\| b_1 \|} [1, 1]^\top = \frac{1}{\sqrt{2}} [1, 1]^\top
            \,\\
            \,\\
            \tilde{b}_2 &= \frac{1}{\| b_2 \|} [1, -1]^\top  = \frac{1}{\sqrt{2}} [1, -1]^\top
            \end{align}
            \,\\
        \]

        Then, compute \( \tilde{b}_1^\top \tilde{b}_2 \) and \( \| \tilde{b}_1 \|, \| \tilde{b}_2 \| \) 
        \[
            \,\\
                \tilde{b}_1^\top \tilde{b}_2 = 
                \begin{bmatrix}
                    \frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}}
                \end{bmatrix}
                \begin{bmatrix}
                    \frac{1}{\sqrt{2}} \\ 
                    -\frac{1}{\sqrt{2}}
                \end{bmatrix} =
                \frac{1}{2} - \frac{1}{2} = 0
            \,\\
            \,\\
            \,\\

            \| \tilde{b}_1 \| = \sqrt{\left( \frac{1}{\sqrt{2}} \cdot \frac{1}{\sqrt{2}} \right) + \left( \frac{1}{\sqrt{2}} \cdot \frac{1}{\sqrt{2}} \right)} = \sqrt{ \frac{1}{2} + \frac{1}{2} } = 1
            \,\\
            \,\\
            \,\\
            \| \tilde{b}_2 \| = \sqrt{\left( \frac{1}{\sqrt{2}} \cdot \frac{1}{\sqrt{2}} \right) + \left( -\frac{1}{\sqrt{2}} \cdot (-\frac{1}{\sqrt{2}}) \right)} = \sqrt{ \frac{1}{2} + \frac{1}{2} } = 1
            \,\\
        \]

        Therefore, the vectors \( \tilde{b}_1, \tilde{b}_2 \) form an orthonormal basis. 
    </div>

    <br><br>

    <h4>3.6 Orthogonal Complement</h4>
    Consider a \(D\)-dimensional vector space \(V\) and
    an \(M\)-dimensional subspace \(U \subseteq V \).
    Then its orthogonal complement \(U^\perp\) is a \( (D-M) \)-dimensional subspace
    of \(V\) and contains all vectors in \(V\) that are orthogonal to every vector in \(U\).
    Furthermore, \(U \cap U^\perp = \{ \boldsymbol{0}\} \).

    <br><br>

    <h4>3.7 Inner Product of Functions</h4>
    <!-- The inner products we discussed so far were defined for vectors with a
    finite number of entries.  -->
    We can think of a vector \(x\in \mathbb{R}^n\) as a function
    with \(n\) function values.

    An inner product of two functions \( u: \mathbb{R} \rightarrow \mathbb{R} \)
    and \( v: \mathbb{R} \rightarrow \mathbb{R} \)
    can be defined as the definite integral
    \[
        \,\\
            \langle u, v \rangle := \int_a^b u(x)v(x)\, dx
        \,\\
    \]

    <figure>
        <img src="/img/mathematics-for-machine-learning/mathematics-for-machine-learning-11.png" width="80%" onerror=handle_image_error(this)>
        <figcaption>
            Integrand of \(f(x) = \sin(x) \cos(x)\)
        </figcaption>
    </figure>
    <br><br>

    <h4>3.8 Orthogonal Projections</h4>
    In machine learning, we often deal
    with data that is high-dimensional. High-dimensional data is often hard
    to analyze or visualize. 
    
    <br><br>
    However, high-dimensional data quite often possesses 
    the property that only a few dimensions contain most information,
    and most other dimensions are not essential to describe key properties
    of the data.
    When we compress or visualize high-dimensional data, we
    will lose information. To minimize this compression loss, we ideally find
    the most informative dimensions in the data. 

    <br><br>

    In this chapter, we will discuss
    some of the fundamental tools for data compression. More specifically, we
    can project the original high-dimensional data onto a lower-dimensional
    feature space.

    <br><br>

    For example, machine learning algorithms, such as 
    principal component analysis (PCA), and deep auto-encoders,
    heavily exploit the idea of dimensionality reduction.

    <br><br>

    <strong>Definition 3.10</strong> (Projection).
    Let \(V\) be a vector space and \(U \subseteq V\) a subspace of \(V\).
    A linear mapping \(\pi: V \rightarrow U\) is called a projection if the following holds
    \[
        \,\\
        \pi^2 = \pi
        \,\\
    \]

    Since linear mappings can be expressed by transformation matrices, 
    the matrix representation also satisfies the property above.
    \[
        \,\\
        P^2_\pi = P_\pi
        \,\\
    \] 

    <br>
    <h4>3.8.1 Projection onto One-Dimensional Subspaces (Lines)</h4>
    Using geometric arguments, let us characterize some properties of the projection \(\pi_U (x)\)
    , where \(b \in \mathbb{R}^n\), and the subspace is \(U \subseteq \mathbb{R}^n\) spanned by \(b\).
    <br><br>
    <figure>
        <img src="/img/mathematics-for-machine-learning/mathematics-for-machine-learning-12.png" width="60%" onerror=handle_image_error(this)>
        <figcaption>
            Orthogonal Projections
        </figcaption>
    </figure>
    <br>
    <ul>
        <li>
            The projection \(\pi_U (x) \) is closest to \(x\),
            where "closest" implies that the distance \( \| x - \pi_U (x) \| \) is minimal.
        </li>
        <li>
            The vector \(\pi_U (x) - x\) is orthogonal to \(U\).
        </li>
        <li>
            The orthogonality condition yields \( \langle \pi_U (x) - x, b \rangle = 0 \). 
        </li>
        <li>
            The projection \(\pi_U (x)\) of \(x\) onto \(U\) must be an element of \(U\).
            Hence, \(\pi_U (x) = \lambda b\) for some \(\lambda \in \mathbb{R}\).
        </li>
        <li>
            The projected vector can be obtained by the dot product and normalized basis \(\tilde{b} = \frac{b}{\|b\|} \)
            \[
                \pi_U (x) = (x^\top \tilde{b}) \tilde{b}
            \]
        </li>
    </ul>

    <br>

    In the following steps, we determine the \(\lambda\), 
    the projection \(\pi_U (x) \in U\), and the projection matrix \(P_{\pi}\) that maps any \(x \in \mathbb{R}^n\) onto \(U\).
    <br><br>
    Finding \(\lambda\). The orthogonality condition yields
    \[
        \,\\
        \langle x-\pi_U (x), b \rangle = 0 \Longleftrightarrow \langle x - \lambda b, b \rangle = 0
        \,\\
    \]

    In the vector form, the right expression is
    \[
        \,\\
        \langle x - \lambda b, b \rangle = 0 \,\, \Longrightarrow \,\, (x - \lambda b)^\top b = 0
        \,\\
    \]

    By expanding it, we get
    \[
        \,\\
        \begin{align}
        (x - \lambda b)^\top b &= (x^\top - (\lambda b)^\top) b  
        \,\\
        \,\\
                                &= x^\top b - \lambda (b^\top b)
        \,\\
        \,\\
                                &= 0
        \end{align}
        \,\\
    \]

    Now, solve the expression for \(\lambda\)
    \[
        \,\\
        x^\top b - \lambda (b^\top b) = 0
        \,\\
        \,\\
        \lambda = \frac{x^\top b}{b^\top b} = \frac{b^\top x}{\|b\|^2}
        \,\\
    \]

    Therefore, 
    \[
        \,\\
        \pi_U (x) = \lambda b = \frac{b^\top x}{\|b\|^2} b
        \,\\
    \]

    Hence,

    \[
        \,\\
        \pi_U (x) = \lambda b = b \lambda = P_\pi (x) = b\frac{b^\top x}{\|b\|^2} = \frac{b b^\top}{\|b\|^2}x
        \,\\ 
        \,\\
        P_\pi = \frac{b b^\top}{\|b\|^2} 
        \,\\
    \]
    
    Here, the \(bb^\top\) is a symmetric matrix, and \(\|b\|^2 = \langle b, b \rangle \) is a scalar.

    <br><br>

    <strong><i>Remark</i></strong>. The projection matrix \(\pi_U (x) \in \mathbb{R}^n \) is 
    still an \(n\)-dimensional vector, not a scalar.

    <br><br>

    <div class="mml-example">
        <strong>Example 3.10 (Projection onto a Line)</strong>
        <br>
        Find the projeection matrix \(P_\pi\) onto the line through the origin spanned by \(b = [1, 2, 2]^\top \).
        \(b\) is a direction and a basis of the one dimensional subspace.
        Based on the above expression, \(P_\pi\) for \(b\) is
        \[
            \,\\
            \begin{align}
                P_\pi &= \frac{b b^\top}{\|b\|^2} = \frac{b b^\top}{b^\top b}
                \,\\
                \,\\
                b^\top b &= 1 + 4 + 4 = 9
                \,\\
                \,\\

                b b^\top &= 
                \begin{bmatrix}
                    1 \\
                    2 \\
                    2 \\
                \end{bmatrix} \begin{bmatrix} 1& 2& 2 \end{bmatrix}
                = 
                \begin{bmatrix}
                    1 & 2 & 2 \\
                    2 & 4 & 4 \\
                    2 & 4 & 4 \\
                \end{bmatrix}
                \,\\
                \,\\
                P_\pi &= \frac{1}{9}
                \begin{bmatrix}
                1 & 2 & 2 \\
                2 & 4 & 4 \\
                2 & 4 & 4 \\
                \end{bmatrix}
            \end{align}
            \,\\
        \]

        Let us now choose a particular \(x\) and see whether it lies in the subspace spanned by \(b\).
        The illustration below shows the result of applying the projection matrix \(P_\pi\) to \(x\).

        <br><br>

        <figure>
            <img src="/img/mathematics-for-machine-learning/mathematics-for-machine-learning-13.png" width="80%" onerror=handle_image_error(this)>
            <figcaption>
                Projection onto a Line
            </figcaption>
        </figure>

    </div>
    <br><br>

    <h4>3.8.2 Projection onto General Subspaces</h4>
    In the following, we look at orthogonal projections of vectors \(x \mathbb{R}^n\)
    onto lower-dimensional subspaces \(U\subseteq \mathbb{R}^n\) with \(\dim(U) = m \ge 1 \).

    <br><br>

    Assume that \( (b_1, \cdots, b_m) \) is an ordered basis of \(U\).
    Any projeection \(\pi_U (x)\) onto \(U\) is necessarily an element of \(U\).
    Therefore, they can be represented as linear combinations of the basis vectors \(\pi_U (x) = \sum_{i=1}^m \lambda_i b_i \).

    <br><br>

    We follow three-step procedure to find the projection \(\pi_U (x)\) and the projection matrix \(P_\pi\):
    <ol>
        <li>
            Find the coordinates coordinates \(\lambda_1, \cdots, \lambda_m\) of the projection, such that the linear combination
            is closest to \(x \mathbb{R}^n\).
            \[
                \begin{align}
                &\pi_U (x) = \sum_{i=1}^m \lambda_i b_i = \boldsymbol{B}\boldsymbol{\lambda}
                \,\\
                \,\\
                &\boldsymbol{B} = [b_1, \cdots, b_m] \in \mathbb{R}^{n \times m}, \quad \boldsymbol{\lambda} = [\lambda_1, \cdots, \lambda_m]^\top \in \mathbb{R}^m
                \end{align}
                \,\\
            \]
            The "closest" means "minimum distance",
            which implies that the vector connecting \(\pi_U (x) \in U\) and \(x \in \mathbb{R}^n\) must be orthogonal to all basis vectors of \(U\).
            \[
                \langle b_1, x - \pi_U (x) \rangle = b_1^\top(x - \pi_U (x)) = 0
                \\
                \vdots
                \\
                \langle b_m, x - \pi_U (x) \rangle = b_m^\top(x - \pi_U (x)) = 0
                \,\\
            \]

            In the matrix form, with \(\pi_U (x) = \boldsymbol{B\lambda}\) we can written as
            \[
                \boldsymbol{B}^\top (x - \boldsymbol{B\lambda}) = 0
                \,\\
                \,\\
                \boldsymbol{B}^\top x - \boldsymbol{B}^\top \boldsymbol{B\lambda} = 0
                \,\\
            \]

            Then, solve the expression for \(\boldsymbol{\lambda}\)
            \[
                \boldsymbol{B}^\top x = \boldsymbol{B}^\top \boldsymbol{B\lambda}
                \,\\
                \,\\
                (\boldsymbol{B}^\top B)^{-1} \boldsymbol{B}^\top x = (\boldsymbol{B}^\top B)^{-1} \boldsymbol{B}^\top \boldsymbol{B\lambda}
                \,\\
                \,\\
                \boldsymbol{\lambda} = (\boldsymbol{B}^\top B)^{-1} \boldsymbol{B}^\top x
                \,\\
            \]
        </li>
        <li>
            Find the projection \(\pi_U (x) \in U\). We already established that \(\pi_U (x) = \boldsymbol{B\lambda}\)
            \[
                \pi_U (x) = \boldsymbol{B\lambda} = \boldsymbol{B} (\boldsymbol{B}^\top B)^{-1} \boldsymbol{B}^\top x
                \,\\
            \]
        </li>
        <li>
            Find the projection matrix \(P_\pi\). From the above, we can obtain the projection matrix immediately
            with \(\pi_U (x) = P_\pi (x)\)
            \[
                P_pi = \boldsymbol{B\lambda} = \boldsymbol{B} (\boldsymbol{B}^\top B)^{-1} \boldsymbol{B}^\top
                \,\\
            \]
        </li>
    </ol>

    <br><br>

    <div class="mml-example">
        <strong>Example 3.11 (Projection onto a Two-dimensional Subspace)</strong>
        <br>
        For a subspace \(U\) with ordered basis \(b_1 = [2, 0, 0]^\top\) and \(b_2 = [0, 2, 2]^\top\),
        we can write the basis vectors of \(U\) into a maxtrix \(\boldsymbol{B}\) 
        \[
            \,\\
            \boldsymbol{B} = 
            \begin{bmatrix}
                2 & 0 \\
                0 & 2 \\
                0 & 2 \\
            \end{bmatrix}
            \,\\
        \]

        Then, we can compute the projection matrix \( P_\pi \)
        \[
            \,\\
            P_\pi = \boldsymbol{B} (\boldsymbol{B}^\top \boldsymbol{B})^{-1} \boldsymbol{B}^\top = 
            \frac{1}{2}
            \begin{bmatrix}
                2 & 0 & 0 \\
                0 & 1 & 1 \\
                0 & 1 & 1 \\
            \end{bmatrix}
            \,\\
        \]

        We can now apply the projection matrix to \( x_1=[0, -3, 1]^\top, x_2=[-1, 0, 3]^\top \). 
        The illustration below shows the result of applying \(P_\pi (x)\).

        <br><br>

        <figure>
            <img src="/img/mathematics-for-machine-learning/mathematics-for-machine-learning-14.png" width="80%" onerror=handle_image_error(this)>
            <figcaption>
                Projection onto a Two-dimensional Subspace
            </figcaption>
        </figure>

        <br><br>

        To verify the results, we can check whether the vector \(P_\pi (x) - x\) is orthogonal to all basis vectors of \(U\).
        Here, the \(X\) is consisting of \(x_1, x_2\).
        \[
            \,\\
            \begin{align}
            \boldsymbol{X} &= 
            \begin{bmatrix}
                0 & -1 \\
                -3 & 0 \\
                1 & 3 \\
            \end{bmatrix}
            \,\\
            \,\\

            P_\pi (\boldsymbol{X}) &= 
            \frac{1}{2}
            \begin{bmatrix}
                2 & 0 & 0 \\
                0 & 1 & 1 \\
                0 & 1 & 1 \\
            \end{bmatrix}
            \begin{bmatrix}
                0 & -1 \\
                -3 & 0 \\
                1 & 3 \\
            \end{bmatrix}
            =
            \begin{bmatrix}
                0 & -1 \\
                -1 & 1.5 \\
                -1 & 1.5 
            \end{bmatrix}
            \,\\
            \,\\
            P_\pi(\boldsymbol{X}) - \boldsymbol{X} &= 
            \begin{bmatrix}
                0 & -1 \\
                -3 & 0 \\
                1 & 3 \\
            \end{bmatrix}
            -
            \begin{bmatrix}
                0 & -1 \\
                -1 & 1.5 \\
                -1 & 1.5 
            \end{bmatrix}
            =
            \begin{bmatrix}
                0 & 0 \\
                -2 & -1.5 \\
                2 & 1.5 
            \end{bmatrix}
            \,\\
            \,\\
            \boldsymbol{B}^\top (P_\pi(\boldsymbol{X}) - \boldsymbol{X}) &= 
            \begin{bmatrix}
                2 & 0 & 0 \\
                0 & 2 & 2 
            \end{bmatrix}
            \begin{bmatrix}
                0 & 0 \\
                -2 & -1.5 \\
                2 & 1.5 
            \end{bmatrix}
            =
            \begin{bmatrix}
                0 & 0 \\
                0 & 0 \\
            \end{bmatrix}
            \end{align}
            \,\\
        \]
    </div>

    <br><br>
    
    <strong><i>Remark.</i></strong>
    If the basis is an orthonormal basis (ONB),
    the projection equation \(\pi_U (x)\) is simplified to
    \[
        \,\\
        \begin{align}
        \pi_U (x) &= \boldsymbol{B} (\boldsymbol{B}^\top \boldsymbol{B})^{-1} \boldsymbol{B}^\top x
        \,\\
        \,\\
                  &= \boldsymbol{B} I \boldsymbol{B}^\top x
        \,\\
        \,\\
                  &= \boldsymbol{B} \boldsymbol{B}^\top x
        \end{align}
        \,\\
    \]

    since an ONB satisfies \(\boldsymbol{B}^\top \boldsymbol{B} = \boldsymbol{I}\).
    This means that we no longer have to compute the inverse, which saves computation time.

    <br><br>

    <h4>3.8.3 Gram-Schmidt Orthogonalization</h4>

    Projections are at the core of the Gram-Schmidt method that allows us to
    constructively transform any basis \((b_1, \cdots, b_n) \) of \(n\)-dimensional vector space \(V\)
    into an orthogonal/orthonormal basis \((u_1, \cdots, u_n)\) of \(V\).

    <br><br>

    The Gram-Schmidt orthogonalization method iteratively constructs an orthogonal basis \((u_1, \cdots, u_n) \)
    from any basis \( (b_1, \cdots, b_n) \) of \(V\) as follows:
    \[
        \,\\
        \begin{align}
            &u_1 := b_1 \\
            &u_2 := b2 - \text{proj}_{u_1} (b2) \\
            &u_3 := b3 - \text{proj}_{u_1} (b3) - \text{proj}_{u_2} (b3)
            \,\\
            \,\\
            &\quad \quad \vdots
            \,\\
            \,\\
            &u_n := b_n - \text{proj}_{u_1} (b_n) - \text{proj}_{u_2} (b_n) - \text{proj}_{u_3} (b_n) - \,\, \cdots \,\,  - \text{proj}_{u_{n-1}} (b_n)
        \end{align}
        \,\\
    \]

    <br>

    <div class="mml-example">
        <strong>Example 3.12 (Gram-Schmidt Orthogonalization)</strong>
        <br>
        Consider a basis \((b_1, b_2)\) of \(\mathbb{R}^2\), where
        \[
            \,\\

            b_1 = 
            \begin{bmatrix}
                2 \\
                0 \\
            \end{bmatrix}, \quad
            b_2 = 
            \begin{bmatrix}
                1 \\
                1 \\
            \end{bmatrix}
            \,\\
        \]
        
        <br>
        Using the Gram-Schmidt method,
        \[
            \,\\
            \begin{align}
                &u_1 := b_1 = \begin{bmatrix} 2 \\ 0 \end{bmatrix}
                \,\\
                \,\\
                &u_2 := b_2 - \mathrm{proj}_{u_1}(b_2)
            \end{align}
            \,\\
        \]

        From Example 3.10 (Projection onto a Line), we can compute the projection as 
        \(\text{proj}_{u_1} (b_2) = \frac{u_1 u_1^\top}{u_1^\top u_1} b_2\).
        Therefore, \(u_2\) is 
        \[
            \,\\
            \begin{align}

            u_1^\top u_1 &= 
            \begin{bmatrix}
                2 & 0
            \end{bmatrix}
            \begin{bmatrix}
                2 
                \\
                0
            \end{bmatrix}
            = 4
            \,\\
            \,\\
            
            u_1 u_1^\top &= 
            \begin{bmatrix}
                2 
                \\
                0
            \end{bmatrix}
            \begin{bmatrix}
                2 & 0
            \end{bmatrix}
            =
            \begin{bmatrix}
                4 & 0 \\
                0 & 0
            \end{bmatrix}
            
            \,\\
            \,\\
            \,\\
            u_2 &= b_2 - \frac{u_1 u_1^{\top}}{u_1^{\top} u_1} b_2 
            \,\\
            \,\\
            
                &= 
                \begin{bmatrix}
                    1 
                    \\
                    1
                \end{bmatrix}
                -
                \frac{1}{4}              
                \begin{bmatrix} 
                    4 & 0 \\
                    0 & 0
                \end{bmatrix}
                \begin{bmatrix}
                    1 
                    \\
                    1
                \end{bmatrix}
                
            \,\\
            \,\\
                
                &= 
                \begin{bmatrix}
                    1
                    \\
                    1
                \end{bmatrix}
                \begin{bmatrix}
                    1 & 0 \\
                    0 & 0
                \end{bmatrix}
                \begin{bmatrix}
                    1
                    \\
                    1
                \end{bmatrix}
                
            \,\\
            \,\\
                &=
                \begin{bmatrix}
                    0
                    \\
                    1
                \end{bmatrix}
            \end{align}            
            \,\\
        \]
        <br>

        The figure below illustrates the process of constructing the orthogonal basis \(u_1, u_2\) from \(b_1, b_2\) using the Gram-Schmidt method.
        <figure>
            <img src="/img/mathematics-for-machine-learning/mathematics-for-machine-learning-15.png" width="80%" onerror=handle_image_error(this)>
            <figcaption>
                Gram-Schmidt Orthogonalization in \(\mathbb{R}^2\)
            </figcaption>
        </figure>

        <br><br>

        In \(\mathbb{R}^3\), let \(b_1, b_2, b_3 \) are as follows
        \[
            \,\\
            b_1 = 
            \begin{bmatrix}
                2 \\
                2 \\
                0 \\
            \end{bmatrix}, \quad
            b_2 = 
            \begin{bmatrix} 
                -1 \\
                -1 \\
                3
            \end{bmatrix}, \quad
            b_3 = 
            \begin{bmatrix}
                0 \\
                2 \\
                0
            \end{bmatrix}
            \,\\
        \]

        <br>

        Applying Gram-Schmidt method, we can also obtain orthogonal basis \(u_1, u_2, u_3\) in \(\mathbb{R}^3\)
        \[
            \,\\
            \begin{align}
                u1 &= b1
                \,\\
                \,\\
                \text{proj}_{u_1} (b_2) &= \frac{u_1 u_1^\top}{u_1^\top u_1} b_2\ = 
                \begin{bmatrix}
                    -1 \\
                    -1 \\
                    0
                \end{bmatrix}
                \,\\
                \,\\
                u_2 &= b_2 - \text{proj}_{u_1} (b_2) = 
                \begin{bmatrix}
                    0 \\
                    0 \\
                    3
                \end{bmatrix}
                \,\\
                \,\\
                \text{proj}_{u_1} (b_3) &= \frac{u_1 u_1^\top}{u_1^\top u_1} b_3\ = 
                \begin{bmatrix}
                    1 \\
                    1 \\
                    0
                \end{bmatrix}
                \,\\
                \,\\
                \text{proj}_{u_2} (b_3) &= \frac{u_2 u_2^\top}{u_2^\top u_2} b_3\ = 
                \begin{bmatrix}
                    0 \\
                    0 \\
                    0
                \end{bmatrix}
                \,\\
                \,\\
                u3 &= b_3 - \text{proj}_{u_1} (b_3) - \text{proj}_{u_2} (b_3) = 
                \begin{bmatrix}
                    -1 \\
                    1 \\
                    0
                \end{bmatrix}
            \end{align}
            \,\\
        \]

        <br>

        The figure below shows the process of constructing the orthogonal basis \(u_1, u_2, u_3\) from \(b_1, b_2, b_3\) in \(\mathbb{R}^3\).
        
        <figure>
            <img src="/img/mathematics-for-machine-learning/mathematics-for-machine-learning-16.png" width="80%" onerror=handle_image_error(this)>
            <figcaption>
                Gram-Schmidt Orthogonalization in \(\mathbb{R}^3\)
            </figcaption>
        </figure>

        <br><br>

        To check whether the constructed orthogonal basis vectors \(u_1, u_2, u_3\) are perpendicular, 
        we can compute \(U^\top U\), where \(U\) is the matrix whose columns are \(u_1, u_2, u_3\).
        \[
            \,\\
            U^\top U = 
            \begin{bmatrix}
                2 & 2 & 0 \\
                0 & 0 & 3 \\
                -1 & 1 & 0 \\
            \end{bmatrix}
            \begin{bmatrix}
                2 & 0 & -1 \\
                2 & 0 & 1 \\
                0 & 3 & 0 \\
            \end{bmatrix}
            =
            \begin{bmatrix}
                8 & 0 & 0 \\
                0 & 9 & 0 \\
                0 & 0 & 2 \\
            \end{bmatrix}
            \,\\
        \]

        <br>

        The matrix derived from \(U^\top U\) shows that all off-diagonal elements are zero.
        This means that all basis vectors are orthogonal to each other, and when performing the inner product with themselves,
        only the inner product produces a non-zero value.
    </div>

    <br><br>

    <h4>3.9 Rotations</h4>
    A rotation is a linear mapping 
    that rotates a plane by an angle \(\theta\) about the
    origin, i.e., the origin is a fixed point. 

    <br><br>

    <h4>3.9.1 Rotations in \(\mathbb{R}^2\)</h4>
    Consider the standard basis \(e_1 = [1, 0]^\top, e_2 = [0, 1]^\top  \) of \(\mathbb{R}^2\).
    We aim to rotate this coordinate system by an angle \(\theta\) as illustrated in the figure below.
    
    <figure>
        <img src="/img/mathematics-for-machine-learning/mathematics-for-machine-learning-17.png" width="50%" onerror=handle_image_error(this)>
        <figcaption>Rotations in \(\mathbb{R}^2\)</figcaption>
    </figure>

    <br><br>

    Note that the rotated
    vectors are still linearly independent and, therefore, are a basis of \(\mathbb{R}^2\).
    This means that the rotation performs a basis change.

    Trigonometry allows us to determine the coordinates of the rotated axes with respect to the standard basis in \(\mathbb{R}^2\).
    We obtain
    \[
        \,\\
            \Phi (e_1) = [ \cos(\theta), \sin(\theta) ]^\top ,\quad
            \Phi (e_2) = [ -\sin(\theta), \cos(\theta) ]^\top
        \,\\
    \]

    In the matrix form, the axes can be represented as \(R(\theta)\)
    \[
        \,\\
        R(\theta) = 
        \begin{bmatrix}
            \cos(\theta) & -\sin(\theta) \\
            \sin(\theta) & \cos(\theta) \\
        \end{bmatrix}
        \,\\
    \]

    You can check whether the rotated basis remains orthogonal by computing the inner product: \(\Phi(e_1)^\top \Phi(e_2) = 0\).

    <br><br>

    <h4>3.9.2 Rotations in \(\mathbb{R}^3 \)</h4>
    In \(\mathbb{R}^3\), there are three rotations about the three standard basis vectors.
    We can obtain a general rotation matrix \(R\) by combining the images of the standard basis.
    Consider the standard basis \(e_1, e_2, e_3\) in \(\mathbb{R}^3\)
    \[
        \,\\
        e_1 = [1, 0, 0]^\top, \quad e_2 = [0, 1, 0]^\top, \quad e_3 = [0, 0, 1]^\top
        \,\\
    \]

    Rotation about the \(e_1\)-axis \(R_1(\theta)\) is
    \[
        \,\\
            R_1(\theta) = 
            \begin{bmatrix}
                1 & 0 & 0 \\
                0 & \cos(\theta) & -\sin(\theta) \\
                0 & \sin(\theta) & \cos(\theta) \\
            \end{bmatrix}
        \,\\
    \]

    Here, the \(e_1\) coordinate is fixed, and the counterclockwise rotation is performed in the \(e_2, e_3\) plane.

    <br><br>

    Rotation about the \(e_2\)-axis \(R_2(\theta)\) is
    \[
        \,\\
        R_2(\theta) = 
        \begin{bmatrix}
            \cos(\theta) & 0 & -\sin(\theta) \\
            0 & 1 & 0 \\
            \sin(\theta) & 0 & \cos(\theta)
        \end{bmatrix}
        \,\\
    \]

    In this matrix, rotation is performed in the \(e_1, e_3\) plane

    <br><br>

    Rotation about the \(e_3\)-axis \(R_3(\theta)\) is
    \[
        \,\\
        R_3(\theta) = 
        \begin{bmatrix}
        \cos(\theta) & -\sin(\theta) & 0 \\
        \sin(\theta) & \cos(\theta) & 0\\
        0 & 0 & 1 \\
        \end{bmatrix}
        \,\\
    \]

    The figure below illustrates rotations in \(\mathbb{R}^3\).

    <figure>
        <img src="/img/mathematics-for-machine-learning/mathematics-for-machine-learning-18.png" width="80%" onerror=handle_image_error(this)>
        <figcaption>
            Rotations in \(\mathbb{R}^3\)
        </figcaption>
    </figure>

    <br><br>

    <h4>3.9.3 Rotations in \(n\) Dimensions</h4>
    The generalization of rotations from 2D and 3D to n-dimensional Euclidean vector spaces 
    can be intuitively described as fixing \(n - 2\) dimensions 
    and restrict the rotation to a two-dimensional plane in the \(n\)-dimensional space.
    As in the three-dimensional case, we can rotate any plane (two-dimensional subspace of \(\mathbb{R}^n\)).

    <br><br>

    <h4>3.9.4 Properties of Rotations</h4>
    Rotations exhibit a number of useful properties, which can be derived by
    considering them as orthogonal matrices.
    <ul>
        <li>
            Rotations preserve distances, i.e., 
            \( \| x - y \| = \| R_{\theta}(x) - R_{\theta}(y) \| \).
        </li>
        <li>
            Rotations preserve angles, i.e., the angle between \(R_{\theta}(x)\) and \(R_{\theta}(y) \)
            equals the angle between \(x\) and \(y\).
        </li>
        <li>
            Rotations in three (or more) dimensions are generally not commutative.
            Therefore, the order in which rotataions are applied is important.
            Only in two dimensions vector rotataions are commutative.
        </li>
    </ul>

</div><br><br>

<h3>4. Matrix Decompositions</h3>
<div class="article">
    In this chapter, we present three aspects of matrices: how
    to summarize matrices, how matrices can be decomposed, and how these
    decompositions can be used for matrix approximations.

    <br><br>

    <h4>4.1 Determinant and Trace</h4>
    A determinant is
    a mathematical object in the analysis and solution of systems of linear
    equations. Determinants are only defined for square matrices \(A\in \mathbb{n \times n}\).
    The determinant of a square matrix \(A\in \mathbb{n \times n}\) is a function that maps \(A\)
    onto a real number.

    <br><br>

    <div class="mml-example">
        <strong>Example 4.1 (Testing for Matrix Invertibility)</strong>
        <br>
        Let us begin with exploring if a square matrix \(A\) is invertible.
        For \(2 \times 2\) matrices, by the definition of the inverse, 
        we know that \(AA^{-1} = I\). Then with the definition 2.3 (Inverse), the inverse of \(A\) is
        \[
            \,\\
            A^{-1} = \frac{1}{a_{11}a_{22}-a_{12}a_{21}} 
            \begin{bmatrix}
            a_{22} & -a_{12} \\
            -a_{21} & a_{11}
            \end{bmatrix}
            \,\\
        \]

        Hence, \(A\) is invertible if and only if 
        \[
            \,\\
            a_{11}a_{22} - a_{12}a_{21} \neq 0
            \,\\
        \]

        This quantity is the determinant of \(A\in \mathbb{R}^{2\times 2}\), i.e.,
        \[
            \,\\
            \det(A) = 
            \begin{vmatrix}
                a_{11} & a_{12} \\
                a_{21} & a_{22}
            \end{vmatrix}
            =
            a_{11}a_{22} - a_{12}a_{21}
            \,\\
        \]

        This example points at the relationship between determinants and the existence of inverse matrices.
    </div>

    <br><br>

    <strong>Theorem 4.1.</strong> For any square matrix \(A\in \mathbb{R}^{n \times n}\) it holds that \(A\) is invertible
    if and only if \(\det(A) \neq 0\).

    <br><br>
    For \(n = 1\),
    \[
        \,\\
        \det(A) = det(a_{11}) = a_{11}
        \,\\
    \]

    For \(n = 2\),
    \[
        \,\\
        \det(A) = 
        \begin{vmatrix}
            a_{11} & a_{12} \\
            a_{21} & a_{22}
        \end{vmatrix}
        =
        a_{11}a_{22} - a_{12}a_{21}
        \,\\
    \]

    For \(n = 3\) (known as Sarrus' rule),
    \[
        \,\\
        \begin{align}
        \det(A) = 
        \begin{vmatrix}
            a_{11} & a_{12} & a_{13} \\
            a_{21} & a_{22} & a_{23} \\
            a_{31} & a_{32} & a_{33} \\
        \end{vmatrix}
        = \color{red}{a_{11}a_{22}a_{33} + a_{12}a_{23}a_{31} + a_{13}a_{21}a_{32}} \\
          \color{blue}{-a_{13}a_{22}a_{31} - a_{11}a_{23}a_{32} - a_{12}a_{21}a_{33}}
        \end{align}
        \,\\
    \]

    Sarrus' rule can be understood intuitively using the augmented matrix.
    \[
        \,\\
        \left[
        \begin{array}{ccc|cc}
            \color{red}{a_{11}} & \color{red}{a_{12}} & \color{red}{a_{13}} & a_{11} & a_{12} \\
            a_{21} & \color{red}{a_{22}} & \color{red}{a_{23}} & \color{red}{a_{21}} & a_{22} \\
            a_{31} & a_{32} & \color{red}{a_{33}} & \color{red}{a_{31}} & \color{red}{a_{32}} \\ 
        \end{array}
        \right]
        \quad
        \left[
        \begin{array}{ccc|cc}
            a_{11} & a_{12} & \color{blue}{a_{13}} & \color{blue}{a_{11}} & \color{blue}{a_{12}} \\
            a_{21} & \color{blue}{a_{22}} & \color{blue}{a_{23}} & \color{blue}{a_{21}} & a_{22} \\
            \color{blue}{a_{31}} & \color{blue}{a_{32}} & \color{blue}{a_{33}} & a_{31} & a_{32} \\ 
        \end{array}
        \right]
        \,\\    
    \]

    <br>

    <div class="mml-example">
        <strong>Example 4.2 (Determinants as Measure of Volume)</strong>
        <br>
        It turns out that the determinant 
        \(\det(A)\) is the signed volume of an n-dimensional parallelepiped
        formed by columns of the matrix \(A\).

        <br><br>

        For \(n=2\), the columns of the matrix form a parallelogram.
        If the angle between vectors is smaller, the area of a parallelogram shrinks, too.
        In particular, if \(b, g\) are linearly dependent so that \(b = \lambda g \) for some \(\lambda \in \mathbb{R} \),
        they no longer form a two-dimensional parallelogram. Therefore, the corresponding area is \(0\).

        <figure>
            <img src="/img/mathematics-for-machine-learning/mathematics-for-machine-learning-19.png" width="45%" onerror=handle_image_error(this)>
            <figcaption>
                Parallelogram for \(n=2\), Parallelepiped for \(n=3\).
            </figcaption>
        </figure>

        <br><br>

        The sign of the determinant indicates the orientation of the spanning vectors.
        If flipping the order \(b, g\) to \(g, b\), the columns of \(A = [b, g]\) becomes \(A = [g, b]\) and 
        the \(A\) has negative area. For example, let \(b\) and \(g\) be standard basis \(b = e_1 = [1, 0]^\top, g = e_2 = [0, 1]^\top\).
        \[
            \,\\
            \begin{align}
                A &=
                \begin{bmatrix}
                    1 & 0 \\
                    0 & 1
                \end{bmatrix}, \quad 
                \det{A} = 1 - 0 = 1
                \,\\
                \,\\
                \tilde{A} &=
                \begin{bmatrix}
                    0 & 1 \\
                    1 & 0
                \end{bmatrix}, \quad
                \det(\tilde{A}) = 0 - 1 = -1
            \end{align}
            \,\\
        \]

        <br>

        In \(\mathbb{R}^3\), the absolute value of the determinant of the \(3 \times 3\) matrix \([r, g, b] \) is the 
        volume of the solid.
        Thus, the determinant acts as a function that measures the
        signed volume formed by column vectors composed in a matrix.
        Let's say that the three linearly independent vectors \(r, g, b\in \mathbb{R}^3\) are given as
        \[
            \,\\
            r = \begin{bmatrix} 1 \\ 0 \\ 0 \end{bmatrix}, \quad 
            g = \begin{bmatrix} 1 \\ 1 \\ 0 \end{bmatrix}, \quad 
            b = \begin{bmatrix} 1\\1\\1 \end{bmatrix}
            \,\\
        \]

        The vectors given can be represented matrix form as
        \[
            \,\\
            A = [r, g, b] = 
            \begin{bmatrix}
                1 & 1 & 1 \\
                0 & 1 & 1 \\
                0 & 0 & 1
            \end{bmatrix}, \quad \det(A) = 1
            \,\\
        \]
    </div>

    <br>

    <strong>Theorem 4.2</strong> (Laplace Expansion)
    Consider a matrix \(A\in\mathbb{R}^{n\times n}\). Then for all \(j=1, \cdots, n\)
    <ol>
        <li>
            Expansion along column \(j\)
            \[
                \,\\
                \det(A) = \sum_{k=1}^n (-1)^{k+j} a_{kj} \det(A_{k, j})
                \,\\
            \]
        </li>
        <li>
            Expansion along row \(j\)
            \[
                \,\\
                \det(A) = \sum_{n=1}^k (-1)^{k+j} a_{jk} \det(A_{j, k})
                \,\\
            \]
        </li>
    </ol>

    Here, \(A_{k, j}\) is the submatrix of \(A\) that we obtain when deleting row \(k\) and column \(j\).
    <br><br>

    <div class="mml-example">
        <strong>Example 4.3 (Laplace Expansion)</strong>
        <br>
        Let \(A\) be 
        \[
            \,\\
            A = 
            \begin{bmatrix}
            2 & 1 & 3 \\
            0 & 2 & 1 \\
            0 & 0 & 2
            \end{bmatrix}
            \,\\
        \]

        To compute the determinant of \(A\), we can use the Laplace Expansion along the first column
        \[
            \,\\
            \begin{align}
            \det(A) &= 
            (-1)^{1+1} \cdot 2
            \begin{pmatrix}
                2 & 1 \\
                0 & 2                
            \end{pmatrix} + 
            (-1)^{2+1} \cdot 0
            \begin{pmatrix}
                1 & 3 \\
                0 & 2                
            \end{pmatrix} +
            (-1)^{3+1} \cdot 0
            \begin{pmatrix}
                1 & 3 \\
                2 & 1                
            \end{pmatrix}
            \,\\
            \,\\
            &= 2 \cdot 4 + 0 + 0 = 8
            \end{align}
            \,\\
        \]

        Additionally, if the matrix is a triangular matrix, 
        computing the determinant of \(A\) is simplified to multiplying all the diagonal elements. 
        In this case, \(2 \times 2 \times 2 = 8\).
        \[
            \,\\
            \det(\tilde{A}) = \prod_{i=1}^n \tilde{A}_{ii}
            \,\\
        \]

        where, \(\tilde{A}\) is a triangular matrix.
    </div>

    <br><br>

    For \(A\in\mathbb{R}^{n\times n}\), the determinant satisfies the following properties
    <ul>
        <li>
            \(\det(AB) = \det(A) \cdot \det(B) \).
        </li>
        <li>
            \(\det(A) = \det(A^\top) \).
        </li>
        <li>
            If \(A\) is invertible (regular), then \(\det(A^{-1}) = \frac{1}{\det(A)} \).
        </li>
        <li>
            Adding a multiple of a column/row to another one does not change \(\det(A)\).
        </li>
        <li>
            Multiplication of a column/row with \(\lambda \in \mathbb{R}\) scales \(\det(A)\) by \(\lambda\). In
            particular, \(\det(\lambda A) = \lambda^n \det(A) \).
        </li>
        <li>
            Swapping two rows/columns changes the sign of \(\det(A)\).
        </li>
    </ul>

    <br>

    <strong>Theorem 4.3.</strong> A square matrix \(A\ in \mathbb{R}^{n \times n}\) has \(\det(A) \neq 0\)
    if and only if \(\text{rk}(A) = n\). In other words, \(A\) is invertible if and only if it is full rank.

    <br><br>

    <strong>Definition 4.4.</strong> The trace of a square matrix \(A\in \mathbb{R}^{n\times n}\) is defined as
    \[
        \,\\
        \text{tr}(A) := \sum_{i=1}^n A_ii
        \,\\
    \]

    The trace is the sum of the diagonal elements of \(A\).
    The trace staisfies the following properties
    <ul>
        <li>
            \(\text{tr}(A) + \text{tr}(B) = \text{tr}(A + B) \), where \(A, B \in \mathbb{R}^{n \times n} \)
        </li>
        <li>
            \(\text{tr}(\alpha A) = \alpha \text{tr}(A) \), where \(\alpha \in \mathbb{R}, A \in \mathbb{R}^{n \times n} \)
        </li>
        <li>
            \(\text{tr} (I_n) = n \)
        </li>
        <li>
            \(\text{tr}(AB) = \text{tr}(BA) \), where \(A \in \mathbb{R}^{n \times k}, B \in \mathbb{R}^{k \times n} \)
        </li>
    </ul>

    The only function that satisfies the above four properties together is trace.

    <br><br>

    The trace is invariant under cyclic permutations, 
    \[
        \,\\
        \text{tr}(AKL) = \text{tr}(KLA)
        \,\\
    \]

    for the matrices \( A \in \mathbb{R}^{a \times k}, K \in \mathbb{R}^{k \times l}, L \in \mathbb{R}^{l \times a} \).

    The traces of similar matrices \(A, B\) are the same due to the cyclic permutations.
    The similar matrix can be defined as \( B = P^{-1} A P \), where \( A, B, P \in \mathbb{R}^{n \times n} \)
    
    \[
        \,\\
            \begin{align}
            \text{tr}(B) &= \text{tr}(P^{-1}AP)
            \,\\
            \,\\
                         &= \text{tr}(APP^{-1})
            \,\\
            \,\\
                         &= \text{tr}(AI)
            \,\\
            \,\\
                         &= \text{tr}(A)
            \end{align}
        \,\\
    \]

    Hence, while matrix representations of linear mappings are basis dependent the trace of a linear mapping \(\Phi\) is independent of the basis.
    
    <br><br>
    Consider a mapping \(\Phi: \mathbb{R}^2 \rightarrow \mathbb{R}^2 \) given by
    \[
        \,\\
        \Phi (x, y) = (2x + y, x + 3y)
        \,\\
    \]
    
    With respect to the standard basis \(\mathcal{B} = \{ [1, 0]^\top, [0, 1]^\top \}\), the mapping \(\Phi\) is represented by
    \[
        \,\\
        \Phi_{\mathcal{B}} = A = 
        \begin{bmatrix}
            2 & 1 \\
            1 & 3
        \end{bmatrix}, \quad \text{tr}(A) = 2+3 = 5
        \,\\
    \]

    Now, choose a new basis \(\mathcal{C} = \{ [1, 1]^\top, [1, 0]^\top \}\). 
    Then,
    \[
        \,\\
        P = 
        \begin{bmatrix}
            1 & 1 \\
            1 & 0
        \end{bmatrix}, 
        \quad P^{-1} = 
        \begin{bmatrix}
            0 & 1 \\
            1 & -1
        \end{bmatrix}
        \,\\
    \]

    Therefore, \(B = P^{-1}AP \) is
    <div class="latex-container">
        \[
            \,\\
            \begin{align}

            B = P^{-1}AP = 
            \begin{bmatrix}
                0 & 1 \\
                1 & -1
            \end{bmatrix}
            \begin{bmatrix}
                2 & 1 \\
                1 & 3
            \end{bmatrix}
            \begin{bmatrix}
                1 & 1 \\
                1 & 0
            \end{bmatrix}
            =
            \begin{bmatrix}
                4 & 1 \\
                -1 & 1
            \end{bmatrix}, \quad
            &\text{tr}(B) = 4 + 1 = 5
            \,\\
            \,\\

            B = APP^{-1} = AI = A = 
            \begin{bmatrix}
                2 & 1 \\
                1 & 3
            \end{bmatrix}, \quad
            &\text{tr}(A) = 2 + 3 = 5

            \end{align}
            \,\\
        \]
    </div>
    Although the matrices \(A\) and \(B\) are different, 
    they are similar, and by the cyclic permutation property of the trace, we have 
    \[
        \,\\
        \text{tr}(A) = \text{tr}(B) = 5
        \,\\
    \]

    <br><br>

    <h4>4.2 Eigenvalues and Eigenvectors</h4>
    We can interpret linear mappings and their associated transformation matrices by
    performing an "eigen" analysis.

    <br><br>

    <strong>Definition 4.6.</strong> Let \(A \in \mathbb{R}^{n \times n} \) be a square matrix.
    Then \(\lambda \mathbb{R}\) is an eigenvalue of \(A\) and \(x\in \mathbb{R}^n \setminus \{0\}\) is the corresponding
    eigenvector of \(A\) if
    \[
        \,\\
        Ax = \lambda x
        \,\\
    \]

    We call this equation the eigenvalue equation.

    <br><br>

    <strong><i>Remark.</i></strong> The following statements are equivalent:
    <ul>
        <li>
            There exists an \(x\in \mathbb{R}^n \setminus \{0\} \) with \(Ax = \lambda x \)
            or, equivalently, \( (A - \lambda I_n)x = 0 \) can be s olved non-trivially, i.e., \(x \neq 0\).
        </li>
        <li>
            \(\text{rk}(A - \lambda I_n) < n \).
        </li>
        <li>
            \(\det(A - \lambda I_n) = 0 \).
        </li>
    </ul>

    <br><br>

    <strong>Definition 4.7 (Colinearity and Codirection).</strong> 
    Two vectors that point in the same direction are called codirected. Two vectors are collinear if they
    point in the same or the opposite direction.

    <br><br>

    <strong><i>Remark</i></strong> (Non-uniqueness of eigenvectors).
    If \(x\) is an eigenvector of \(A\) associated with eigenvalue \(\lambda\),
    then for any \(c\in \mathbb{R} \setminus \{0\}\) it holds that
    \(cx\) is an eigenvector of \(A\)
    \[
        \,\\
        A(cx) = cAx = c\lambda x = \lambda(cx)
        \,\\
    \]

    <br><br>

    <strong>Definition 4.10 (Eigenspace and Eigenspectrun)</strong>.
    For \(A\ in \mathbb{R}^{n \times n}\), the set of all eigenvectors of \(A\) associated with an eigenvalue \(\lambda\)
    spans a subspace of \(\mathbb{R}^{n}\), which is called the eigenspace of \(A\) with respect to \(\lambda\)
    and is denote by \(E_{\lambda}\).
    The set of all eigenvalues of \(A\) is called the eigenspectrum, or just spectrum of \(A\).

    <br><br>

    If \(\lambda\) is an eigenvalue of \(A\in \mathbb{R}^{n\times n} \),
    then the corresponding eigenspace \(E_{\lambda}\) is the solution of the linear equations
    \((A - \lambda I)x = 0 \).
    Geometrically, the eigenvector corresponding to a nonzero
    eigenvalue points in a direction that is stretched by the linear mapping.

    <br><br>

    The properties of eigenvalues and eigenvectors include:
    <ul>
        <li>
            A matrix \(A\) and its transpose \(A^\top\) possess the same eigenvalues, but not
            necessarily the same eigenvectors.
        </li>
        <li>
            The eigenspace \(E_{\lambda}\) is the null space of \(A - \lambda I \) since
            <div class="latex-container">
                \[
                    Ax - \lambda x
                    \,\, \Longleftrightarrow \,\, Ax - \lambda x = 0 
                    \,\, \Longleftrightarrow \,\, (A - \lambda I) x = 0
                    \,\, \Longleftrightarrow \,\, x \in \ker(A - \lambda I)
                \]
            </div>

            If \(\det(A - \lambda I = 0)\), then the linear transformation \(A - \lambda I\) is singular.
            In a geometric sense, a zero determinant means that the transformation flattens the space into a lower-dimensional subspace.
        </li>
    </ul>

</div><br><br>

<!-- <h3>Vector Calculus</h3>
<div class="article">

</div><br><br> -->

<!-- <h3>Probability and Distributions</h3>
<div class="article">

</div><br><br> -->

<!-- <h3>Continuous Optimization</h3>
<div class="article">

</div><br><br> -->
