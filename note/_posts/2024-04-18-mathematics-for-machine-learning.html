---
title:  "Mathematics for Machine Learning"
layout: post
done: true
emoji: /emoji/books.png
---

<style>
    .mml-example {
        background-color: rgb(245, 245, 245);
        padding: 1em;
        overflow: auto;
    }
</style>

<br>

<!-- <ul>
    <img src="/img/mathematics-for-machine-learning-1.jpg" width="20%">
    <br>
    <li>
        <a href="https://mml-book.github.io/">https://mml-book.github.io/</a>
    </li>
    <li>
        <a href="https://mml-book.github.io/book/mml-book.pdf">https://mml-book.github.io/book/mml-book.pdf</a>
    </li>
</ul> -->


<div id="toc"></div>

<ul>
    <img src="/img/mathematics-for-machine-learning/mathematics-for-machine-learning-1.jpg" width="20%" style="box-shadow: 0 4px 8px rgba(0, 0, 0, 0.2);">
    <br>
    <li>
        <a href="https://mml-book.github.io/">Mathematics for Machine Learning</a>, Marc Peter Disenroth, A. Aldo Faisal, Cheng Soon Ong
    </li>
</ul>

<br>

<h3>1. Introduction</h3>
<div class="article">
    We believe that the mathematical foundations of machine learning 
    are important in order to understand fundamental principles upon
    which more complicated machine learning systems are built. 
    Understanding these principles can facilitate creating new machine learning solutions,
    understanding and debugging existing approaches, and learning about the
    inherent assumptions and limitations of the methodologies we are working with.

    <br><br>

    A model is typically used to describe a process for generating data, similar 
    to the dataset at hand. Therefore, good models can also be thought
    of as simplified versions of the real (unknown) data-generating process,
    capturing aspects that are relevant for modeling the data and extracting
    hidden patterns from it. 

    <br><br>

    Let us summarize the main concepts of machine learning that we cover in this book
    <ul>
        <li>
            We represent data as vectors
        </li>
        <li>
            We choose an appropriate model, either using the probabilistic or optimization view
        </li>
        <li>
            We learn from available data by using numerical optimization methods
            with the aim that the model performs well on data not used for training.
        </li>
    </ul>
</div><br><br>

<h3>2. Linear Algebra</h3>
<div class="article">

    Linear algebra is the study of vectors and certain rules to manipulate vectors.
    The vectors many of us know from school are
    called "geometric vectors", which are usually denoted by a small arrow
    above the letter, e.g., \(\overrightarrow{x}\) and \(\overrightarrow{y}\).
    Interpreting vectors as geometric vectors 
    enables us to use our intuitions about direction and magnitude to
    reason about mathematical operations.

    <br><br>

    Here are some examples of such vector objects:
    <ol>
        <li>
            <strong>Geometric vectors</strong>.
            Two geometric vectors \(\overrightarrow{\boldsymbol{x}}\), \(\overrightarrow{\boldsymbol{y}}\)
            can be added, such that \(\overrightarrow{\boldsymbol{x}} + \overrightarrow{\boldsymbol{y}} = \overrightarrow{\boldsymbol{z}}\)
            is another geometric vector.
            Furthermore, multiplication by a scalar \(\lambda \overrightarrow{\boldsymbol{x}}, \lambda \in \mathbf{R} \),
            is also a geometric vector.

            Interpreting vectors as geometric vectors 
            enables us to use our intuitions about direction and magnitude to
            reason about mathematical operations.
        </li>
        <li>
            <strong>Polynomials are also vectors</strong>. 
            Two polynomials can be added together,  which results in another polynomial; and they can
            be multiplied by a scalar \(\lambda \in \mathbb{R}\) and the result is a polynomial as well.
            Note that polynomials are very different from geometric vectors. While
            geometric vectors are concrete “drawings”, polynomials are abstract
            concepts. However, they are both vectors in the sense.
        </li>
        <li>
            <strong>Audio signals are vectors</strong>. 
            Audio signals are represented as a series of numbers.
            We can add audio signals together, and their sum is a new
            audio signal. If we scale an audio signal, we also obtain an audio signal.
        </li>
        <li>
            <strong>Elements of \(\mathbb{R}^n\) are vectors</strong> (tuples of \(n\) real numbers).
            \(\mathbb{R}^n \) is more abstract than polynomials, and it is the concept we focus on in this book. For instance,
            
            \[
                \,\\
                \boldsymbol{a} = 
                \begin{bmatrix}
                1 \\
                2 \\
                3
                \end{bmatrix}
                \in \mathbb{R}^3
                \,\\
            \]

            Adding two vectors \(\boldsymbol{a}, \boldsymbol{b} \in \mathbb{R}^n \)
            component-wise results in another vector: \(\boldsymbol{a}+\boldsymbol{b}=\boldsymbol{c} \in \mathbb{R}^n \).
            Moreover, multiplying \(\boldsymbol{a} \in \mathbb{R}^n \) by \(\lambda \in \mathbb{R} \) results in a scaled vector \(\lambda a \in \mathbb{R}^n\).
            Considering vectors as elements of \(\mathbb{R}^n\) has an additional benefit that
            it loosely corresponds to arrays of real numbers on a computer. 
            Many programming languages support array operations, which allow for convenient implementation of algorithms that involve vector operations.
        </li>
    </ol>
    <br>

    Linear algebra focuses on the similarities between these vector concepts.
    We will largely focus on vectors in \(\mathbb{R}^n\) since most algorithms in linear algebra are formulated in \(\mathbb{R}^n\)

    <br><br>

    One major idea in mathematics is the idea of "closure". 
    This is the question: 
    What is the set of all things that can result from my proposed operations? 
    In the case of vectors: 
    What is the set of vectors that can result by
    starting with a small set of vectors, and adding them to each other and
    scaling them? This results in a vector space. The concept of
    a vector space and its properties underlie much of machine learning.

    <br><br>

    Linear algebra plays an important role in machine learning and 
    general mathematics. The concepts introduced in this chapter are further expanded to include the idea of geometry.
    <br><br>
    <figure>
        <img src="/img/mathematics-for-machine-learning/mathematics-for-machine-learning-2.png" width="80%" onerror=handle_image_error(this)>
        <figcaption>
            A mind map of the concepts where Linear Algebra is used in other parts
            
        </figcaption>
    </figure>

    <br><br>

    <h4>2.1 Systems of Linear Equations</h4>
    Systems of linear equations play a central part of linear algebra. Many
    problems can be formulated as systems of linear equations, and linear
    algebra gives us the tools for solving them.

    <br><br>
    <div class="mml-example">
        <strong>Example 2.2</strong>
        <br>
        The system of linear equations
        \[
            \,\\
            \begin{matrix}
            x_1  & + & x_2 & + & x_3  & = & 3 & (1) \\
            x_1  & - & x_2 & + & 2x_3 & = & 2 & (2) \\
            2x_1 &   &     & + & 3x_3 & = & 1 & (3)
            \end{matrix}
            \,\\
        \]
        has no solution: Adding the first two equations yields \(2x_1+3_3 = 5 \),
        which constradicts the third equation \((3)\).
        <br><br>

        Let us have a look at the system of linear equations
        \[
            \,\\
            \begin{matrix}
            x_1 & + & x_2 & + & x_3  & = & 3 & (1) \\
            x_1 & - & x_2 & + & 2x_3 & = & 2 & (2) \\
                &   & x_2 & + & x_3  & = & 2 & (3)
            \end{matrix}
            \,\\
        \]
        From the first and third equation, it follows that \(x_1 = 1\).
        From \((1)+(2)\), we get \(2x_1 + 3x_3 = 5 \), i.e., \(x_3 = 1\).
        From \((3)\), we then get that \(x_2 = 1\).
        Therefore \((1,1,1)\) is the only possible and unique solution.

        <br><br>

        As a third example, we consider
        \[
            \,\\
            \begin{matrix}
            x_1  & + & x_2 & + & x_3  & = & 3 & (1) \\ 
            x_1  & - & x_2 & + & 2x_3 & = & 2 & (2) \\
            2x_1 &   &     & + & 3x_3 & = & 5 & (3)
            \end{matrix}
            \,\\
        \]
        Since \((1)+(2)=(3)\), we can obmit the third equation.
        From \((1)\) and \((2)\), we get \(2_x1 = 5-3x_3 \) and \(2x_2 = 1 + x_3\).
        We can define \(x_3 = a \in \mathbb{R}\) as a free variable, 
        such that any triplet is a solution of the system of linear equations, i.e., we can obtain a
        solution set that contains infinitely many solutions.
        \[
            \,\\
            \left( \frac{5}{2}-\frac{3}{2}a, \quad \frac{1}{2}+\frac{1}{2}a, \quad a \right)
            \,\\
        \]
    </div>
    <br><br>

    In general, for a real-valued system of linear equations we obtain either no, exactly one, or infinitely many solutions. 

    <br><br>

    <strong><i>Remark</i></strong> (Geometric Interpretation of Systems of Linear Equations).
    In a system of linear equations with two variables \(x_1, x_2\), each linear equation
    define a line on the \(x_1, x_2\text{-plane}\).
    Since a solution to a system of linear
    equations must satisfy all equations simultaneously, the solution set is the
    intersection of these lines.
    This intersection set can be a line (if the linear
    equations describe the same line), a point, or empty (when the lines are
    parallel).
    \[
        \,\\
        \begin{align}
        4x_1 + 4x_2 &= 5 \\
        2x_1 - 4x_2 &= 1
        \end{align}
        \,\\
    \]

    For the above system, we can get geometric intuition of the solution by plotting both equations as lines in the \(x_1, x_2\)-plane. 
    The point where the two lines intersect represents the unique solution to the system.

    <br><br>
    <figure>
        <img src="/img/mathematics-for-machine-learning/mathematics-for-machine-learning-3.png" width="50%" onerror=handle_image_error(this)>
        <figcaption>
            The solution space of a system of two linear equations with two variables
            <br>
            can be geometrically interpreted as the intersection of two lines
        </figcaption>
    </figure>

    <br><br>

    <h4>2.2 Matrices</h4>
    Matrices play a central role in linear algebra. They can be used to compactly represent systems of linear equations,
    but they also represent linear functions (linear mappings).

    <br><br>

    <strong>Definition 2.1 (Matrix)</strong>. With \(m, n \in \mathbb{N} \) a real-valued \((m,n)\) matrix \(A\) is an
    \(m\cdot n\)-tuple of elements \(a_{ij}, i=1, ..., m, \, j=1, ..., n\), which ordered
    according to a rectangular scheme consisting of \(m\) rows and \(n\) columns:
    \[
        \,\\
        A = 
        \begin{bmatrix}
        a_{11} & a_{12} & \cdots & a_{1n} \\
        a_{21} & a_{22} & \cdots & a_{2n} \\
        \vdots & \vdots &        & \vdots \\
        a_{m1} & a_{m2} & \cdots & a_{mn} 
        \end{bmatrix}
        ,\quad  a_{ij} \in \mathbb{R}
        \,\\
    \]

    \((1, n)\) or \((n, 1)\) matrices are called row/column vectors.
    \(\mathbb{R}^{m \times n}\) is the set of all real-valued \((m,n)\)-matrices. \(A\in \mathbb{R}^{m \times n} \) can be
    equivalently represented as \(a \in \mathbb{R}^{mn} \) by stacking all \(n\) columns of the matrix into a long vector.

    <br><br>

    <h4>2.2.1 Matrix Addition and Multiplication</h4>
    The sum of two matrices \(A \in \mathbb{R}^{m \times n} \), \(B \in \mathbb{R}^{m \times n} \) is defined as the element-wise sum.
    For matrices \(A \in \mathbb{R}^{m \times n} \), \(B \in \mathbb{R}^{n \times k} \), the elements of the product 
    \(C = AB \in \mathbb{R}^{m \times k} \) are computed as 
    \[
        \,\\
        c_{ij} = \sum_{l=1}^{n} a_{il} b_{lj}, \quad i = 1, ..., m, \quad j = 1, ..., k
        \,\\
    \]

    This means, to compute element \(c_{ij}\) we multipy the elements of the \(i\)th row of \(A\)
    with the \(j\)th column of \(B\) and sum them up. We will call this the dot product of the corresponding row and column.

    <br><br>
    
    <strong><i>Remark</i></strong>. Matrices can only be multiplied if their "neighboring" dimensions match.
    For instance, an \(n \times k \)-matrix \(A\) can be multiplied with a \(k \times m \)-matrix \(B\),
    but only from the left side:
    \[
        \,\\
        \underbrace{A}_{n \times k} \; \underbrace{B}_{k \times m} = \underbrace{C}_{n \times m}
        \,\\
    \]

    The product \(BA\) is not defined if \(m \neq n\) since the neighboring dimensions do not match.

    <br><br>

    <strong><i>Remark</i></strong>. Matrix multiplication is not defined as an element-wise operation on matrix elements,
    i.e., \(c_{ij} \neq a_{ij} b_{ij} \), and is called a Hadamard product.
    
    <br><br>
    <div class="mml-example">
        <strong>Example 2.3</strong>
        <br>
        For \(
            A = 
            \begin{bmatrix}
                1 & 2 & 3 \\
                3 & 2 & 1
            \end{bmatrix} \in \mathbb{R}^{2 \times 3}, \quad

            B = 
            \begin{bmatrix}
                0 & 2 \\
                1 & -1 \\
                0 & 1
            \end{bmatrix} \in \mathbb{R}^{3 \times 2}
        \),　we obtain

        \[
            \,\\
            AB = 
            \begin{bmatrix}
            1 & 2 & 3 \\
            3 & 2 & 1
            \end{bmatrix}
            \begin{bmatrix}
                0 & 2 \\
                1 & -1 \\
                0 & 1
            \end{bmatrix}
            =
            \begin{bmatrix}
                2 & 3 \\
                2 & 5 
            \end{bmatrix} \in \mathbb{R}^{2 \times 2}
            \,\\
            \,\\
            BA = 
            \begin{bmatrix}
            0 & 2 \\
            1 & -1 \\
            0 & 1
            \end{bmatrix}
            \begin{bmatrix}
            1 & 2 & 3 \\
            3 & 2 & 1
            \end{bmatrix}
            =
            \begin{bmatrix}
            6 & 4 & 2 \\
            -2 & 0 & 2 \\
            3 & 2 & 1
            \end{bmatrix} \in \mathbb{R}^{3 \times 3}
            \,\\
        \]
    </div>
    <br><br>

    From this example, we can already see that the matrix multiplication is not commutative, i.e., \(AB \neq BA\).

    <br><br>
    
    <strong>Definition 2.2 (Identity Matrix)</strong>. In \(\mathbb{R}^{n\times m} \), we define the identity matrix
    as the \(n \times m \)-matrix containing \(1\) on the diagonal and \(0\) everywhere else.

    \[
        \,\\
        I_n = 
        \begin{bmatrix}
        1      & 0 & \cdots & 0 & \cdots & 0 \\
        0      & 1 & \cdots & 0 & \cdots & 0 \\
        \vdots & \vdots & \ddots & \vdots & \ddots & \vdots \\
        0 & 0 & \cdots & 1 & \cdots & 0 \\
        \vdots & \vdots & \ddots & \vdots & \ddots & \vdots \\
        0 & 0 & \cdots & 0 & \cdots & 1
        \end{bmatrix} \in \mathbb{R}^{n\times n}
        \,\\
    \]

    Now that we defined matrix multiplication, matrix addition and the
    identity matrix, let us have a look at some properties of matrices: 

    <ul>
        <li>
            Associativity:
            \[
                \forall A \in \mathbb{R}^{m\times n}, \quad B \in \mathbb{R}^{n \times p}, \quad C \in \mathbb{R}^{p \times q}: \,\, (AB)C = A(BC)
            \]
        </li>
        <li>
            Distributivity:
            \[
                \begin{align}
                \forall A, B \in \mathbb{R}^{m \times n}, \quad C, D \in \mathbb{R}^{n \times p}:& \,\, (A+B)C = AC+BC \\
                                                                                                 & \,\, A(C+D) = AC+AD
                \end{align}
            \]
        </li>
        <li>
            Multiplication with the identity matrix:
            \[
                \forall A \in \mathbb{R}^{m \times n}: \,\, I_m A = AI_n = A
            \]
        </li>
    </ul>
    <br><br>

    <h4>2.2.2 Inverse and Transpose</h4>
    <strong>Definition 2.3 (Inverse)</strong>. 
    Consider a square matrix \(A \in \mathbb{R}^{n \times n} \).
    Let matrix \(B \in \mathbb{R}^{n \times n} \) have he property that \(AB = I_n = BA\).
    \(B\) is called the inverse of \(A\) and denoted by \(A^{-1}\).

    <br><br>

    Unfortunately, not every matrix \(A\) possesses an inverse \(A^{-1}\). 
    If the inverse does exist, \(A\) is called regular/invertible/nonsingular, otherwise
    singular/noninvertible. When the matrix inverse exists, it is unique.

    <br><br>

    <strong><i>Remark</i></strong> (Existence of the Inverse of a \(2 \times 2\) matrix). Consider a matrix
    \[  
        \,\\
        A := 
        \begin{bmatrix}
        a_{11} & a_{12} \\
        a_{21} & a_{22} 
        \end{bmatrix} \in \mathbb{R}^{2 \times 2}
        \,\\
    \]

    If we multiply \(A\) with
    \[
        \,\\
        B := 
        \begin{bmatrix}
            a_{22} & -a_{12} \\
            -a_{21} & a_{11}
        \end{bmatrix}
        \,\\
    \]

    we obtain
    \[
        \,\\
        \begin{align}
        AB = 
        \begin{bmatrix}
        a_{11}a_{22} - a_{12}a_{21} & 0 \\
        0 & a_{11}a_{22} - a_{12}a_{21}
        \end{bmatrix}
        &= (a_{11}a_{22} - a_{12}a_{21}) \, I  \\
        &= \det (A) \, I
        \end{align}
        \,\\
    \]

    Therefore
    \[
        \,\\
        A^{-1} = \frac{1}{a_{11}a_{22}-a_{12}a_{21}} 
        \begin{bmatrix}
        a_{22} & -a_{12} \\
        -a_{21} & a_{11}
        \end{bmatrix}
        \,\\
    \]

    if and only if \(\det(A) \neq 0\). Furthermore we can generally use the determinant to check whether a matrix is invertible.

    <br><br>
    <div class="mml-example">
        <strong>Example 2.4 (Inverse Matrix)</strong>
        <br>
        The matrices
        \[
            \,\\
            A = 
            \begin{bmatrix}
                1 & 2 & 1 \\
                4 & 4 & 5 \\
                6 & 7 & 7
            \end{bmatrix}
            , \quad
            B = 
            \begin{bmatrix}
                -7 & -7 & 6 \\
                2 & 1 & -1 \\
                4 & 5 & -4 
            \end{bmatrix}
            \,\\
            \,\\
            \begin{align}
            AB &= 
            \begin{bmatrix}
            -7+4+4   & -7+2+5   & 6-2-4 \\
            -28+8+20 & -28+4+25 & 24-4-20 \\
            -42+14+28 & -42+7+35 & 36-7-28
            \end{bmatrix}
            = 
            \begin{bmatrix}
            1 & 0 & 0 \\
            0 & 1 & 0 \\
            0 & 0 & 1
            \end{bmatrix}
            = I

            \,\\
            \,\\
            BA &= 
            \begin{bmatrix}
               -7-28+36 & -14-28+42 & -7-35+42 \\
                2+4-6   & 4+4-7     & 2+5-7 \\
                4+20-24 & 8+20-28   & 4+25-28
            \end{bmatrix}
            = 
            \begin{bmatrix}
            1 & 0 & 0 \\
            0 & 1 & 0 \\
            0 & 0 & 1
            \end{bmatrix}
            = I
            \end{align}
            \,\\
        \]

        are inverse to each other since \(AB = I = BA\)
    </div>

    <br><br>

    <strong>Definition 2.4 (Transpose)</strong>. For \(A \in \mathbb{R}^{m \times n} \)
    the matrix \(B \in \mathbb{R}^{n \times m} \) with \(b_{ij}=a_{ji} \) is called the transpose of \(A\).
    We write \(B = A^{\top} \).

    <br><br>

    In general, \(A^{\top}\) can be obtained by writing the columns of \(A\) as the rows of \(A^{\top}\).
    The following are some important properties of inverses and transposes:
    \[
        \,\\

        \begin{align}
        AA^{-1}           &= I = A^{-1}A \\
        \,\\
        (AB)^{-1}         &= B^{-1}A^{-1} \\
        \,\\
        (A+B)^{-1}        &\neq A^{-1} + B^{-1} \\
        \,\\
        (A^{\top})^{\top} &= A \\
        \,\\
        (A+B)^{\top}      &= A^{\top}+B^{\top} \\
        \,\\
        (AB)^{\top}       &= B^{\top} + A^{\top} 
        \end{align}

        \,\\
    \]

    <strong>Definition 2.5 (Symmetric Matrix)</strong>. A matrix \(A \in \mathbb{R}^{n \times n} \) is symmetric if
    \(A = A^{\top} \).

    <br><br>
    Note that, if \(A\) is invertible, then so is \(A^{\top}\), and \((A^{-1})^{\top} = (A^{\top})^{-1} =: A^{-\top} \).

    <br><br>

    <strong><i>Remark</i></strong> (Sum and Product of Symmetric Matrices). 
    The sum of symmetric matrices \(A, B \in \mathbb{R}^{n\times n} \) is always symmetric.
    However, although their product is always defined, it is generally not symmetric:
    \[
        \,\\
        \begin{bmatrix}
            1 & 0 \\
            0 & 0 
        \end{bmatrix}
        \begin{bmatrix}
            1 & 1 \\
            1 & 1
        \end{bmatrix}
        =
        \begin{bmatrix}
            1 & 1 \\
            0 & 0
        \end{bmatrix}
        \,\\
    \]

    <h4>2.2.3 Multiplication by a Scalar</h4>
    Let us look at what happens to matrices when they are multiplied by a scalar
    \(\lambda \in \mathbb{R}\). Let \(A \in \mathbb{R}^{m\times n} \). Then \(\lambda A = K, \, K_{ij} = \lambda a_{ij} \).
    Pratically, \(\lambda\) scales each element of \(A\). For \(\lambda, \psi \in \mathbb{R} \), the following holds:

    <ul>
        <li>
            Associativity:
            \[
                (\lambda \psi)C = \lambda(\psi C), C \in \mathbb{R}^{m \times n} \\
            \]
        </li>
        <li>
            \(\lambda(BC) = (\lambda B)C = B(\lambda C) = (BC)\lambda, \quad B \in \mathbb{R}^{m \times n},\quad C \in \mathbb{R}^{n \times k}\)
            <br>
            Note that this allows us to move scalar values around.
        </li>
        <li>
            \( (\lambda C)^{\top} = C^{\top}\lambda^{\top} = C^{\top}\lambda = \lambda C^{\top} \) since \(\lambda = \lambda^{\top}, \forall \lambda \in \mathbb{R}  \)
        </li>
        <li>
            Distributivity:
            \[
                (\lambda + \psi)C = \lambda C + \psi C, \quad C \in \mathbb{R}^{m \times n}
                \\
                \lambda (B+C) = \lambda B + \lambda C, \quad B, C \in \mathbb{R}^{m \times n}  
            \]
        </li>
    </ul>
    
    <div class="mml-example">
        <strong>Example 2.5 (Distributivity</strong>
        <br>
        If we define
        \[
            \,\\
            C := 
            \begin{bmatrix}
                1 & 2 \\
                3 & 4
            \end{bmatrix},
            \,\\
        \]
        then for any \(\lambda, \psi \in \mathbb{R}\) we obtain
        \[
            \,\\
            \begin{align}
            (\lambda + \psi)C 
            &= 
            \begin{bmatrix}
            (\lambda + \psi)1 & (\lambda + \psi)2 \\
            (\lambda + \psi)3 & (\lambda + \psi)4 
            \end{bmatrix}
            =
            \begin{bmatrix}
            \lambda + \psi & 2\lambda + 2\psi \\
            3\lambda + 3\psi & 4\lambda + 4\psi
            \end{bmatrix}
            \,\\
            \,\\
            &=
            \begin{bmatrix}
            \lambda & 2\lambda \\
            3\lambda & 4\lambda
            \end{bmatrix}
            +
            \begin{bmatrix}
            \psi & 2\psi \\
            3\psi & 4\psi
            \end{bmatrix}
            = \lambda C + \psi C
            \end{align}
            \,\\
        \]
    </div>

    <br><br>

    <h4>2.3 Solving Systems of Linear Equations</h4>
    <h4>2.3.1 Particular and General Solution</h4>
    Before discussing how to generally solve systems of linear equations, let
    us have a look at an example. Consider the system of equations
    \[
        \,\\
        \begin{bmatrix}
        1 & 0 & 8 & -4 \\
        0 & 1 & 2 & 12
        \end{bmatrix}
        \begin{bmatrix}
        x_1 \\
        x_2 \\
        x_3 \\
        x_4 
        \end{bmatrix}
        =
        \begin{bmatrix}
        42 \\
        8
        \end{bmatrix}
        \,\\
    \]

    The system has two equations and four unknowns. Therefore, in general
    we would expect infinitely many solutions. This system of equations is
    in a particularly easy form, where the first two columns consist of a \(1\)
    and a \(0\).
    A solution to the problem can be found
    found immediately by taking 42 times the first column and 8 times the
    second column so that
    \[
        \,\\
        b = 
        \begin{bmatrix}
        42 \\
        8
        \end{bmatrix}
        = 42
        \begin{bmatrix}
        1 \\
        0
        \end{bmatrix}
        + 8
        \begin{bmatrix}
        0 \\
        1
        \end{bmatrix}
        \,\\
    \]

    Therefore, a solution is \([42, 8, 0, 0]^{\top}\). This solution is called a particular
    solution or special solution. However, this is not the only solution of this system of linear equations.
    To capture all the other solutions, we need
    to be creative in generating 0 in a non-trivial way using the columns of
    the matrix
    \[
        \,\\

        c_3 = 
        \begin{bmatrix}
        8 \\
        2
        \end{bmatrix}
        = 8
        \begin{bmatrix}
        1 \\
        0
        \end{bmatrix}
        + 2
        \begin{bmatrix}
        0 \\
        1 
        \end{bmatrix}
        \,\\
    \]

    so that \(8c_1 + 2c_2 = 1c_3 \), and \( 8c_1 + 2c_2 - 1c_3 + 0c_4 = 0\).
    Therefore \((x_1, x_2, x_3, x_4) = (8, 2, -1, 0)\). In fact, any scaling of this solution
    by \(\lambda_1 \in \mathbb{R}\) produces the 0 vector, i.e.,
    \[
        \,\\
        \begin{bmatrix}
        1 & 0 & 8 & -4 \\
        0 & 1 & 2 & 12
        \end{bmatrix}
        \left(
        \lambda_1
        \begin{bmatrix}
        8 \\
        2 \\
        -1 \\
        0
        \end{bmatrix}
        \right)
        = \lambda_1(8c_1 + 2c_2-c_3) = 0
        \,\\
    \]

    Following the same line of reasoning, we express the fourth column of the
    matrix sing the first two columns and generate another set as 
    \[
        \,\\
        \begin{bmatrix}
        1 & 0 & 8 & -4 \\
        0 & 1 & 2 & 12
        \end{bmatrix}
        \left(
        \lambda_2
        \begin{bmatrix}
        -4 \\
        12 \\
        0 \\
        -1
        \end{bmatrix}
        \right)
        = \lambda_2(-4c_1 + 12c_2-c_4) = 0
        \,\\
    \]

    for any \(\lambda \in \mathbb{R} \). Putting everthing together, we obtain all solutions of the
    equation system, which is called the general solution, as the set
    \[
        \,\\
        \left\{
            x \in \mathbb{R}^4 :
            x = 
            \begin{bmatrix}
                42 \\
                8 \\
                0 \\
                0
            \end{bmatrix}
            + \lambda_1
            \begin{bmatrix}
                8 \\
                2 \\
                -1 \\
                0
            \end{bmatrix}
            + \lambda_2
            \begin{bmatrix}
                -4 \\
                12 \\
                0 \\
                -1
            \end{bmatrix}
            ,\quad \lambda_1, \, \lambda_2 \in \mathbb{R}
        \right\}
        \,\\
    \]

    <strong><i>Remark</i></strong>. The general approach we followed consisted of the following three steps:
    <ol>
        <li>
            Find a particular solution to \(Ax = b\).
        </li>
        <li>
            Find all solutions to \(Ax = 0\).
        </li>
        <li>
            Combine the solutions from steps 1. and 2. to the general solution.
        </li>
    </ol>

    <br>
    The system of linear equations in the preceding example was easy to
    solve because the matrix.
    However, general equation systems are not of this simple form.
    Fortunately, there exists a constructive algorithmic way of transforming
    any system of linear equations into this particularly simple form: Gaussian
    elimination. 
    Key to Gaussian elimination are elementary transformations
    of systems of linear equations, which transform the equation system into
    a simple form. Then, we can apply the three steps to the simple form

    <br><br>
    
    <h4>2.3.2 Elementary Transformations</h4>
    <ul>
        <li>
            Exchange of two equations (rows in the matrix representing the system
            of equations)
        </li>
        <li>
            Multiplication of an equation (row) with a constant \(\lambda \in \mathbb{R} \setminus \{0\} \)
        </li>
        <li>
            Addition of two equations(rows)
        </li>
    </ul>

    <br>

    <div class="mml-example">
        <strong>Example 2.6</strong>
        <br>
        For \(a \in \mathbb{R}\), we seek all solutions of the following system of equations:
        \[
            \,\\
            \begin{array}{*{11}{r}} 
            -2x_1 & + & 4x_2 & - & 2x_3 & - &  x_4 & + & 4x_5 & = & -3 \\
             4x_1 & - & 8x_2 & + & 3x_3 & - & 3x_4 & + &  x_5 & = & 2 \\
              x_1 & - & 2x_2 & + &  x_3 & - &  x_4 & + &  x_5 & = & 0 \\
              x_1 & - & 2x_2 &   &      & - & 3x_4 & + & 4x_5 & = & a 
            \end{array}
            \,\\
        \]

        the system can be converted into compact matrix \(Ax = b\) as an augmented matrix (in the form \([A \mid b]\))
        \[
            \left[
            \begin{array}{rrrrr|r}
                -2 & 4 & -2 & -1 & 4 & -3 \\
                4 & -8 & 3 & -3 & 1 & 2 \\
                 1 & -2 & 1 & -1 & 1 & 0 \\
                 1 & -2 & 0 & -3 & 4 & a
            \end{array}
            \right]
            \begin{array}{l}
                \text{Swap with } R_3 \\
                \\
                \text{Swap with } R_1 \\
                \\
            \end{array}
        \] 

        Swapping \(R_1\) and \(R_3\) leads to
        \[
            \,\\
            \left[
                \begin{array}{rrrrr|r}
                    1 & -2 & 1 & -1 & 1 & 0 \\
                    4 & -8 & 3 & -3 & 1 & 2 \\
                    -2 & 4 & -2 & -1 & 4 & -3 \\
                    1 & -2 & 0 & -3 & 4 & a
                \end{array}
            \right]
            \begin{array}{l}
                \\
                -4R_1 \\
                +2R_1 \\
                -R_1
            \end{array}
            \,\\
        \]
        When we now apply the transformations, we obtain
        \[
            \,\\
            \begin{align}
            &\left[
                \begin{array}{rrrrr|r}
                    1 & -2 & 1 & -1 & 1 & 0 \\
                    0 & 0 & -1 & 1 & -3 & 2 \\
                    0 & 0 & 0 & -3 & 6 & -3 \\
                    0 & 0 & -1 & -2 & 3 & a
                \end{array}
            \right]
            \begin{array}{l}
                \\
                \\
                \\
                -R_2 - R_3
            \end{array}
            \,\\
            \,\\
            &\left[
                \begin{array}{rrrrr|r}
                    1 & -2 & 1 & -1 & 1 & 0 \\
                    0 & 0 & -1 & 1 & -3 & 2 \\
                    0 & 0 & 0 & -3 & 6 & -3 \\
                    0 & 0 & 0 & 0 & 0 & a + 1
                \end{array}
            \right]
            \begin{array}{l}
                \\
                \times \, (-1) \\
                \times \, (-\frac{1}{3})
                \\
            \end{array}
            \,\\
            \,\\
            &\left[
                \begin{array}{rrrrr|r}
                    1 & -2 & 1 & -1 & 1 & 0 \\
                    0 & 0 & 1 & -1 & 3 & -2 \\
                    0 & 0 & 0 & 1 & -2 & 1 \\
                    0 & 0 & 0 & 0 & 0 & a + 1
                \end{array}
            \right]
            \begin{array}{l}
                \\
                \\
                \\
            \end{array}
            \end{align}
            \,\\
        \]

        This matrix is in a convenient form, the row-echelon form.
        Reverting this compact notation back into the explicit notation with
        the variables we seek, we obtain
        \[
            \,\\
            \begin{array}{*{11}{r}} 
               x_1 & - & 2x_2 & + &  x_3 & - &  x_4 & + &  x_5 & = & 0 \\
                   &   &      &   &  x_3 & - &  x_4 & + & 3x_5 & = & -2 \\
                   &   &      &   &      &   &  x_4 & - & 2x_5 & = & 1 \\
                   &   &      &   &      &   &      &   & 0    & = & a + 1
            \end{array}
            \,\\
        \]

        <!-- Only for \(a = =1\) this system can be solved -->

    </div>

</div><br><br>