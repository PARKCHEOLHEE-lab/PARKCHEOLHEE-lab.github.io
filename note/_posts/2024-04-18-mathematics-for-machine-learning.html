---
title:  "Mathematics for Machine Learning"
layout: post
done: true
emoji: /emoji/books.png
---

<style>
    .mml-example {
        background-color: rgb(245, 245, 245);
        padding: 1em;
        overflow: auto;
    }
</style>

<br>

<!-- <ul>
    <img src="/img/mathematics-for-machine-learning-1.jpg" width="20%">
    <br>
    <li>
        <a href="https://mml-book.github.io/">https://mml-book.github.io/</a>
    </li>
    <li>
        <a href="https://mml-book.github.io/book/mml-book.pdf">https://mml-book.github.io/book/mml-book.pdf</a>
    </li>
</ul> -->


<div id="toc"></div>

<ul>
    <img src="/img/mathematics-for-machine-learning/mathematics-for-machine-learning-1.jpg" width="20%" style="box-shadow: 0 4px 8px rgba(0, 0, 0, 0.2);">
    <br>
    <li>
        <a href="https://mml-book.github.io/">Mathematics for Machine Learning</a>, Marc Peter Disenroth, A. Aldo Faisal, Cheng Soon Ong
    </li>
</ul>

<br>

<h3>1. Introduction</h3>
<div class="article">
    We believe that the mathematical foundations of machine learning 
    are important in order to understand fundamental principles upon
    which more complicated machine learning systems are built. 
    Understanding these principles can facilitate creating new machine learning solutions,
    understanding and debugging existing approaches, and learning about the
    inherent assumptions and limitations of the methodologies we are working with.

    <br><br>

    A model is typically used to describe a process for generating data, similar 
    to the dataset at hand. Therefore, good models can also be thought
    of as simplified versions of the real (unknown) data-generating process,
    capturing aspects that are relevant for modeling the data and extracting
    hidden patterns from it. 

    <br><br>

    Let us summarize the main concepts of machine learning that we cover in this book
    <ul>
        <li>
            We represent data as vectors
        </li>
        <li>
            We choose an appropriate model, either using the probabilistic or optimization view
        </li>
        <li>
            We learn from available data by using numerical optimization methods
            with the aim that the model performs well on data not used for training.
        </li>
    </ul>
</div><br><br>

<h3>2. Linear Algebra</h3>
<div class="article">

    Linear algebra is the study of vectors and certain rules to manipulate vectors.
    The vectors many of us know from school are
    called "geometric vectors", which are usually denoted by a small arrow
    above the letter, e.g., \(\overrightarrow{x}\) and \(\overrightarrow{y}\).
    Interpreting vectors as geometric vectors 
    enables us to use our intuitions about direction and magnitude to
    reason about mathematical operations.

    <br><br>

    Here are some examples of such vector objects:
    <ol>
        <li>
            <strong>Geometric vectors</strong>.
            Two geometric vectors \(\overrightarrow{\boldsymbol{x}}\), \(\overrightarrow{\boldsymbol{y}}\)
            can be added, such that \(\overrightarrow{\boldsymbol{x}} + \overrightarrow{\boldsymbol{y}} = \overrightarrow{\boldsymbol{z}}\)
            is another geometric vector.
            Furthermore, multiplication by a scalar \(\lambda \overrightarrow{\boldsymbol{x}}, \lambda \in \mathbf{R} \),
            is also a geometric vector.

            Interpreting vectors as geometric vectors 
            enables us to use our intuitions about direction and magnitude to
            reason about mathematical operations.
        </li>
        <li>
            <strong>Polynomials are also vectors</strong>. 
            Two polynomials can be added together,  which results in another polynomial; and they can
            be multiplied by a scalar \(\lambda \in \mathbb{R}\) and the result is a polynomial as well.
            Note that polynomials are very different from geometric vectors. While
            geometric vectors are concrete “drawings”, polynomials are abstract
            concepts. However, they are both vectors in the sense.
        </li>
        <li>
            <strong>Audio signals are vectors</strong>. 
            Audio signals are represented as a series of numbers.
            We can add audio signals together, and their sum is a new
            audio signal. If we scale an audio signal, we also obtain an audio signal.
        </li>
        <li>
            <strong>Elements of \(\mathbb{R}^n\) are vectors</strong> (tuples of \(n\) real numbers).
            \(\mathbb{R}^n \) is more abstract than polynomials, and it is the concept we focus on in this book. For instance,
            
            \[
                \,\\
                \boldsymbol{a} = 
                \begin{bmatrix}
                1 \\
                2 \\
                3
                \end{bmatrix}
                \in \mathbb{R}^3
                \,\\
            \]

            Adding two vectors \(\boldsymbol{a}, \boldsymbol{b} \in \mathbb{R}^n \)
            component-wise results in another vector: \(\boldsymbol{a}+\boldsymbol{b}=\boldsymbol{c} \in \mathbb{R}^n \).
            Moreover, multiplying \(\boldsymbol{a} \in \mathbb{R}^n \) by \(\lambda \in \mathbb{R} \) results in a scaled vector \(\lambda a \in \mathbb{R}^n\).
            Considering vectors as elements of \(\mathbb{R}^n\) has an additional benefit that
            it loosely corresponds to arrays of real numbers on a computer. 
            Many programming languages support array operations, which allow for convenient implementation of algorithms that involve vector operations.
        </li>
    </ol>
    <br>

    Linear algebra focuses on the similarities between these vector concepts.
    We will largely focus on vectors in \(\mathbb{R}^n\) since most algorithms in linear algebra are formulated in \(\mathbb{R}^n\)

    <br><br>

    One major idea in mathematics is the idea of "closure". 
    This is the question: 
    What is the set of all things that can result from my proposed operations? 
    In the case of vectors: 
    What is the set of vectors that can result by
    starting with a small set of vectors, and adding them to each other and
    scaling them? This results in a vector space. The concept of
    a vector space and its properties underlie much of machine learning.

    <br><br>

    Linear algebra plays an important role in machine learning and 
    general mathematics. The concepts introduced in this chapter are further expanded to include the idea of geometry.
    <br><br>
    <figure>
        <img src="/img/mathematics-for-machine-learning/mathematics-for-machine-learning-2.png" width="80%" onerror=handle_image_error(this)>
        <figcaption>
            A mind map of the concepts where Linear Algebra is used in other parts
            
        </figcaption>
    </figure>

    <br><br>

    <h4>2.1 Systems of Linear Equations</h4>
    Systems of linear equations play a central part of linear algebra. Many
    problems can be formulated as systems of linear equations, and linear
    algebra gives us the tools for solving them.

    <br><br>
    <div class="mml-example">
        <strong>Example 2.2</strong>
        <br>
        The system of linear equations
        \[
            \,\\
            \begin{matrix}
            x_1  & + & x_2 & + & x_3  & = & 3 & (1) \\
            x_1  & - & x_2 & + & 2x_3 & = & 2 & (2) \\
            2x_1 &   &     & + & 3x_3 & = & 1 & (3)
            \end{matrix}
            \,\\
        \]
        has no solution: Adding the first two equations yields \(2x_1+3_3 = 5 \),
        which constradicts the third equation \((3)\).
        <br><br>

        Let us have a look at the system of linear equations
        \[
            \,\\
            \begin{matrix}
            x_1 & + & x_2 & + & x_3  & = & 3 & (1) \\
            x_1 & - & x_2 & + & 2x_3 & = & 2 & (2) \\
                &   & x_2 & + & x_3  & = & 2 & (3)
            \end{matrix}
            \,\\
        \]
        From the first and third equation, it follows that \(x_1 = 1\).
        From \((1)+(2)\), we get \(2x_1 + 3x_3 = 5 \), i.e., \(x_3 = 1\).
        From \((3)\), we then get that \(x_2 = 1\).
        Therefore \((1,1,1)\) is the only possible and unique solution.

        <br><br>

        As a third example, we consider
        \[
            \,\\
            \begin{matrix}
            x_1  & + & x_2 & + & x_3  & = & 3 & (1) \\ 
            x_1  & - & x_2 & + & 2x_3 & = & 2 & (2) \\
            2x_1 &   &     & + & 3x_3 & = & 5 & (3)
            \end{matrix}
            \,\\
        \]
        Since \((1)+(2)=(3)\), we can obmit the third equation.
        From \((1)\) and \((2)\), we get \(2_x1 = 5-3x_3 \) and \(2x_2 = 1 + x_3\).
        We can define \(x_3 = a \in \mathbb{R}\) as a free variable, 
        such that any triplet is a solution of the system of linear equations, i.e., we can obtain a
        solution set that contains infinitely many solutions.
        \[
            \,\\
            \left( \frac{5}{2}-\frac{3}{2}a, \quad \frac{1}{2}+\frac{1}{2}a, \quad a \right)
            \,\\
        \]
    </div>
    <br><br>

    In general, for a real-valued system of linear equations we obtain either no, exactly one, or infinitely many solutions. 

    <br><br>

    <strong><i>Remark</i></strong> (Geometric Interpretation of Systems of Linear Equations).
    In a system of linear equations with two variables \(x_1, x_2\), each linear equation
    define a line on the \(x_1, x_2\text{-plane}\).
    Since a solution to a system of linear
    equations must satisfy all equations simultaneously, the solution set is the
    intersection of these lines.
    This intersection set can be a line (if the linear
    equations describe the same line), a point, or empty (when the lines are
    parallel).
    \[
        \,\\
        \begin{align}
        4x_1 + 4x_2 &= 5 \\
        2x_1 - 4x_2 &= 1
        \end{align}
        \,\\
    \]

    For the above system, we can get geometric intuition of the solution by plotting both equations as lines in the \(x_1, x_2\)-plane. 
    The point where the two lines intersect represents the unique solution to the system.

    <br><br>
    <figure>
        <img src="/img/mathematics-for-machine-learning/mathematics-for-machine-learning-3.png" width="50%" onerror=handle_image_error(this)>
        <figcaption>
            The solution space of a system of two linear equations with two variables
            <br>
            can be geometrically interpreted as the intersection of two lines
        </figcaption>
    </figure>

    <br><br>

    <h4>2.2 Matrices</h4>
    Matrices play a central role in linear algebra. They can be used to compactly represent systems of linear equations,
    but they also represent linear functions (linear mappings).

    <br><br>

    <strong>Definition 2.1 (Matrix)</strong>. With \(m, n \in \mathbb{N} \) a real-valued \((m,n)\) matrix \(A\) is an
    \(m\cdot n\)-tuple of elements \(a_{ij}, i=1, ..., m, \, j=1, ..., n\), which ordered
    according to a rectangular scheme consisting of \(m\) rows and \(n\) columns:
    \[
        \,\\
        A = 
        \begin{bmatrix}
        a_{11} & a_{12} & \cdots & a_{1n} \\
        a_{21} & a_{22} & \cdots & a_{2n} \\
        \vdots & \vdots &        & \vdots \\
        a_{m1} & a_{m2} & \cdots & a_{mn} 
        \end{bmatrix}
        ,\quad  a_{ij} \in \mathbb{R}
        \,\\
    \]

    \((1, n)\) or \((n, 1)\) matrices are called row/column vectors.
    \(\mathbb{R}^{m \times n}\) is the set of all real-valued \((m,n)\)-matrices. \(A\in \mathbb{R}^{m \times n} \) can be
    equivalently represented as \(a \in \mathbb{R}^{mn} \) by stacking all \(n\) columns of the matrix into a long vector.

    <br><br>

    <h4>2.2.1 Matrix Addition and Multiplication</h4>
    The sum of two matrices \(A \in \mathbb{R}^{m \times n} \), \(B \in \mathbb{R}^{m \times n} \) is defined as the element-wise sum.
    For matrices \(A \in \mathbb{R}^{m \times n} \), \(B \in \mathbb{R}^{n \times k} \), the elements of the product 
    \(C = AB \in \mathbb{R}^{m \times k} \) are computed as 
    \[
        \,\\
        c_{ij} = \sum_{l=1}^{n} a_{il} b_{lj}, \quad i = 1, ..., m, \quad j = 1, ..., k
        \,\\
    \]

    This means, to compute element \(c_{ij}\) we multipy the elements of the \(i\)th row of \(A\)
    with the \(j\)th column of \(B\) and sum them up. We will call this the dot product of the corresponding row and column.

    <br><br>
    
    <strong><i>Remark</i></strong>. Matrices can only be multiplied if their "neighboring" dimensions match.
    For instance, an \(n \times k \)-matrix \(A\) can be multiplied with a \(k \times m \)-matrix \(B\),
    but only from the left side:
    \[
        \,\\
        \underbrace{A}_{n \times k} \; \underbrace{B}_{k \times m} = \underbrace{C}_{n \times m}
        \,\\
    \]

    The product \(BA\) is not defined if \(m \neq n\) since the neighboring dimensions do not match.

    <br><br>

    <strong><i>Remark</i></strong>. Matrix multiplication is not defined as an element-wise operation on matrix elements,
    i.e., \(c_{ij} \neq a_{ij} b_{ij} \), and is called a Hadamard product.
    
    <br><br>
    <div class="mml-example">
        <strong>Example 2.3</strong>
        <br>
        For \(
            A = 
            \begin{bmatrix}
                1 & 2 & 3 \\
                3 & 2 & 1
            \end{bmatrix} \in \mathbb{R}^{2 \times 3}, \quad

            B = 
            \begin{bmatrix}
                0 & 2 \\
                1 & -1 \\
                0 & 1
            \end{bmatrix} \in \mathbb{R}^{3 \times 2}
        \),　we obtain

        \[
            \,\\
            AB = 
            \begin{bmatrix}
            1 & 2 & 3 \\
            3 & 2 & 1
            \end{bmatrix}
            \begin{bmatrix}
                0 & 2 \\
                1 & -1 \\
                0 & 1
            \end{bmatrix}
            =
            \begin{bmatrix}
                2 & 3 \\
                2 & 5 
            \end{bmatrix} \in \mathbb{R}^{2 \times 2}
            \,\\
            \,\\
            BA = 
            \begin{bmatrix}
            0 & 2 \\
            1 & -1 \\
            0 & 1
            \end{bmatrix}
            \begin{bmatrix}
            1 & 2 & 3 \\
            3 & 2 & 1
            \end{bmatrix}
            =
            \begin{bmatrix}
            6 & 4 & 2 \\
            -2 & 0 & 2 \\
            3 & 2 & 1
            \end{bmatrix} \in \mathbb{R}^{3 \times 3}
            \,\\
        \]
    </div>
    <br><br>

    From this example, we can already see that the matrix multiplication is not commutative, i.e., \(AB \neq BA\).

    <br><br>
    
    <strong>Definition 2.2 (Identity Matrix)</strong>. In \(\mathbb{R}^{n\times m} \), we define the identity matrix
    as the \(n \times m \)-matrix containing \(1\) on the diagonal and \(0\) everywhere else.

    \[
        \,\\
        I_n = 
        \begin{bmatrix}
        1      & 0 & \cdots & 0 & \cdots & 0 \\
        0      & 1 & \cdots & 0 & \cdots & 0 \\
        \vdots & \vdots & \ddots & \vdots & \ddots & \vdots \\
        0 & 0 & \cdots & 1 & \cdots & 0 \\
        \vdots & \vdots & \ddots & \vdots & \ddots & \vdots \\
        0 & 0 & \cdots & 0 & \cdots & 1
        \end{bmatrix} \in \mathbb{R}^{n\times n}
        \,\\
    \]

    Now that we defined matrix multiplication, matrix addition and the
    identity matrix, let us have a look at some properties of matrices: 

    <ul>
        <li>
            Associativity:
            \[
                \forall A \in \mathbb{R}^{m\times n}, \quad B \in \mathbb{R}^{n \times p}, \quad C \in \mathbb{R}^{p \times q}: \,\, (AB)C = A(BC)
            \]
        </li>
        <li>
            Distributivity:
            \[
                \begin{align}
                \forall A, B \in \mathbb{R}^{m \times n}, \quad C, D \in \mathbb{R}^{n \times p}:& \,\, (A+B)C = AC+BC \\
                                                                                                 & \,\, A(C+D) = AC+AD
                \end{align}
            \]
        </li>
        <li>
            Multiplication with the identity matrix:
            \[
                \forall A \in \mathbb{R}^{m \times n}: \,\, I_m A = AI_n = A
            \]
        </li>
    </ul>
    <br><br>

    <h4>2.2.2 Inverse and Transpose</h4>
    <strong>Definition 2.3 (Inverse)</strong>. 
    Consider a square matrix \(A \in \mathbb{R}^{n \times n} \).
    Let matrix \(B \in \mathbb{R}^{n \times n} \) have he property that \(AB = I_n = BA\).
    \(B\) is called the inverse of \(A\) and denoted by \(A^{-1}\).

    <br><br>

    Unfortunately, not every matrix \(A\) possesses an inverse \(A^{-1}\). 
    If the inverse does exist, \(A\) is called regular/invertible/nonsingular, otherwise
    singular/noninvertible. When the matrix inverse exists, it is unique.

    <br><br>

    <strong><i>Remark</i></strong> (Existence of the Inverse of a \(2 \times 2\) matrix). Consider a matrix
    \[  
        \,\\
        A := 
        \begin{bmatrix}
        a_{11} & a_{12} \\
        a_{21} & a_{22} 
        \end{bmatrix} \in \mathbb{R}^{2 \times 2}
        \,\\
    \]

    If we multiply \(A\) with
    \[
        \,\\
        B := 
        \begin{bmatrix}
            a_{22} & -a_{12} \\
            -a_{21} & a_{11}
        \end{bmatrix}
        \,\\
    \]

    we obtain
    \[
        \,\\
        \begin{align}
        AB = 
        \begin{bmatrix}
        a_{11}a_{22} - a_{12}a_{21} & 0 \\
        0 & a_{11}a_{22} - a_{12}a_{21}
        \end{bmatrix}
        &= (a_{11}a_{22} - a_{12}a_{21}) \, I  \\
        &= \det (A) \, I
        \end{align}
        \,\\
    \]

    Therefore
    \[
        \,\\
        A^{-1} = \frac{1}{a_{11}a_{22}-a_{12}a_{21}} 
        \begin{bmatrix}
        a_{22} & -a_{12} \\
        -a_{21} & a_{11}
        \end{bmatrix}
        \,\\
    \]

    if and only if \(\det(A) \neq 0\). Furthermore we can generally use the determinant to check whether a matrix is invertible.

    <br><br>
    <div class="mml-example">
        <strong>Example 2.4 (Inverse Matrix)</strong>
        <br>
        The matrices
        \[
            \,\\
            A = 
            \begin{bmatrix}
                1 & 2 & 1 \\
                4 & 4 & 5 \\
                6 & 7 & 7
            \end{bmatrix}
            , \quad
            B = 
            \begin{bmatrix}
                -7 & -7 & 6 \\
                2 & 1 & -1 \\
                4 & 5 & -4 
            \end{bmatrix}
            \,\\
            \,\\
            \begin{align}
            AB &= 
            \begin{bmatrix}
            -7+4+4   & -7+2+5   & 6-2-4 \\
            -28+8+20 & -28+4+25 & 24-4-20 \\
            -42+14+28 & -42+7+35 & 36-7-28
            \end{bmatrix}
            = 
            \begin{bmatrix}
            1 & 0 & 0 \\
            0 & 1 & 0 \\
            0 & 0 & 1
            \end{bmatrix}
            = I

            \,\\
            \,\\
            BA &= 
            \begin{bmatrix}
               -7-28+36 & -14-28+42 & -7-35+42 \\
                2+4-6   & 4+4-7     & 2+5-7 \\
                4+20-24 & 8+20-28   & 4+25-28
            \end{bmatrix}
            = 
            \begin{bmatrix}
            1 & 0 & 0 \\
            0 & 1 & 0 \\
            0 & 0 & 1
            \end{bmatrix}
            = I
            \end{align}
            \,\\
        \]

        are inverse to each other since \(AB = I = BA\)
    </div>

    <br><br>

    <strong>Definition 2.4 (Transpose)</strong>. For \(A \in \mathbb{R}^{m \times n} \)
    the matrix \(B \in \mathbb{R}^{n \times m} \) with \(b_{ij}=a_{ji} \) is called the transpose of \(A\).
    We write \(B = A^{\top} \).

    <br><br>

    In general, \(A^{\top}\) can be obtained by writing the columns of \(A\) as the rows of \(A^{\top}\).
    The following are some important properties of inverses and transposes:
    \[
        \,\\

        \begin{align}
        AA^{-1}           &= I = A^{-1}A \\
        \,\\
        (AB)^{-1}         &= B^{-1}A^{-1} \\
        \,\\
        (A+B)^{-1}        &\neq A^{-1} + B^{-1} \\
        \,\\
        (A^{\top})^{\top} &= A \\
        \,\\
        (A+B)^{\top}      &= A^{\top}+B^{\top} \\
        \,\\
        (AB)^{\top}       &= B^{\top} + A^{\top} 
        \end{align}

        \,\\
    \]

    <strong>Definition 2.5 (Symmetric Matrix)</strong>. A matrix \(A \in \mathbb{R}^{n \times n} \) is symmetric if
    \(A = A^{\top} \).

    <br><br>
    Note that, if \(A\) is invertible, then so is \(A^{\top}\), and \((A^{-1})^{\top} = (A^{\top})^{-1} =: A^{-\top} \).

    <br><br>

    <strong><i>Remark</i></strong> (Sum and Product of Symmetric Matrices). 
    The sum of symmetric matrices \(A, B \in \mathbb{R}^{n\times n} \) is always symmetric.
    However, although their product is always defined, it is generally not symmetric:
    \[
        \,\\
        \begin{bmatrix}
            1 & 0 \\
            0 & 0 
        \end{bmatrix}
        \begin{bmatrix}
            1 & 1 \\
            1 & 1
        \end{bmatrix}
        =
        \begin{bmatrix}
            1 & 1 \\
            0 & 0
        \end{bmatrix}
        \,\\
    \]

    <h4>2.2.3 Multiplication by a Scalar</h4>
    Let us look at what happens to matrices when they are multiplied by a scalar
    \(\lambda \in \mathbb{R}\). Let \(A \in \mathbb{R}^{m\times n} \). Then \(\lambda A = K, \, K_{ij} = \lambda a_{ij} \).
    Pratically, \(\lambda\) scales each element of \(A\). For \(\lambda, \psi \in \mathbb{R} \), the following holds:

    <ul>
        <li>
            Associativity:
            \[
                (\lambda \psi)C = \lambda(\psi C), C \in \mathbb{R}^{m \times n} \\
            \]
        </li>
        <li>
            \(\lambda(BC) = (\lambda B)C = B(\lambda C) = (BC)\lambda, \quad B \in \mathbb{R}^{m \times n},\quad C \in \mathbb{R}^{n \times k}\)
            <br>
            Note that this allows us to move scalar values around.
        </li>
        <li>
            \( (\lambda C)^{\top} = C^{\top}\lambda^{\top} = C^{\top}\lambda = \lambda C^{\top} \) since \(\lambda = \lambda^{\top}, \forall \lambda \in \mathbb{R}  \)
        </li>
        <li>
            Distributivity:
            \[
                (\lambda + \psi)C = \lambda C + \psi C, \quad C \in \mathbb{R}^{m \times n}
                \\
                \lambda (B+C) = \lambda B + \lambda C, \quad B, C \in \mathbb{R}^{m \times n}  
            \]
        </li>
    </ul>

    <br><br>
    
    <div class="mml-example">
        <strong>Example 2.5 (Distributivity)</strong>
        <br>
        If we define
        \[
            \,\\
            C := 
            \begin{bmatrix}
                1 & 2 \\
                3 & 4
            \end{bmatrix},
            \,\\
        \]
        then for any \(\lambda, \psi \in \mathbb{R}\) we obtain
        \[
            \,\\
            \begin{align}
            (\lambda + \psi)C 
            &= 
            \begin{bmatrix}
            (\lambda + \psi)1 & (\lambda + \psi)2 \\
            (\lambda + \psi)3 & (\lambda + \psi)4 
            \end{bmatrix}
            =
            \begin{bmatrix}
            \lambda + \psi & 2\lambda + 2\psi \\
            3\lambda + 3\psi & 4\lambda + 4\psi
            \end{bmatrix}
            \,\\
            \,\\
            &=
            \begin{bmatrix}
            \lambda & 2\lambda \\
            3\lambda & 4\lambda
            \end{bmatrix}
            +
            \begin{bmatrix}
            \psi & 2\psi \\
            3\psi & 4\psi
            \end{bmatrix}
            = \lambda C + \psi C
            \end{align}
            \,\\
        \]
    </div>

    <br><br>

    <h4>2.3 Solving Systems of Linear Equations</h4>
    <h4>2.3.1 Particular and General Solution</h4>
    Before discussing how to generally solve systems of linear equations, let
    us have a look at an example. Consider the system of equations
    \[
        \,\\
        \begin{bmatrix}
        1 & 0 & 8 & -4 \\
        0 & 1 & 2 & 12
        \end{bmatrix}
        \begin{bmatrix}
        x_1 \\
        x_2 \\
        x_3 \\
        x_4 
        \end{bmatrix}
        =
        \begin{bmatrix}
        42 \\
        8
        \end{bmatrix}
        \,\\
    \]

    The system has two equations and four unknowns. Therefore, in general
    we would expect infinitely many solutions. This system of equations is
    in a particularly easy form, where the first two columns consist of a \(1\)
    and a \(0\).
    A solution to the problem can be found
    found immediately by taking 42 times the first column and 8 times the
    second column so that
    \[
        \,\\
        b = 
        \begin{bmatrix}
        42 \\
        8
        \end{bmatrix}
        = 42
        \begin{bmatrix}
        1 \\
        0
        \end{bmatrix}
        + 8
        \begin{bmatrix}
        0 \\
        1
        \end{bmatrix}
        \,\\
    \]

    Therefore, a solution is \([42, 8, 0, 0]^{\top}\). This solution is called a particular
    solution or special solution. However, this is not the only solution of this system of linear equations.
    To capture all the other solutions, we need
    to be creative in generating 0 in a non-trivial way using the columns of
    the matrix
    \[
        \,\\

        c_3 = 
        \begin{bmatrix}
        8 \\
        2
        \end{bmatrix}
        = 8
        \begin{bmatrix}
        1 \\
        0
        \end{bmatrix}
        + 2
        \begin{bmatrix}
        0 \\
        1 
        \end{bmatrix}
        \,\\
    \]

    so that \(8c_1 + 2c_2 = 1c_3 \), and \( 8c_1 + 2c_2 - 1c_3 + 0c_4 = 0\).
    Therefore \((x_1, x_2, x_3, x_4) = (8, 2, -1, 0)\). In fact, any scaling of this solution
    by \(\lambda_1 \in \mathbb{R}\) produces the 0 vector, i.e.,
    \[
        \,\\
        \begin{bmatrix}
        1 & 0 & 8 & -4 \\
        0 & 1 & 2 & 12
        \end{bmatrix}
        \left(
        \lambda_1
        \begin{bmatrix}
        8 \\
        2 \\
        -1 \\
        0
        \end{bmatrix}
        \right)
        = \lambda_1(8c_1 + 2c_2-c_3) = 0
        \,\\
    \]

    Following the same line of reasoning, we express the fourth column of the
    matrix using the first two columns and generate another set as 
    \[
        \,\\
        \begin{bmatrix}
        1 & 0 & 8 & -4 \\
        0 & 1 & 2 & 12
        \end{bmatrix}
        \left(
        \lambda_2
        \begin{bmatrix}
        -4 \\
        12 \\
        0 \\
        -1
        \end{bmatrix}
        \right)
        = \lambda_2(-4c_1 + 12c_2-c_4) = 0
        \,\\
    \]

    for any \(\lambda \in \mathbb{R} \). Putting everthing together, we obtain all solutions of the
    equation system, which is called the general solution, as the set
    \[
        \,\\
        \left\{
            x \in \mathbb{R}^4 :
            x = 
            \begin{bmatrix}
                42 \\
                8 \\
                0 \\
                0
            \end{bmatrix}
            + \lambda_1
            \begin{bmatrix}
                8 \\
                2 \\
                -1 \\
                0
            \end{bmatrix}
            + \lambda_2
            \begin{bmatrix}
                -4 \\
                12 \\
                0 \\
                -1
            \end{bmatrix}
            ,\quad \lambda_1, \, \lambda_2 \in \mathbb{R}
        \right\}
        \,\\
    \]

    <strong><i>Remark</i></strong>. The general approach we followed consisted of the following three steps:
    <ol>
        <li>
            Find a particular solution to \(Ax = b\).
        </li>
        <li>
            Find all solutions to \(Ax = 0\).
        </li>
        <li>
            Combine the solutions from steps 1. and 2. to the general solution.
        </li>
    </ol>

    <br>
    The system of linear equations in the preceding example was easy to
    solve because the matrix.
    However, general equation systems are not of this simple form.
    Fortunately, there exists a constructive algorithmic way of transforming
    any system of linear equations into this particularly simple form: Gaussian
    elimination. 
    Key to Gaussian elimination are elementary transformations
    of systems of linear equations, which transform the equation system into
    a simple form. Then, we can apply the three steps to the simple form

    <br><br>
    
    <h4>2.3.2 Elementary Transformations</h4>
    <ul>
        <li>
            Exchange of two equations (rows in the matrix representing the system
            of equations)
        </li>
        <li>
            Multiplication of an equation (row) with a constant \(\lambda \in \mathbb{R} \setminus \{0\} \)
        </li>
        <li>
            Addition of two equations(rows)
        </li>
    </ul>

    <br>

    <div class="mml-example">
        <strong>Example 2.6</strong>
        <br>
        For \(a \in \mathbb{R}\), we seek all solutions of the following system of equations:
        \[
            \,\\
            \begin{array}{*{11}{r}} 
            -2x_1 & + & 4x_2 & - & 2x_3 & - &  x_4 & + & 4x_5 & = & -3 \\
             4x_1 & - & 8x_2 & + & 3x_3 & - & 3x_4 & + &  x_5 & = & 2 \\
              x_1 & - & 2x_2 & + &  x_3 & - &  x_4 & + &  x_5 & = & 0 \\
              x_1 & - & 2x_2 &   &      & - & 3x_4 & + & 4x_5 & = & a 
            \end{array}
            \,\\
        \]

        the system can be converted into compact matrix \(Ax = b\) as an augmented matrix (in the form \([A \mid b]\))
        \[
            \left[
            \begin{array}{rrrrr|r}
                -2 & 4 & -2 & -1 & 4 & -3 \\
                4 & -8 & 3 & -3 & 1 & 2 \\
                 1 & -2 & 1 & -1 & 1 & 0 \\
                 1 & -2 & 0 & -3 & 4 & a
            \end{array}
            \right]
            \begin{array}{l}
                \text{Swap with } R_3 \\
                \\
                \text{Swap with } R_1 \\
                \\
            \end{array}
        \] 

        Swapping \(R_1\) and \(R_3\) leads to
        \[
            \,\\
            \left[
                \begin{array}{rrrrr|r}
                    1 & -2 & 1 & -1 & 1 & 0 \\
                    4 & -8 & 3 & -3 & 1 & 2 \\
                    -2 & 4 & -2 & -1 & 4 & -3 \\
                    1 & -2 & 0 & -3 & 4 & a
                \end{array}
            \right]
            \begin{array}{l}
                \\
                -4R_1 \\
                +2R_1 \\
                -R_1
            \end{array}
            \,\\
        \]
        When we now apply the transformations, we obtain
        \[
            \,\\
            \begin{align}
            &\left[
                \begin{array}{rrrrr|r}
                    1 & -2 & 1 & -1 & 1 & 0 \\
                    0 & 0 & -1 & 1 & -3 & 2 \\
                    0 & 0 & 0 & -3 & 6 & -3 \\
                    0 & 0 & -1 & -2 & 3 & a
                \end{array}
            \right]
            \begin{array}{l}
                \\
                \\
                \\
                -R_2 - R_3
            \end{array}
            \,\\
            \,\\
            \rightarrow \,\, &\left[
                \begin{array}{rrrrr|r}
                    1 & -2 & 1 & -1 & 1 & 0 \\
                    0 & 0 & -1 & 1 & -3 & 2 \\
                    0 & 0 & 0 & -3 & 6 & -3 \\
                    0 & 0 & 0 & 0 & 0 & a + 1
                \end{array}
            \right]
            \begin{array}{l}
                \\
                \times \, (-1) \\
                \times \, (-\frac{1}{3})
                \\
            \end{array}
            \,\\
            \,\\
            \rightarrow \,\, &\left[
                \begin{array}{rrrrr|r}
                    1 & -2 & 1 & -1 & 1 & 0 \\
                    0 & 0 & 1 & -1 & 3 & -2 \\
                    0 & 0 & 0 & 1 & -2 & 1 \\
                    0 & 0 & 0 & 0 & 0 & a + 1
                \end{array}
            \right]
            \begin{array}{l}
                \\
                \\
                \\
            \end{array}
            \end{align}
            \,\\
        \]

        This matrix is in a convenient form, the row-echelon form.
        Reverting this compact notation back into the explicit notation with
        the variables we seek, we obtain
        \[
            \,\\
            \begin{array}{*{11}{r}} 
               x_1 & - & 2x_2 & + &  x_3 & - &  x_4 & + &  x_5 & = & 0 \\
                   &   &      &   &  x_3 & - &  x_4 & + & 3x_5 & = & -2 \\
                   &   &      &   &      &   &  x_4 & - & 2x_5 & = & 1 \\
                   &   &      &   &      &   &      &   & 0    & = & a + 1
            \end{array}
            \,\\
        \]

        Only for \(a = =1\) this system can be solved. A particular solution is
        \[
            \,\\
            \begin{bmatrix}
            x_1 \\
            x_2 \\
            x_3 \\
            x_4 \\
            x_5 
            \end{bmatrix}
            =
            \begin{bmatrix}
            2 \\
            0 \\
            -1 \\
            1 \\
            0 
            \end{bmatrix}
            \,\\
        \]

    From the row-echelon form, we can get free variables \(x_2, x_5\)
    \[
        \,\\
        \begin{array}{*{11}{r}} 
        x_1 & = & 2x_2 & + & 2x_5 \\
        x_2 & = & x_2  &   & \\
        x_3 & = &      & - & x_5 \\
        x_4 & = &      &   & 2x_5   \\
        x_5 & = &      &   & x_5 
        \end{array}
        \,\\
    \]

    Then, substitute them as \(x_2 = \lambda_1\) and \(x_5 = \lambda_2\)
    \[
        \,\\
        \begin{array}{*{11}{r}} 
        x_1 & = & 2\lambda_1 & + & 2\lambda_2 \\
        x_2 & = &  \lambda_1 &   & \\
        x_3 & = &      & - & \lambda_2 \\
        x_4 & = &      &   & 2\lambda_2   \\
        x_5 & = &      &   & \lambda_2
        \end{array}
        \,\\
    \]

    The general solution, which capture the set of all possible solutions, is
    \[
        \,\\
        \left\{
            x \in \mathbb{R}^5 :
            x = 
            \begin{bmatrix}
                2 \\
                0 \\
                -1 \\
                1 \\
                0
            \end{bmatrix}
            + \lambda_1
            \begin{bmatrix}
                2 \\
                1 \\
                0 \\
                0 \\
                0
            \end{bmatrix}
            + \lambda_2
            \begin{bmatrix}
                2 \\
                0 \\
                -1 \\
                2 \\
                1
            \end{bmatrix}
            ,\quad \lambda_1, \, \lambda_2 \in \mathbb{R}
        \right\}
        \,\\
    \]
    </div>
    
    <br><br>

    <strong><i>Remark</i></strong> (Povots and Staircase Structure).
    The leading coefficient of a row (first nonzero number from the left) is called the pivot and is always
    strictly to the right of the pivot of the row above it.
    Therefore, any equation system in row-echelon form always has a "staircase" structure.

    <br><br>

    <strong>Definition 2.6 (Row-Echelon Form)</strong>. 
    A matrix is in row-echeon form if
    <ul>
        <li>
            All rows that contain only zeros are at the bottom of the matrix;
            all rows that contain at least one nonzero element are on
            top of rows that contain only zeros.
        </li>
        <li>
            Looking at nonzero rows only, the first nonzero number from the left
            (also called the pivot or the leading coefficient) is always strictly to the
            right of the pivot of the row above it.
        </li>
    </ul>

    <br>

    <strong><i>Remark</i></strong> (Basic and Free Variables).
    The variables corresponding to the pivots in the row-echelon form are called basic variables and the other
    variables are free variables.

    <br><br>

    <strong><i>Remark</i></strong> (Obtaining a Particular Solution).
    We express the right-hand side of the equation system using the pivot
    columns, such that, \(b = \sum_{i=1}^P \lambda_i p_i  \), where \(p_i, \, i=1, ..., P\), are the pivot columns.
    
    In the previous example (2.6), we would try to find \(\lambda_1, \lambda_2, \lambda_3 \) so that
    \[
        \,\\
        \lambda_1
        \begin{bmatrix}
            1\\
            0\\
            0\\
            0
        \end{bmatrix}
        + \lambda_2
        \begin{bmatrix}
            1\\
            1\\
            0\\
            0
        \end{bmatrix}
        + \lambda_3
        \begin{bmatrix}
            -1\\
            -1\\
            1\\
            0
        \end{bmatrix}
        = 
        \begin{bmatrix}
            0\\
            -2\\
            1\\
            0
        \end{bmatrix}
        \,\\
    \]

    From here, we find relatively directly that \(\lambda_1 = 2, \lambda_2 = -1, \lambda_3 = 1\).
    When we put everything together, we must not forget the non-pivot columns
    for which we set the coefficients implicitly to 0, i.e., \([\lambda_1, 0, \lambda_2, \lambda_3, 0] \).
    Therefore, we get the particular solution \(x = [2, 0, -1, 1, 0]^{\top} \).

    <br><br>

    <strong><i>Remark</i></strong> (Reduced Row Echelon Form).
    An equation is in reduced row-echelon form if
    <ul>
        <li>
            It is in row-echelon form.
        </li>
        <li>
            Every pivot is 1.
        </li>
        <li>
            The pivot is the only nonzero entry in its column.
        </li>
    </ul>

    <br>

    <strong><i>Gaussian Elimination</i></strong>.
    Gaussian elimination is an algorithm that
    performs elementary transformations to bring a system of linear equations
    into reduced row-echelon form.

    <br><br>
    <div class="mml-example">
        <strong>Example 2.7</strong>
        <br>
        Verify that the following matrix is in reduced row-echelon form:

        \[
            \,\\
            A = 
            \begin{bmatrix}
            1 & 3 & 0 & 0 & 3 \\
            0 & 0 & 1 & 0 & 9 \\
            0 & 0 & 0 & 1 & -4 \\
            \end{bmatrix}
            \,\\
        \]

        The key idea for finding the solutions of \(Ax = 0\) is to look at the non-pivot columns,
        which we will need to express as a (linear) combination of the pivot columns. In other words,
        <br><br>
        \[
            \text{non-pivot column} = \text{some combination of pivot columns}
        \]
        <br>

        Returning to the example, we want to solve \(Ax = 0\).
        Here, we can define \(A\) as a collection of column vectors as \(A = [c_1, c_2, c_3, c_4, c_5]\),
        and \(Ax\) is to find all \(x\) for the equation below:
        \[
            \,\\
            \begin{align}
            x_1 c_1 + x_2 c_2 + x_3 c_3 + x_4 c_4 + x_5 c_5 &= 0
            \,\\
            \,\\
            x_1
            \begin{bmatrix}
                1\\
                0\\
                0
            \end{bmatrix}
            + x_2
            \begin{bmatrix}
                3\\
                0\\
                0
            \end{bmatrix}
            + x_3
            \begin{bmatrix}
                0\\
                1\\
                0
            \end{bmatrix}
            + x_4
            \begin{bmatrix}
                0\\
                0\\
                1
            \end{bmatrix}
            + x_5
            \begin{bmatrix}
                3\\
                9\\
                4
            \end{bmatrix}
            &= 0
            \end{align}
            \,\\
        \]

        In the matrix \(A\), the first non-pivot column \(c_2\) is
        \[
            \,\\
            c_2 = 
            \begin{bmatrix}
                3\\
                0\\
                0
            \end{bmatrix}
                = 3c_1
            \,\\
        \] 

        The above is the first dependency. The second non-pivot column \(c_5\) is the second:
        \[
            \,\\
            c_5 = 
            \begin{bmatrix}
                3\\
                9\\
                -4
            \end{bmatrix}
                = 3c_1 + 9c_3 - 4c_4
            \,\\
        \]

        From the relationship between vectors below, we can get the right expressions behind rightarrow

        \[
            \,\\
            \begin{align}
            \begin{aligned}
            3c_1 - c_2 + 0c_3 + 0c_4 + 0c_5  &= 0 \quad \Rightarrow \quad  3x_1 - x_2 + 0x_3 + 0x_4 + 0x_5 = 0 \\
            3c_1 + 0c_2 + 9c_3 - 4c_4 - c_5 &= 0 \quad \Rightarrow  \quad  3x_1 + 0x_2 + 9x_3 - 4x_4 - x_5 = 0
            \end{aligned}
            \end{align}
            \,\\
        \]

        To summarize, all solutions of \(Ax = 0, x\in \mathbb{R}^5 \) are given by
        \[
            \,\\
            \left\{
                x \in \mathbb{R}^5: x = \lambda_1
                \begin{bmatrix}
                    3\\
                    -1\\
                    0\\
                    0\\
                    0
                \end{bmatrix}
                + \lambda_2
                \begin{bmatrix}
                    3\\
                    0\\
                    9\\
                    -4\\
                    -1
                \end{bmatrix},
                \quad
                \lambda_1, \lambda_2 \in \mathbb{R}
            \right\}
            \,\\
        \]

        where the free variables \(x_2, x_5\) are  \(\lambda_1\) and \(\lambda_2\) respectively.
        
    </div>

    <br><br>

    <h4>2.3.3 The Minus-1 Trick</h4>
    We introduce a practical trick for reading out the solutions \(x\), where \(A \in \mathbb{R}^{k \times n}, x \in \mathbb{R}^n \) .
    To start, we assume that \(A\) is in reduced row-echelon form without any rows that just contain zeros.
    We extend the matrix \(A\) to an \(n \times n\)-matrix \(\tilde{A}\).

    <br><br>

    <div class="mml-example">
        <strong>Example 2.8 (Minus-1 Trick)</strong>
        <br>
        Let us revisit the matrix below, which is already in reduced REF:

        \[
            \,\\
            A = 
            \begin{bmatrix}
            1 & 3 & 0 & 0 & 3 \\
            0 & 0 & 1 & 0 & 9 \\
            0 & 0 & 0 & 1 & -4 \\
            \end{bmatrix}
            \,\\
        \]

        we now augment this matrix to a \(5 \times 5\) matrix by adding rows at the places where the pivots on the diagonal are missing
        and obtain

        \[
            \,\\
            \tilde{A} = 
            \begin{bmatrix}
            1 & 3 & 0 & 0 & 3 \\
            \color{blue}{0} & \color{blue}{-1} & \color{blue}{0} & \color{blue}{0} & \color{blue}{0} \\
            0 & 0 & 1 & 0 & 9 \\
            0 & 0 & 0 & 1 & -4 \\
            \color{blue}{0} & \color{blue}{0} & \color{blue}{0} & \color{blue}{0} & \color{blue}{-1}
            \end{bmatrix}
            \,\\
        \]

        From this form, we can immediately read out the solutions of \(Ax = 0\),
        which is the same as in the Example 2.7
        \[
            \,\\
            \left\{
                x \in \mathbb{R}^5: x = \lambda_1
                \begin{bmatrix}
                    3\\
                    -1\\
                    0\\
                    0\\
                    0
                \end{bmatrix}
                + \lambda_2
                \begin{bmatrix}
                    3\\
                    0\\
                    9\\
                    -4\\
                    -1
                \end{bmatrix},
                \quad
                \lambda_1, \lambda_2 \in \mathbb{R}
            \right\}
            \,\\
        \]
    </div>

    <br><br>
    Calculating the Inverse.
    To compute the inverse \(A^{-1}\) of \(A \in \mathbb{R}^{n\times n} \),
    we need to find a matrix \(X\) that satisfies \(AX = I_n\).
    Then \(X = A^{-1} \). We can write this down as a set of simultaneous linear equations \(AX = I_n\).
    If we bring the augmented equation system into row-echelon form,
    we can read out the inverse on the right-hand side of the equation system.

    <br><br>
    <div class="mml-example">
        <strong>Example 2.9 (Calculating an Inverse Matrix by Gaussian Elimination)</strong>
        <br>
        To determine the inverse of 
        \[
            \,\\
            A = 
            \begin{bmatrix}
                1 & 0 & 2 & 0 \\
                1 & 1 & 0 & 0 \\
                1 & 2 & 0 & 1 \\
                1 & 1 & 1 & 1 
            \end{bmatrix}
            \,\\
        \]

        we write down the augmented matrix
        \[
            \,\\
            \left[
            \begin{array}{cccc|cccc}
                 1 & 0 & 2 & 0 & 1 & 0 & 0 & 0 \\
                 1 & 1 & 0 & 0 & 0 & 1 & 0 & 0 \\
                 1 & 2 & 0 & 1 & 0 & 0 & 1 & 0 \\
                 1 & 1 & 1 & 1 & 0 & 0 & 0 & 1
            \end{array}
            \right]
            \,\\
        \]

        and use Gaussian elimination to bring it into reduced row-echelon form
        \[
            \,\\
            \begin{align}
            &\left[
            \begin{array}{cccc|cccc}
                 1 & 0 & 2 & 0 & 1 & 0 & 0 & 0 \\
                 0 & 1 & -2 & 0 & -1 & 1 & 0 & 0 \\
                 1 & 2 & 0 & 1 & 0 & 0 & 1 & 0 \\
                 1 & 1 & 1 & 1 & 0 & 0 & 0 & 1
            \end{array}
            \right]

            \rightarrow

            \left[
            \begin{array}{cccc|cccc}
                 1 & 0 & 2 & 0 & 1 & 0 & 0 & 0 \\
                 0 & 1 & -2 & 0 & -1 & 1 & 0 & 0 \\
                 0 & 1 & -1 & 0 & 0 & 0 & 1 & -1 \\
                 1 & 1 & 1 & 1 & 0 & 0 & 0 & 1
            \end{array}
            \right]

            \,\\
            \,\\

            \rightarrow
            
            &\left[
            \begin{array}{cccc|cccc}
                 1 & 0 &  2 & 0 &  1 &  0 & 0 & 0 \\
                 0 & 1 & -2 & 0 & -1 &  1 & 0 & 0 \\
                 0 & 0 &  1 & 0 &  1 & -1 & 1 & -1 \\
                 1 & 1 &  1 & 1 &  0 &  0 & 0 & 1
            \end{array}
            \right]

            \rightarrow 
            
            \left[
            \begin{array}{cccc|cccc}
                 1 & 0 &  2 & 0 &  1 &  0 & 0 & 0 \\
                 0 & 1 &  0 & 0 & 1 &  -1 & 2 & -2 \\
                 0 & 0 &  1 & 0 &  1 & -1 & 1 & -1 \\
                 1 & 1 &  1 & 1 &  0 &  0 & 0 & 1
            \end{array}
            \right]

            \,\\
            \,\\

            \rightarrow
            
            &\left[
            \begin{array}{cccc|cccc}
                1 & 0 &  0 & 0 &  -1 &  2 & -2 & 2 \\
                0 & 1 &  0 & 0 & 1 &  -1 & 2 & -2 \\
                0 & 0 &  1 & 0 &  1 & -1 & 1 & -1 \\
                1 & 1 &  1 & 1 &  0 &  0 & 0 & 1
            \end{array}
            \right]

            \rightarrow 
            
            \left[
            \begin{array}{cccc|cccc}
                1 & 0 &  0 & 0 &  -1 &  2 & -2 & 2 \\
                0 & 1 &  0 & 0 & 1 &  -1 & 2 & -2 \\
                0 & 0 &  1 & 0 &  1 & -1 & 1 & -1 \\
                0 & 0 &  0 & 1 &  -1 &  0 & -1 & 2
            \end{array}
            \right]

            \end{align}
            \,\\
        \]

        such that the desired inverse is given as its right-hand side:
        \[
            \,\\
            A^{-1} = 
            \left[
            \begin{array}{cccc}
                 -1 &  2 & -2 & 2 \\
                1 &  -1 & 2 & -2 \\
                 1 & -1 & 1 & -1 \\
                 -1 &  0 & -1 & 2
            \end{array}
            \right]

            \,\\
        \]
    </div>

    <br><br>

    Each elementary row operation \(E\) can be represented as a left multiplication of a matrix \(A\), that is \(EA\).
    Performing multiple elementary row operations \(E_1, E_2, ..., E_k\) is equivalent to 
    \[
        \,\\
        E_k\, E_{k-1} \cdots E_1 \, A = I
        \,\\
    \]
    
    In other words, transforming \(A\) into the identity matrix \(I\) by using elementary transformations is 
    equivalent to multiplying \(A\) by the matrix \(E\), where \(E\) is defined as
    \[
        \,\\
        E = E_k\, E_{k-1} \cdots E_1 \
        \,\\
    \]

    Therefore, 
    \[
        \,\\
        \begin{align}
            EA &= I \\
            E  &= A^{-1}
        \end{align}

        \,\\
    \]

    <br>
    
    <h4>2.3.4 Algorithms for Solving a System off Linear Equations</h4>
    Gaussian elimination plays an important role when 
    1) computing determinants,
    2) checking whether a set of vectors is linearly independent,
    3) computing the inverse of a matrix,
    4) computing the rank of a matrix,
    and 4) determining a basis of a vector space.

    <br><br>

    <h4>2.4 Vector Spaces</h4>
    In the following,
    we will have a closer look at vector spaces, i.e., a structured space in which
    vectors live.

    <br><br>
    In the beginning of this chapter, we informally characterized vectors as objects
    that can be added together and multiplied by a scalar, and they remain objects of the same type
    Now, we are ready to formalize this, and we will start by introducing the concept of a group, 
    which is a set of elements and an operation defined on these elements that keeps some structure of the set intact.

    <br><br>
    <h4>2.4.1 Groups</h4>
    <strong>Definition 2.7 (Group)</strong>. 
    Consider a set \(\mathcal{G}\) and an operation \(\otimes : \mathcal{G} \times \mathcal{G} \rightarrow \mathcal{G} \)
    defined on \(\mathcal{G}\). Then \(G := (\mathcal{G}, \otimes)\) is called a group if the following hold:
    <ol>
        <li>
            Closure of \(\mathcal{G}\) under \(\otimes : \forall x, y \in \mathcal{G}: x \otimes y \in \mathcal{G} \)
        </li>
        <li>
            Associativity: \(\forall x, y, z \in \mathcal{G}: (x \otimes y) \otimes z = x \otimes (y \otimes z) \)
        </li>
        <li>
            Neutral element (Identity element): 
            \(\exists e \in \mathcal{G} \, \forall x \in \mathcal{G}: x \otimes e = x\) and \(e \otimes x = x \)
        </li>
        <li>
            Inverse element: \(\forall x \in \mathcal{G} \, \exists y \in \mathcal{G}: x \otimes y = e \) and \(y \otimes x = e \)
            where \(e\) is the neutral element (identity element).
            We often write \(x^{-1}\) to denote the inverse element of \(x\).
        </li>
    </ol>

    <br>

    <strong><i>Remark</i></strong>. The inverse element is defined with respect to the operation \(\otimes\) and does not 
    necessarily mean \(\frac{1}{x}\). If additionally \(\forall x, y \in \mathcal{G}: x \otimes y = y \otimes x \), then
    \(G = (\mathcal{G}, \otimes)\) is an Abelian group (commutative).

    <br><br>

    <div class="mml-example">
        <strong>Example 2.10 (Groups)</strong>
        <br>
        Let us have a look at some examples of sets with associated operations
        and see whether they are groups:
        <ul>
            <li>
                \( (\mathbb{Z}, +) \) is an Abelian group.
            </li>
            <li>
                \( (\mathbb{N}_0, +) \) is not a group, where \(\mathbb{N}_0 := \mathbb{N} \cup \{0\} \):
                Although \( (\mathbb{N}_0, +) \) possesses a neutral element \((0)\), the inverse elements are missing.
            </li>
            <li>
                \( (\mathbb{Z}, \cdot) \) is not a group: Although \( (\mathbb{Z}, \cdot) \) contains a neutral element
                \((1)\), the inverse elements for any \(z \in \mathbb{Z} \setminus \{\pm 1\} \) are missing.
                In addition, \(0 \in \mathbb{Z} \) cannot have an inverse element since 
                the equation \(0 \cdot x = 1 \) cannot be satisfied for any \(x \in \mathbb{Z}\).
            </li>
            <li>
                \( (\mathbb{R}, \cdot) \) is not a group since \(0\) does not possess an inverse element.
            </li>
            <li>
                \( (\mathbb{R} \setminus \{0\}, \cdot) \) is Abelian.
            </li>
            <li>
                \( (\mathbb{R}^n, +), (\mathbb{Z}^n, + ), n \in \mathbb{N} \) are Abelian if \(+\)
                is defined componentwise, i.e.,
                \[
                    \,\\
                    (x_1, \cdots, x_n) + (y_1, \cdots, y_n) = (x_1 + y_1, \cdots, x_n + y_n)
                    \,\\
                \]
                Then, \( (x_1, \cdots, x_n)^{-1} := (-x_1, \cdots, -x_n) \) is the inverse element and
                \(e = (0, \cdots, 0) \) is the neutral element. 
                For example, if \( x = (1, 2, 3, 4, 5) \), then its inverse is
                \[
                    \,\\
                    x^{-1}  = (-1, -2, -3, -4, -5)
                    \,\\
                \]

                and by adding them componentwise, we obtain the neutral element:
                \[
                    \,\\
                    (1-1, \,\, 2-2, \,\, 3-3, \,\, 4-4, \,\, 5-5) = (0, \,\, 0, \,\, 0, \,\, 0, \,\, 0).
                    \,\\
                \]
            </li>
            <li>
                \( (\mathbb{R}^{m \times n}, +) \), the set of \(m \times n\)-matrices is Abelian (with componentwise addition)
            </li>
            <li>
                \( (\mathbb{R}^{n \times n}, \cdot ) \), the set of \(n \times n\)-matrices with matrix multiplication
                <ul>
                    <li>
                        Closure and associativity follow directly from the definition of matrix multiplication
                        \[
                            \,\\
                                (A \cdot B) \cdot C = A\cdot (B \cdot C)
                            \,\\
                        \]
                    </li>
                    <li>
                        The identity matrix \(I_n\) is the neutral element
                        \[
                            \,\\
                            I_n \cdot A = A \cdot I_n = A
                            \,\\
                        \]
                    </li>
                    <li>
                        For a regular matrix \( A \in \mathbb{R}^{n \times n} \), the inverse element \( A^{-1} \) exists. 
                        If \( A \) is singular, it does not have an inverse element.
                        <br><br>
                        The set of all regular (invertible) matrices forms a group under matrix multiplication,
                        denoted by \( GL(n, \mathbb{R}) = \{ A \in \mathbb{R}^{n \times n} \mid \det(A) \neq 0 \} \),
                        which is called the General Linear Group. 
                        <br><br>
                        Therefore, \( (GL(n, \mathbb{R}), \cdot) \) is a group. 
                        However, since matrix multiplication is not commutative, the group is not Abelian.
                    </li>
                </ul>
            </li>
        </ul>
    </div>

    <br><br>

    <h4>2.4.2 Vector Spaces</h4>
    <strong>Definition 2.9 (Vector Space)</strong>. A real-valued vector space \(V = \mathcal{V, +, \cdot}\) is
    a set \(\mathcal{V} \) with two operations
    \[
        \,\\
        \begin{align}
        +     &: \mathcal{V} \times \mathcal{V} \rightarrow \mathcal{V} \\
        \cdot &: \mathbb{R}  \times \mathcal{V} \rightarrow \mathcal{V}
        \end{align}
        \,\\
    \]

    where \( (\mathcal{V}, +) \) is an abelian group.
    The elements \(x \in V\) are called vectors. The neutral element of \( (\mathcal{V}, +) \)
    is the ero vector \( [0, ..., 0]^{\top} \), and the operation \(+\) is called vector addition.
    The elements \(\lambda \in \mathbb{R} \) are called scalars and the operation \( \cdot \) is a multiplication by scalars. 
    
    <br><br>

    <strong><i>Remark</i></strong>. A vector multiplication \(ab, \,\, a, b \in \mathbb{R}^n\), is not defined.
    By treating vectors as \(n \times 1\) matrices, we can use the matrix multiplication.
    Only the following multiplications for vectors are defined: \(ab^{\top} \in \mathbb{R}^{n\times n}\) (outer product),
    \( a^{\top}b \in \mathbb{R} \) (inner/scalar/dot product). 
    \[
        \,\\
        a = 
        \begin{bmatrix}
            0\\
            1\\
            2
        \end{bmatrix}, \,\,
        b = 
        \begin{bmatrix}
            3\\
            4\\
            5
        \end{bmatrix}

        \,\\
        \,\\

        \begin{align}
        ab^{\top} &= 
        \begin{bmatrix}
            0\\
            1\\
            2
        \end{bmatrix}
        \begin{bmatrix}
            3&4&5
        \end{bmatrix}
        = 
        \begin{bmatrix}
            0 & 0 & 0 \\
            3 & 4 & 5 \\
            6 & 8 & 10 
        \end{bmatrix}

        \,\\
        \,\\

        a^{\top}b &= 
        \begin{bmatrix}
            0&1&2
        \end{bmatrix}
        \begin{bmatrix}
            3\\
            4\\
            5
        \end{bmatrix}
        = 14
        \end{align}
        \,\\
    \]

    <h4>2.4.3 Vector Subspaces</h4>
    Intuitively, they are
    sets contained in the original vector space with the property that when
    we perform vector space operations on elements within this subspace, we
    will never leave it. In this sense, they are "closed".

    <br><br>

    <strong>Definition 2.10 (Vector Subspace)</strong>. 
    Let \(V = (\mathcal{V}, +, \cdot) \) be a vector space and
    \(\mathcal{U} \subseteq \mathcal{V}, \mathcal{U} \neq \emptyset \).
    Then \(U = (\mathcal{U}, +, \cdot) \) is called vector subspace of \(V\) (or linear subspace)
    if \(U\) is a vector space with the vector space operations \(+\) and \(\cdot\)
    restricted to \(\mathcal{U} \times \mathcal{U} \) and \( \mathbb{R} \times \mathcal{U} \).
    We write \( U \subseteq V \) to denote a subspace \(U\) of \(V\).

    <br><br>

    If \(\mathcal{U} \subseteq \mathcal{V} \) and \(V\) is a vector space, then \(U\) naturally inherits 
    the operations of \(V\), that is,
    \[
        \,\\
        \begin{align}
        +     &: \mathcal{U} \times \mathcal{U} \rightarrow \mathcal{U} \\
        \cdot &: \mathbb{R}  \times \mathcal{U} \rightarrow \mathcal{U}
        \end{align}
        \,\\
    \]

    For \(U\) to be a subspace of \(V\), the following conditions must hold:
    <ol>
        <li>
            neutral element (zero vector): 
            \[
                \mathcal{U} \neq \emptyset, \, \boldsymbol{0} \in \mathcal{U}
            \]
        </li>
        <li>
            closure:
            \[
                \begin{align}
                &\forall \lambda \in \mathbb{R} \, \forall x \in \mathcal{U} : \lambda x \in \mathcal{U}
                \,\\
                &\forall x, y \in \mathcal{U} : x + y \in \mathcal{U}
                \end{align}
            \]
        </li>
    </ol>

    <br>
    
    <div class="mml-example">
        <strong>Example 2.12 (Vector Subspaces)</strong>
        <ul>
            <li>
                For every vector space \(V\), the trivial subspaces are \(V\) itself and \( \{\boldsymbol{0}\}\).
            </li>
            <li>
                Only example \(D\) in the figure below is a subspace of \(\mathbb{R}^2 \).
                In \(A\) and \(C\), the closure property is violated; \(B\) does not contain \(\boldsymbol{0}\).
            </li>
            <figure>
                <img src="/img/mathematics-for-machine-learning/mathematics-for-machine-learning-4.png" width="100%" onerror=handle_image_error(this)>
                <figcaption>
                    Only \(D\) is a subspace
                </figcaption>
            </figure>
        </ul>

    </div>

    <br><br>

    <strong><i>Remark</i></strong>. Every subspace \(U \subseteq (\mathbb{R}^n, +, \cdot) \) is the solution
    space of a homogeneous system of linear equations \(Ax = 0\) for \(x \in \mathbb{R}^n\).
    \[
        \,\\
        A(\boldsymbol{0}) = \boldsymbol{0}
        \,\\
        \,\\
        A(x_1) = \boldsymbol{0}, \,\, A(x_2) = \boldsymbol{0} 
        \,\\
        \,\\
        A(x_1 + x_2) = A(x_1) + A(x_2) = \boldsymbol{0} + \boldsymbol{0} = \boldsymbol{0}
        \,\\
        \,\\
        A(\lambda x_1) = \lambda A(X_1) = \lambda \cdot \boldsymbol{0} = \boldsymbol{0}
        \,\\
    \]

    <h4>2.5 Linear Independence</h4>
    The closure property guarantees that we
    end up with another vector in the same vector space. It is possible to find
    a set of vectors with which we can represent every vector in the vector
    space by adding them together and scaling them. 
    This set of vectors is a basis. Before we get concepts of the basis,
    we will need to introduce the concepts of linear combinations and linear independence.
    
    <br><br>

    <strong>Definition 2.11 (Linear Combination)</strong>.
    Consider a vector space \(V\) and a finite number of vectors \(x_1, ..., x_k \in V \).
    Then, every \(v \in V\) of the form
    \[
        \,\\
        v = \lambda_1 x_1 + \cdots + \lambda_k x_k = \sum_{i=1}^k \lambda_i x_i \in V
        \,\\
    \]

    with \(\lambda_1, ..., \lambda_k \in \mathbb{R} \) is a linear combination of the vectors \(x_1, ..., x_k \).

    <br><br>

    The \(\boldsymbol{0} \)-vector can always be written as the linear combination of \(k\) vectors \(x_1, ..., x_k \)
    because \(\boldsymbol{0} = \sum_{i=1}^k 0x_i \) is always true, and is trivial.
        \[
            \,\\
            \boldsymbol{0} = 0x_1 + \cdots + 0x_k
            \,\\
        \]

    In the following, we are interested in non-trivial linear combinations of a set of vectors to represet \(\boldsymbol{0}\),
    i.e., linear combinations of vectors \(x_1, ..., x_k \), where not all coefficients \(\lambda_i\)in are \(0\).

    <br><br>

    <strong>Definition 2.12 (Linear (In)dependence)</strong>.
    Let us consider a vector space \(V\) with \(k\in \mathbb{N}\) and \(x_1, ..., x_k \in V  \).
    If there is a non-trivial linear combination, such that \(\boldsymbol{0} = \sum_{i=1}^k \lambda_i x_i  \)
    with at least one \(\lambda_i \neq 0\), the vectors are linearly dependent.
    \[
        \,\\
        \begin{align}
        x_3 &= 2x_1 - 3x_2 \\
        0   &= 2x_1 - 3x_2 - x_3
        \end{align}
        \,\\
    \]
    
    In contrast,
    \(\boldsymbol{0} = \lambda_1 x_1 + \cdots + \lambda_k x_k\),
    only the trivial solution exists, i.e.,
    \(\lambda_1 = \cdots = \lambda_k = 0\),
    the vectors \(x_1, \dots, x_k\) are said to be linearly independent.
    \[
        \,\\
        x_1 = 
        \begin{bmatrix}
            1\\
            0\\
        \end{bmatrix},
        \,\, 
        x_2 = 
        \begin{bmatrix}
            0\\
            1\\
        \end{bmatrix}
        \,\\
        \,\\
        \lambda_1 
        \begin{bmatrix}
            1\\
            0\\
        \end{bmatrix}
        + \lambda_2 
        \begin{bmatrix}
            0\\
            1\\
        \end{bmatrix} = 
        0 
        \begin{bmatrix}
            1\\
            0\\
        \end{bmatrix}
        + 0 
        \begin{bmatrix}
            0\\
            1\\
        \end{bmatrix} = 
        
        \boldsymbol{0}
        \,\\
    \]

    <strong><i>Remark</i></strong>. 
    The following properties are useful to find out whether vectors are linearly independent:
    <ul>
        <li>
            \(k\) vectors are either linearly dependent or independent. There is no third option.
        </li>
        <li>
            If at least one of the vectors \(x_1, ..., x_k \) is \(\boldsymbol{0}\) then they are linearly dependent.
            The same vectors exist, they are linearly dependent.
        </li>
        <li>
            The vectors \( \{ x_1, ..., x_k : x_i \neq 0, i = 1, ..., k \}, k \ge 2 \), are linearly dependent
            if and only if (at least) one of them is a linear combination of the others.
            In particular, if one vector is a multiple of another vector, i.e., \(x_i = \lambda x_j, \lambda \in \mathbb{R} \ \)
            then the vectors linearly dependent.
        </li>
        <li>
            A practical way of checking whether vectors are linearly independent
            is to use Gaussian elimination: Write all vectors as columns
            of a matrix \(A\) and perform Gaussian elimination until the matrix is in
            row echelon form (the reduced row-echelon form is unnecessary here).
        </li>
        <li>
            The non-pivot columns can be expressed as linear combinations of the pivot columns.
            For example, the row-echelon form
            
            \[
                x_1 = 
                \begin{bmatrix}
                    1\\
                    0\\
                    0
                \end{bmatrix}, \quad
                x_2 = 
                \begin{bmatrix}
                    2\\
                    1\\
                    0
                \end{bmatrix}, \quad
                x_3 = 
                \begin{bmatrix}
                    3\\
                    1\\
                    0
                \end{bmatrix}

                \,\\
                \,\\
                
                A = 
                \begin{bmatrix}
                    1 & 2 & 3 \\
                    0 & 1 & 1 \\
                    0 & 0 & 0
                \end{bmatrix}
            \]

            tells us that the first and the second columns are pivot columns.
            The third column is a non-pivot column because it can be expressed \(x_1 + x_2\). 
        </li>
        <li>
            All column vectors are linearly independent if and only if all columns are pivot columns.
        </li>
    </ul>

    <br>

    <div class="mml-example">
        <strong>Example 2.14</strong>
        <br>
        Consider \(\mathbb{R}^4\) with
        \[
            \,\\
            x_1 = 
            \begin{bmatrix}
                1 \\
                2 \\
                -3 \\
                4 \\
            \end{bmatrix}, \quad
            x_2 = 
            \begin{bmatrix}
                1 \\
                1 \\
                0 \\
                2 \\
            \end{bmatrix}, \quad
            x_3 = 
            \begin{bmatrix}
                -1 \\
                -2 \\
                1 \\
                1 \\
            \end{bmatrix}, \quad
            
            A = 
            \begin{bmatrix}
                1 & 1 & -1 \\
                2 & 1 & -2 \\
                -3 & 0 & 1 \\
                4 & 2 & 1
            \end{bmatrix}
            \,\\
        \]

        To check whether they are linearly independent, perform elementary row operations
        until we identify the pivot columns:
        \[
            \,\\
            \begin{align}
            A &=
            \begin{bmatrix}
                1 & 1 & -1 \\
                2 & 1 & -2 \\
                -3 & 0 & 1 \\
                4 & 2 & 1
            \end{bmatrix}
            \,\\
            \,\\
            & \Rightarrow
            \begin{bmatrix}
                1 & 1 & -1 \\
                0 & 1 & 0 \\
                -3 & 0 & 1 \\
                4 & 2 & 1
            \end{bmatrix}
            \Rightarrow
            \begin{bmatrix}
                1 & 1 & -1 \\
                0 & 1 & 0 \\
                0 & 3 & -2 \\
                0 & 2 & -5
            \end{bmatrix}
            \Rightarrow
            \begin{bmatrix}
                1 & 1 & -1 \\
                0 & 1 & 0 \\
                0 & 0 & 1 \\
                0 & 0 & 5
            \end{bmatrix}
            \,\\
            \,\\
            &\Rightarrow
            \begin{bmatrix}
                1 & 1 & -1 \\
                0 & 1 & 0 \\
                0 & 0 & 1 \\
                0 & 0 & 0
            \end{bmatrix}
            \end{align}
            \,\\
        \]

        Here, every column of the matrix is a pivot column.
        Therefore, there is no non-trivial solution, and we require \(\lambda_1 = 0, \lambda_2 = 0, \lambda_3 = 0 \)
        to solve the equation system. Hence, the vectors \(x_1, x_2, x_3 \) are linearly independent.
    </div>

    <br><br>

    <!-- <strong><i>Remark</i></strong>. Consider a vector space \(V\) with \(k\) linearly independent vectors
    \(b_1, ..., b_k\) and \(m\) linear combinations
    \[
        \,\\
        x_1 = \sum_{i=1}^{k} \lambda_{i1} b_i
        \,\\
        \,\\
        \vdots
        \,\\
        \,\\
        x_m = \sum_{i=1}^{k} \lambda_{im} b_i
        \,\\
    \]

    Defining \(\boldsymbol{B} = [b_1, ..., b_k] \) as the matrix whose columns are the linearly independent
    vectors, we can write
    \[
        \,\\
        x_j = \boldsymbol{B} \boldsymbol{\lambda}_j, 
        \quad \boldsymbol{\lambda}_j = 
        \begin{bmatrix}
            \lambda_{1j} \\
            \vdots \\
            \lambda_{kj}
        \end{bmatrix}
        , \quad
        j = 1, ..., n
        \,\\
    \]

    <br><br> -->

    <strong><i>Remark</i></strong>. In a vector space \(V\), \(m\) linear combinations of \(k\) vectors are
    linearly dependent if \(m > k\).

    <br><br>

    <h4>2.6 Basis and Rank</h4>
    In a vector space \(V\) , we are particularly interested in sets of vectors \(\mathcal{A}\) that
    possess the property that any vector \(v \in V\) can be obtained by a linear
    combination of vectors in \(\mathcal{A}\).

    <br><br>

    <h4>2.6.1 Generating Set and Basis</h4>
    <strong>Definition 2.13 (Generating Set and Span)</strong>.
    Consider a vector space \(V - (\mathcal{V}, +, \cdot) \) and set of vectors
    \(\mathcal{A} = \{ a_1, ..., a_k\} \subseteq \mathcal{V} \).
    If every vector \(v \in \mathcal{V} \) can be expressed as a linear combination of \(a_1, ..., a_k \),
    \(\mathcal{A}\) is called a generating set of \(V\).
    The set of all linear combinations of vectors in \(\mathcal{A} \) is called the span of \(\mathcal{A}\).
    If \(\mathcal{A}\) spans the vector space \(V\), we write
    \[
        \,\\
        \begin{align}
        V &= \text{span}[\mathcal{A}] \\
          &= \text{span}[a_1, ..., a_k]
        \end{align}
        \,\\
    \]

    <strong>Definition 2.14 (Basis)</strong>. 
    Consider a vector space \(V = (\mathcal{V}, +, \cdot) \) and \(\mathcal{A} \subseteq \mathcal{V}\).
    A generating set \(\mathcal{A}\) of \(V\) is called minimal if there exists no smaller set
    \(\tilde{\mathcal{A}} \subsetneq \mathcal{A} \subseteq \mathcal{V}\) that spans \(V\).
    Every linearly independent generating set of \(V\) is minimal and is called a basis of \(V\).
    For example, consider the following two generating sets in \(\mathbb{R}^2\).
    \[
        \,\\
        \mathcal{A}_1 = \{(1, 0), (0, 1)\}, \quad \mathcal{A}_2 = \{(1, 0), (0, 1), (1, 1)\}
        \,\\
    \]

    However, there is an unnecessary vector \( (1, 1) \in mathcal{A}_2\)
    since \( (1, 1) = (1, 0) + (0, 1) \). 
    Therefore, even if we remove \((1, 1)\) from \(\mathcal{A}_2\),
    the remaining vectors can still generate \(\mathbb{R}^2\)
    Hence, \(\mathcal{A}_2\) is a generating set but not a minimal,
    while \(\mathcal{A}_1\) is a minimal generating set, that is a basis of \(\mathbb{R}^2\).

</div><br><br>