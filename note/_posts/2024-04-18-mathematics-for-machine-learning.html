---
title:  "Mathematics for Machine Learning"
layout: post
done: true
emoji: /emoji/books.png
---

<style>
    .mml-example {
        background-color: rgb(245, 245, 245);
        padding: 1em;
        overflow: auto;
    }
</style>

<br>

<!-- <ul>
    <img src="/img/mathematics-for-machine-learning-1.jpg" width="20%">
    <br>
    <li>
        <a href="https://mml-book.github.io/">https://mml-book.github.io/</a>
    </li>
    <li>
        <a href="https://mml-book.github.io/book/mml-book.pdf">https://mml-book.github.io/book/mml-book.pdf</a>
    </li>
</ul> -->


<div id="toc"></div>

<ul>
    <img src="/img/mathematics-for-machine-learning/mathematics-for-machine-learning-1.jpg" width="20%" style="box-shadow: 0 4px 8px rgba(0, 0, 0, 0.2);">
    <br>
    <li>
        <a href="https://mml-book.github.io/">Mathematics for Machine Learning</a>, Marc Peter Disenroth, A. Aldo Faisal, Cheng Soon Ong
    </li>
</ul>

<br>

<h3>1. Introduction</h3>
<div class="article">
    We believe that the mathematical foundations of machine learning 
    are important in order to understand fundamental principles upon
    which more complicated machine learning systems are built. 
    Understanding these principles can facilitate creating new machine learning solutions,
    understanding and debugging existing approaches, and learning about the
    inherent assumptions and limitations of the methodologies we are working with.

    <br><br>

    A model is typically used to describe a process for generating data, similar 
    to the dataset at hand. Therefore, good models can also be thought
    of as simplified versions of the real (unknown) data-generating process,
    capturing aspects that are relevant for modeling the data and extracting
    hidden patterns from it. 

    <br><br>

    Let us summarize the main concepts of machine learning that we cover in this book
    <ul>
        <li>
            We represent data as vectors
        </li>
        <li>
            We choose an appropriate model, either using the probabilistic or optimization view
        </li>
        <li>
            We learn from available data by using numerical optimization methods
            with the aim that the model performs well on data not used for training.
        </li>
    </ul>
</div><br><br>

<h3>2. Linear Algebra</h3>
<div class="article">

    Linear algebra is the study of vectors and certain rules to manipulate vectors.
    The vectors many of us know from school are
    called "geometric vectors", which are usually denoted by a small arrow
    above the letter, e.g., \(\overrightarrow{x}\) and \(\overrightarrow{y}\).
    Interpreting vectors as geometric vectors 
    enables us to use our intuitions about direction and magnitude to
    reason about mathematical operations.

    <br><br>

    Here are some examples of such vector objects:
    <ol>
        <li>
            <strong>Geometric vectors</strong>.
            Two geometric vectors \(\overrightarrow{\boldsymbol{x}}\), \(\overrightarrow{\boldsymbol{y}}\)
            can be added, such that \(\overrightarrow{\boldsymbol{x}} + \overrightarrow{\boldsymbol{y}} = \overrightarrow{\boldsymbol{z}}\)
            is another geometric vector.
            Furthermore, multiplication by a scalar \(\lambda \overrightarrow{\boldsymbol{x}}, \lambda \in \mathbf{R} \),
            is also a geometric vector.

            Interpreting vectors as geometric vectors 
            enables us to use our intuitions about direction and magnitude to
            reason about mathematical operations.
        </li>
        <li>
            <strong>Polynomials are also vectors</strong>. 
            Two polynomials can be added together,  which results in another polynomial; and they can
            be multiplied by a scalar \(\lambda \in \mathbb{R}\) and the result is a polynomial as well.
            Note that polynomials are very different from geometric vectors. While
            geometric vectors are concrete “drawings”, polynomials are abstract
            concepts. However, they are both vectors in the sense.
        </li>
        <li>
            <strong>Audio signals are vectors</strong>. 
            Audio signals are represented as a series of numbers.
            We can add audio signals together, and their sum is a new
            audio signal. If we scale an audio signal, we also obtain an audio signal.
        </li>
        <li>
            <strong>Elements of \(\mathbb{R}^n\) are vectors</strong> (tuples of \(n\) real numbers).
            \(\mathbb{R}^n \) is more abstract than polynomials, and it is the concept we focus on in this book. For instance,
            
            \[
                \,\\
                \boldsymbol{a} = 
                \begin{bmatrix}
                1 \\
                2 \\
                3
                \end{bmatrix}
                \in \mathbb{R}^3
                \,\\
            \]

            Adding two vectors \(\boldsymbol{a}, \boldsymbol{b} \in \mathbb{R}^n \)
            component-wise results in another vector: \(\boldsymbol{a}+\boldsymbol{b}=\boldsymbol{c} \in \mathbb{R}^n \).
            Moreover, multiplying \(\boldsymbol{a} \in \mathbb{R}^n \) by \(\lambda \in \mathbb{R} \) results in a scaled vector \(\lambda a \in \mathbb{R}^n\).
            Considering vectors as elements of \(\mathbb{R}^n\) has an additional benefit that
            it loosely corresponds to arrays of real numbers on a computer. 
            Many programming languages support array operations, which allow for convenient implementation of algorithms that involve vector operations.
        </li>
    </ol>
    <br>

    Linear algebra focuses on the similarities between these vector concepts.
    We will largely focus on vectors in \(\mathbb{R}^n\) since most algorithms in linear algebra are formulated in \(\mathbb{R}^n\)

    <br><br>

    One major idea in mathematics is the idea of "closure". 
    This is the question: 
    What is the set of all things that can result from my proposed operations? 
    In the case of vectors: 
    What is the set of vectors that can result by
    starting with a small set of vectors, and adding them to each other and
    scaling them? This results in a vector space. The concept of
    a vector space and its properties underlie much of machine learning.

    <br><br>

    Linear algebra plays an important role in machine learning and 
    general mathematics. The concepts introduced in this chapter are further expanded to include the idea of geometry.
    <br><br>
    <figure>
        <img src="/img/mathematics-for-machine-learning/mathematics-for-machine-learning-2.png" width="80%" onerror=handle_image_error(this)>
        <figcaption>
            A mind map of the concepts where Linear Algebra is used in other parts
            
        </figcaption>
    </figure>

    <br><br>

    <h4>2.1 Systems of Linear Equations</h4>
    Systems of linear equations play a central part of linear algebra. Many
    problems can be formulated as systems of linear equations, and linear
    algebra gives us the tools for solving them.

    <br><br>
    <div class="mml-example">
        <strong>Example 2.2</strong>
        <br>
        The system of linear equations
        \[
            \,\\
            \begin{matrix}
            x_1  & + & x_2 & + & x_3  & = & 3 & (1) \\
            x_1  & - & x_2 & + & 2x_3 & = & 2 & (2) \\
            2x_1 &   &     & + & 3x_3 & = & 1 & (3)
            \end{matrix}
            \,\\
        \]
        has no solution: Adding the first two equations yields \(2x_1+3_3 = 5 \),
        which constradicts the third equation \((3)\).
        <br><br>

        Let us have a look at the system of linear equations
        \[
            \,\\
            \begin{matrix}
            x_1 & + & x_2 & + & x_3  & = & 3 & (1) \\
            x_1 & - & x_2 & + & 2x_3 & = & 2 & (2) \\
                &   & x_2 & + & x_3  & = & 2 & (3)
            \end{matrix}
            \,\\
        \]
        From the first and third equation, it follows that \(x_1 = 1\).
        From \((1)+(2)\), we get \(2x_1 + 3x_3 = 5 \), i.e., \(x_3 = 1\).
        From \((3)\), we then get that \(x_2 = 1\).
        Therefore \((1,1,1)\) is the only possible and unique solution.

        <br><br>

        As a third example, we consider
        \[
            \,\\
            \begin{matrix}
            x_1  & + & x_2 & + & x_3  & = & 3 & (1) \\ 
            x_1  & - & x_2 & + & 2x_3 & = & 2 & (2) \\
            2x_1 &   &     & + & 3x_3 & = & 5 & (3)
            \end{matrix}
            \,\\
        \]
        Since \((1)+(2)=(3)\), we can obmit the third equation.
        From \((1)\) and \((2)\), we get \(2_x1 = 5-3x_3 \) and \(2x_2 = 1 + x_3\).
        We can define \(x_3 = a \in \mathbb{R}\) as a free variable, 
        such that any triplet is a solution of the system of linear equations, i.e., we can obtain a
        solution set that contains infinitely many solutions.
        \[
            \,\\
            \left( \frac{5}{2}-\frac{3}{2}a, \quad \frac{1}{2}+\frac{1}{2}a, \quad a \right)
            \,\\
        \]
    </div>
    <br><br>

    In general, for a real-valued system of linear equations we obtain either no, exactly one, or infinitely many solutions. 

    <br><br>

    <strong><i>Remark</i></strong> (Geometric Interpretation of Systems of Linear Equations).
    In a system of linear equations with two variables \(x_1, x_2\), each linear equation
    define a line on the \(x_1, x_2\text{-plane}\).
    Since a solution to a system of linear
    equations must satisfy all equations simultaneously, the solution set is the
    intersection of these lines.
    This intersection set can be a line (if the linear
    equations describe the same line), a point, or empty (when the lines are
    parallel).
    \[
        \,\\
        \begin{align}
        4x_1 + 4x_2 &= 5 \\
        2x_1 - 4x_2 &= 1
        \end{align}
        \,\\
    \]

    For the above system, we can get geometric intuition of the solution by plotting both equations as lines in the \(x_1, x_2\)-plane. 
    The point where the two lines intersect represents the unique solution to the system.

    <br><br>
    <figure>
        <img src="/img/mathematics-for-machine-learning/mathematics-for-machine-learning-3.png" width="50%" onerror=handle_image_error(this)>
        <figcaption>
            The solution space of a system of two linear equations with two variables
            <br>
            can be geometrically interpreted as the intersection of two lines
        </figcaption>
    </figure>

    <br><br>

    <h4>2.2 Matrices</h4>
    Matrices play a central role in linear algebra. They can be used to compactly represent systems of linear equations,
    but they also represent linear functions (linear mappings).

    <br><br>

    <strong>Definition 2.1 (Matrix)</strong>. With \(m, n \in \mathbb{N} \) a real-valued \((m,n)\) matrix \(A\) is an
    \(m\cdot n\)-tuple of elements \(a_{ij}, i=1, ..., m, \, j=1, ..., n\), which ordered
    according to a rectangular scheme consisting of \(m\) rows and \(n\) columns:
    \[
        \,\\
        A = 
        \begin{bmatrix}
        a_{11} & a_{12} & \cdots & a_{1n} \\
        a_{21} & a_{22} & \cdots & a_{2n} \\
        \vdots & \vdots &        & \vdots \\
        a_{m1} & a_{m2} & \cdots & a_{mn} 
        \end{bmatrix}
        ,\quad  a_{ij} \in \mathbb{R}
        \,\\
    \]

    \((1, n)\) or \((n, 1)\) matrices are called row/column vectors.
    \(\mathbb{R}^{m \times n}\) is the set of all real-valued \((m,n)\)-matrices. \(A\in \mathbb{R}^{m \times n} \) can be
    equivalently represented as \(a \in \mathbb{R}^{mn} \) by stacking all \(n\) columns of the matrix into a long vector.

    <br><br>

    <h4>2.2.1 Matrix Addition and Multiplication</h4>
    The sum of two matrices \(A \in \mathbb{R}^{m \times n} \), \(B \in \mathbb{R}^{m \times n} \) is defined as the element-wise sum.
    For matrices \(A \in \mathbb{R}^{m \times n} \), \(B \in \mathbb{R}^{n \times k} \), the elements of the product 
    \(C = AB \in \mathbb{R}^{m \times k} \) are computed as 
    \[
        \,\\
        c_{ij} = \sum_{l=1}^{n} a_{il} b_{lj}, \quad i = 1, ..., m, \quad j = 1, ..., k
        \,\\
    \]

    This means, to compute element \(c_{ij}\) we multipy the elements of the \(i\)th row of \(A\)
    with the \(j\)th column of \(B\) and sum them up. We will call this the dot product of the corresponding row and column.

    <br><br>
    
    <strong><i>Remark</i></strong>. Matrices can only be multiplied if their "neighboring" dimensions match.
    For instance, an \(n \times k \)-matrix \(A\) can be multiplied with a \(k \times m \)-matrix \(B\),
    but only from the left side:
    \[
        \,\\
        \underbrace{A}_{n \times k} \; \underbrace{B}_{k \times m} = \underbrace{C}_{n \times m}
        \,\\
    \]

    The product \(BA\) is not defined if \(m \neq n\) since the neighboring dimensions do not match.

    <br><br>

    <strong><i>Remark</i></strong>. Matrix multiplication is not defined as an element-wise operation on matrix elements,
    i.e., \(c_{ij} \neq a_{ij} b_{ij} \), and is called a Hadamard product.
    
    <br><br>
    <div class="mml-example">
        <strong>Example 2.3</strong>
        <br>
        For \(
            A = 
            \begin{bmatrix}
                1 & 2 & 3 \\
                3 & 2 & 1
            \end{bmatrix} \in \mathbb{R}^{2 \times 3}, \quad

            B = 
            \begin{bmatrix}
                0 & 2 \\
                1 & -1 \\
                0 & 1
            \end{bmatrix} \in \mathbb{R}^{3 \times 2}
        \),　we obtain

        \[
            \,\\
            AB = 
            \begin{bmatrix}
            1 & 2 & 3 \\
            3 & 2 & 1
            \end{bmatrix}
            \begin{bmatrix}
                0 & 2 \\
                1 & -1 \\
                0 & 1
            \end{bmatrix}
            =
            \begin{bmatrix}
                2 & 3 \\
                2 & 5 
            \end{bmatrix} \in \mathbb{R}^{2 \times 2}
            \,\\
            \,\\
            BA = 
            \begin{bmatrix}
            0 & 2 \\
            1 & -1 \\
            0 & 1
            \end{bmatrix}
            \begin{bmatrix}
            1 & 2 & 3 \\
            3 & 2 & 1
            \end{bmatrix}
            =
            \begin{bmatrix}
            6 & 4 & 2 \\
            -2 & 0 & 2 \\
            3 & 2 & 1
            \end{bmatrix} \in \mathbb{R}^{3 \times 3}
            \,\\
        \]
    </div>
    <br><br>

    From this example, we can already see that the matrix multiplication is not commutative, i.e., \(AB \neq BA\).

    <br><br>
    
    <strong>Definition 2.2 (Identity Matrix)</strong>. In \(\mathbb{R}^{n\times m} \), we define the identity matrix
    as the \(n \times m \)-matrix containing \(1\) on the diagonal and \(0\) everywhere else.

    \[
        \,\\
        I_n = 
        \begin{bmatrix}
        1      & 0 & \cdots & 0 & \cdots & 0 \\
        0      & 1 & \cdots & 0 & \cdots & 0 \\
        \vdots & \vdots & \ddots & \vdots & \ddots & \vdots \\
        0 & 0 & \cdots & 1 & \cdots & 0 \\
        \vdots & \vdots & \ddots & \vdots & \ddots & \vdots \\
        0 & 0 & \cdots & 0 & \cdots & 1
        \end{bmatrix} \in \mathbb{R}^{n\times n}
        \,\\
    \]

    Now that we defined matrix multiplication, matrix addition and the
    identity matrix, let us have a look at some properties of matrices: 

    <ul>
        <li>
            Associativity:
            \[
                \forall A \in \mathbb{R}^{m\times n}, \quad B \in \mathbb{R}^{n \times p}, \quad C \in \mathbb{R}^{p \times q}: \,\, (AB)C = A(BC)
            \]
        </li>
        <li>
            Distributivity:
            \[
                \begin{align}
                \forall A, B \in \mathbb{R}^{m \times n}, \quad C, D \in \mathbb{R}^{n \times p}:& \,\, (A+B)C = AC+BC \\
                                                                                                 & \,\, A(C+D) = AC+AD
                \end{align}
            \]
        </li>
        <li>
            Multiplication with the identity matrix:
            \[
                \forall A \in \mathbb{R}^{m \times n}: \,\, I_m A = AI_n = A
            \]
        </li>
    </ul>
    <br><br>

    <h4>2.2.2 Inverse and Transpose</h4>
    <strong>Definition 2.3 (Inverse)</strong>. 
    Consider a square matrix \(A \in \mathbb{R}^{n \times n} \).
    Let matrix \(B \in \mathbb{R}^{n \times n} \) have he property that \(AB = I_n = BA\).
    \(B\) is called the inverse of \(A\) and denoted by \(A^{-1}\).

    <br><br>

    Unfortunately, not every matrix \(A\) possesses an inverse \(A^{-1}\). 
    If the inverse does exist, \(A\) is called regular/invertible/nonsingular, otherwise
    singular/noninvertible. When the matrix inverse exists, it is unique.

    <br><br>

    <strong><i>Remark</i></strong> (Existence of the Inverse of a \(2 \times 2\) matrix). Consider a matrix
    \[  
        \,\\
        A := 
        \begin{bmatrix}
        a_{11} & a_{12} \\
        a_{21} & a_{22} 
        \end{bmatrix} \in \mathbb{R}^{2 \times 2}
        \,\\
    \]

    If we multiply \(A\) with
    \[
        \,\\
        B := 
        \begin{bmatrix}
            a_{22} & -a_{12} \\
            -a_{21} & a_{11}
        \end{bmatrix}
        \,\\
    \]

    we obtain
    \[
        \,\\
        \begin{align}
        AB = 
        \begin{bmatrix}
        a_{11}a_{22} - a_{12}a_{21} & 0 \\
        0 & a_{11}a_{22} - a_{12}a_{21}
        \end{bmatrix}
        &= (a_{11}a_{22} - a_{12}a_{21}) \, I  \\
        &= \det (A) \, I
        \end{align}
        \,\\
    \]

    Therefore
    \[
        \,\\
        A^{-1} = \frac{1}{a_{11}a_{22}-a_{12}a_{21}} 
        \begin{bmatrix}
        a_{22} & -a_{12} \\
        -a_{21} & a_{11}
        \end{bmatrix}
        \,\\
    \]

    if and only if \(\det(A) \neq 0\). Furthermore we can generally use the determinant to check whether a matrix is invertible.

    <br><br>
    <div class="mml-example">
        <strong>Example 2.4 (Inverse Matrix)</strong>
        <br>
        The matrices
        \[
            \,\\
            A = 
            \begin{bmatrix}
                1 & 2 & 1 \\
                4 & 4 & 5 \\
                6 & 7 & 7
            \end{bmatrix}
            , \quad
            B = 
            \begin{bmatrix}
                -7 & -7 & 6 \\
                2 & 1 & -1 \\
                4 & 5 & -4 
            \end{bmatrix}
            \,\\
            \,\\
            \begin{align}
            AB &= 
            \begin{bmatrix}
            -7+4+4   & -7+2+5   & 6-2-4 \\
            -28+8+20 & -28+4+25 & 24-4-20 \\
            -42+14+28 & -42+7+35 & 36-7-28
            \end{bmatrix}
            = 
            \begin{bmatrix}
            1 & 0 & 0 \\
            0 & 1 & 0 \\
            0 & 0 & 1
            \end{bmatrix}
            = I

            \,\\
            \,\\
            BA &= 
            \begin{bmatrix}
               -7-28+36 & -14-28+42 & -7-35+42 \\
                2+4-6   & 4+4-7     & 2+5-7 \\
                4+20-24 & 8+20-28   & 4+25-28
            \end{bmatrix}
            = 
            \begin{bmatrix}
            1 & 0 & 0 \\
            0 & 1 & 0 \\
            0 & 0 & 1
            \end{bmatrix}
            = I
            \end{align}
            \,\\
        \]

        are inverse to each other since \(AB = I = BA\)
    </div>

    <br><br>

    <strong>Definition 2.4 (Transpose)</strong>. For \(A \in \mathbb{R}^{m \times n} \)
    the matrix \(B \in \mathbb{R}^{n \times m} \) with \(b_{ij}=a_{ji} \) is called the transpose of \(A\).
    We write \(B = A^{\top} \).
    
</div><br><br>