---
title:  "Attention Is All You Need"
layout: post
emoji: /emoji/brain.png
---

<br>

<ul>
    <li>
        <a href="https://arxiv.org/pdf/1706.03762">Attention Is All You Need</a>
    </li>
    <li>
        <a href="https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html">torch.nn.Transformer</a>
    </li>
    <li>
        <a href="https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html">torch.nn.MultiheadAttention</a>
    </li>
    <br>
    <i>keywords to search: 
        <a href="https://medium.com/humanscape-tech/rnn-recurrent-neural-network-%EC%88%9C%ED%99%98%EC%8B%A0%EA%B2%BD%EB%A7%9D-%EC%9D%84-%EC%9D%B4%ED%95%B4%ED%95%B4%EB%B3%B4%EC%9E%90-1697a5472af2">
            RNN, 
        </a>
        LSTM, 
        Gated RNN
    </i>
    <br><br>
    <ol>
        <li>
            Introduction
        </li>
            <ul>
                <li>
                    Recurrent models typically factor computation along the symbol positions of the input and output sequences.
                    Inherently seqquential nature of Recurrent model precludes parallelization within training examples, 
                    which becomes critical at longer sequence lengths, as memory constraints limit batching across examples.
                </li>
                <li>
                    <a href="https://en.wikipedia.org/wiki/Attention_(machine_learning)">Attention</a> mechanisms have become an integral part of sequence modeling and transduction models in various tasks, 
                    <mark>allowing modeling of dependencies without regard to their distance in the input or output sequences.</mark>
                </li>
                <li>
                    In this work we propose the <b>Transformer</b>, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output.
                </li>
            </ul>
        <!-- <br>
        <li>
            Background
        </li>
            <ul>
                <li>
                    Self-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence.
                </li>
            </ul> -->
        <br>
        <li>
            Model Architecture
        </li>
            <ul>
                <li>
                    Most competitive neural sequence transduction models have an <b>encoder-decoder structure</b>.
                    The encoder maps an input sequence of symbol representations \((x_1, \, ..., \, x_n) \) to a sequence of continuous representations \(z = (z_1, \, ..., \, z_n) \).
                    Given \(z\), the decoder then generates an output sequence \((y_1, \, ..., \, y_m ) \).
                </li>
                <li>
                    At each step the model is <b>auto-regressive</b>, consuming the previously generated symbols as additional input when generating the next.
                    <figure>
                        <img src="/img/attention-is-all-you-need-1.png" width="50%">
                        <figcaption>Transformer model architecture</figcaption>
                    </figure>
                </li>
                <br>
                <li>
                    The Transformer follows this overall architecture using stacked self-attention and point-wise (position-wise), 
                    fully connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1, respectively.
                </li>
                <br>
                        <li>
                            Encoder and Decoder stacks
                        </li>
                            <ul type="circle">
                                <li>
                                    <b>Encoder</b>: Each layer has two sub-layers.
                                    The first is a multi-head self-attention mechanism, and the second is a simple, position-wise fully connected feed-forward network. 
                                    <mark>We employ a residual connection around each of the two sub-layers</mark>, followed by <a href="https://velog.io/@tri2601/Layer-Normalization#:~:text=%EB%95%8C%20%ED%95%B4%EB%8B%B9%EB%90%98%EB%8A%94%20%EB%8C%80%EC%83%81%EB%93%A4%EC%9D%B4%EB%8B%A4.)-,%EB%B0%98%EB%A9%B4%20layer%20normalization%EC%97%90%EC%84%9C%EB%8A%94%20%EC%95%84%EB%9E%98%EC%99%80%20%EA%B0%99%EC%9D%B4%20%ED%95%98%EB%82%98%EC%9D%98%20%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%97%90%EC%84%9C%20Feature%20%EB%8C%80%EC%83%81%EC%9C%BC%EB%A1%9C%EB%A7%8C%20%ED%91%9C%EC%A4%80%ED%99%94%EA%B0%80%20%EC%9D%BC%EC%96%B4%EB%82%98%EB%8A%94%20%EA%B2%83%EC%9D%84%20%ED%99%95%EC%9D%B8%ED%95%A0%20%EC%88%98%20%EC%9E%88%EB%8B%A4.,-%EC%A6%89%2C%20layer%20normalization">layer normalization</a>.
                                    The output of each sub-layer is as:
                                    \[
                                        \,\\
                                        \text{LayerNorm}(x + \text{Sublayer}(x))
                                        \,\\
                                    \]
                                </li>
                                <li>
                                    <b>Decoder</b>: The decoder inserts a third sub-layer, which performs multi-head
                                    attention over the output of the encoder stack.
                                    Similar to the encoder, we employ residual connections around each of the sub-layers, followed by layer normalization. 
                                    <!-- <mark> -->
                                        The masking, combined with fact that the output embeddings are offset by one position, ensures that the
                                        predictions for position \(i\) can depend only on the known outputs at positions less than \(i\).
                                    <!-- </mark> -->
                                </li>
                            </ul>
                        <br>
                        <li>
                            Attention
                        </li>
                            <ul type="circle">
                                <li>
                                    An attention function can be described as mapping a query and a set of key-value pairs to an output,
                                    where the query, keys, values, and output are all vectors.
                                    The output is computed as a weighted sum of the values.

                                    <figure>
                                        <img src="/img/attention-is-all-you-need-2.png" width="85%">
                                        <figcaption>From the left, Scaled Dot-Product Attention · Multi-Head Attention</figcaption>
                                    </figure>
                                </li>
                                <br>
                                <li>
                                    Scaled Dot-Product Attention
                                </li>
                                    <ul>
                                        <li>
                                            The input consists of queries and keys of dimension \(d_k\), and values of dimension \(d_v\).
                                            We compute the dot products of the query with all keys, divide each by
                                            \(\sqrt{d_k} \), and apply a softmax function to obtain the weights on the values.
                                            \[
                                                \,\\
                                                \text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
                                                \,\\
                                            \]
                                        </li>
                                        <li>
                                            Dot-product attention is identical to our algorithm, except for the scaling factor of \(\frac{1}{\sqrt{d_k}} \).
                                        </li>
                                        <br>
<pre><code class="python">
    class ScaledDotProductAttention(nn.Module):
        def __init__(self):
            super().__init__()
            self.softmax = nn.Softmax(dim=-1)

        def forward(self, q, k, v):
            attention = self.softmax(
                torch.matmul(q, k.transpose(-1, -2))    # \(QK^{T}\)
                / torch.sqrt(torch.tensor(k.size(-1)))  # \(\sqrt(d_{k})\)
            )

            return torch.matmul(attention, v)
</code></pre>
                                    </ul>
                                <br>
                                <li>
                                    Multi-Head Attention
                                </li>
                                    <ul>
                                        <li>
                                            Instead of performing a single attention function with dmodel-dimensional keys, values and queries,
                                            <mark>
                                                we found it beneficial to linearly project the queries, keys and values \(h\) times with different, 
                                                learned linear projections to \(d_{k}\), \(d_{k}\) and \(d_{v}\) dimensions, respectively. 
                                            </mark>
                                        </li>
                                        <li>
                                            Multi-head attention allows the model to jointly attend to information from different representation
                                            subspaces at different positions. With a single attention head, averaging inhibits this.
                                            \[
                                                \,\\
                                                \text{MultiHead}(Q, K, V) = \text{Concat(head_1, ..., head_h)}W^O
                                                \,\\
                                            \]

                                            where \(head_i = \text{Attention}(QW^Q_i, \, KW^K_i, \, VW^V_i) \).
                                        </li>
                                        <li>
                                            In this work we employ \(h = 8\) parallel attention layers, or heads.
                                            For each of these we use \(d_k = d_v = d_{model}/h = 64  \). 
                                        </li>
                                        <br>
<pre><code class="python">
    class MultiHeadAttention(nn.Module):
        def __init__(self, h=8, d_model=512):
            super().__init__()
            self.h = h
            self.d_model = d_model
            
            self.d_k = d_model // h

            self.linear_q = nn.Linear(d_model, d_model)
            self.linear_k = nn.Linear(d_model, d_model)
            self.linear_v = nn.Linear(d_model, d_model)
            self.linear_o = nn.Linear(d_model, d_model)

            self.scaled_dot_product_attention = ScaledDotProductAttention()
            
        def forward(self, q, k, v):
            
            qw = self.linear_q(q).reshape(q.shape[0], -1, self.h, self.d_k).transpose(1, 2)
            kw = self.linear_k(k).reshape(q.shape[0], -1, self.h, self.d_k).transpose(1, 2)
            vw = self.linear_v(v).reshape(q.shape[0], -1, self.h, self.d_k).transpose(1, 2)
            
            attention_scores = self.scaled_dot_product_attention(qw, kw, vw).transpose(1, 2)
            attention_output = self.linear_o(attention_scores.reshape(q.shape[0], -1, self.d_model)) 
            
            return attention_output
</code></pre>
                                    </ul>
                            </ul>
                        <br>
                        <li>
                            Position-wise Feed-Forawrd Networks
                        </li>
                            <ul type="circle">
                                <li>
                                    In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully connected feed-forward network, 
                                    which is applied to each position separately and identically. 
                                    This consists of two linear transformations with a \(\text{ReLU}\) activation in between.
                                    \[
                                        \,\\
                                        \text{FFN}(x) = \text{max}(0, xW_1 + b_1)W_2 + b_2
                                        \,\\
                                    \]
                                </li>
                            </ul>
                            <br>
                        <li>
                            Positional Encoding
                        </li>
                            <ul type="circle">
                                <li>
                                    Since our model contains no recurrence and no convolution, in order for the model to make use of the
                                    order of the sequence, we must inject some information about the relative or absolute position of the
                                    tokens in the sequence.
                                </li>
                                <li>
                                    The positional encodings have the same dimension \(d_{model}\) as the embeddings, so that the two can be summed.
                                    \[
                                        \,\\
                                        \begin{aligned}
                                        PE_{(pos, 2i)} &= sin(pos / 10000^{2i/d_{model}}) \\
                                        PE_{(pos, 2i + 1)} &= cos(pos / 10000^{2i/d_{model}})
                                        \end{aligned}
                                        \,\\
                                    \]

                                    where \(pos\) is the position and \(i\) is the dimension.
                                    <br>
                                    (
                                        \(pos = 1\)인 경우 첫 번째 단어, \(pos = 2\)인 경우 두 번째 단어. <br>
                                        \(d_{model} = 512\)인 경우 \(i\) 는 0 ~ 511. <br>
                                        \(pos\)를 큰 수로 나누어 노말라이즈? <br>
                                        임베딩 된 단어 벡터들이 서로 가까운 위치에 있으면 비슷한 주기의 포지셔널 인코딩 값을 가지므로,
                                        단어벡터와 포지셔널 인코딩 값이 더해졌을 때 유사한 값이 됨

                                    )
                                </li>
                            </ul>
                        </ul>
            </ul>
    </ol>
    <!-- <br>
    <li>
        Implementation
    </li> -->
</ul>


<br><br>