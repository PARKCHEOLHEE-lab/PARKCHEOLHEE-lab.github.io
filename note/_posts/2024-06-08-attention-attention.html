---
title:  "Attention? Attention!"
layout: post
emoji: /emoji/eyes.png
---


<div id="toc"></div>


<div class="article">

    This is based on the post <a href="https://lilianweng.github.io/posts/2018-06-24-attention/">Attention? Attention!</a> 

    <!-- <br><br><br><br> -->

    <hr>
    <br>
    <figure>
        <img src="https://lilianweng.github.io/posts/2018-06-24-attention/shiba-example-attention.png" width="100%">
        <figcaption>
            A Shiba Inu in a men's outfit. The credit of the original photo goes to Instagram 
            <br>
            <a href="https://www.instagram.com/mensweardog/?hl=en">@mensweardog</a>.
        </figcaption>
    </figure><br>

    Given a small patch of an image, pixels in the rest provide clues what should be displayed there. 
    <strong>We expect to see a pointy ear in the yellow box because we have seen a dog's nose, another pointy ear on the right</strong> (stuff in the red boxes).
    However, the sweater and blanket at the bottom (stuff in the gray boxes) would not be as helpful as those doggy features.
    
    <br><br>

    Similarly, we can explain the relationship between words in one sentence or close context. 
    <strong>When we see "eating", we expect to encounter a food word very soon.</strong> 

    Attention in deep learning can be broadly interpreted as a vector of importance weights:
    in order to predict or infer one element, such as a pixel in an image or a word in a sentence, 
    <strong>we estimate using the attention vector how strongly it is correlated with (attends to) other elements.</strong>

    <br><br>
    <figure>
        <img src="https://lilianweng.github.io/posts/2018-06-24-attention/sentence-example-attention.png" width="70%">
        <figcaption>
            One word "attends" to other words in the same sentence differently.
        </figcaption>
    </figure><br>
</div><br><br>

<h3>
    What's Wrong with Seq2Seq Model?
</h3>
<div class="article">
    The seq2seq model normally has an encoder-decoder architecture, composed of:
    <ul>
        <li>
            An <strong>encoder</strong> processes the input sequence and compresses the information into a context vector of a fixed length. 
            This representation is expected to be a good summary of the meaning of the whole source sequence.
        </li>
        <li>
            A <strong>decoder</strong> is initialized with the context vector to emit the transformed output. 
            The early work only used the last state of the encoder network as the decoder initial state.
        </li>
        <figure>
            <img src="https://lilianweng.github.io/posts/2018-06-24-attention/encoder-decoder-example.png" width="80%">
            <figcaption>
                The encoder-decoder model, translating the sentence "she is eating a green apple" to Chinese
            </figcaption>
        </figure>
        <br>
        <figure>
            <img src="https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-unrolled.png" width="70%">
            <figcaption>
                An unrolled recurrent neural network diagram taken from
                <br>  <a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/">Understanding-LSTMs</a>
            </figcaption>
        </figure><br>

        A critical and apparent disadvantage of this fixed-length context vector design 
        is incapability of remembering long sentences. 
        <strong>
            Often it has forgotten the first part once it completes processing the whole input.
        </strong>
            The attention mechanism was born to resolve this problem.
    </ul>
</div><br><br>

<h3>Born for Translation</h3>
<div class="article">
    Rather than building a single context vector out of the encoder's last hidden state, 
    the secret sauce invented by attention is to create <strong>shortcuts</strong> between the context vector and the entire source input. 
    While the context vector has access to the entire input sequence, we donâ€™t need to worry about forgetting.

    <br><br>

    The context vector consumes three pieces of information:
    <ul>
        <li>
            encoder hidden states
        </li>
        <li>
            decoder hidden states
        </li>
        <li>
            alignment between source and target
        </li>
    </ul>

    <br>
    <figure>
        <img src="https://lilianweng.github.io/posts/2018-06-24-attention/encoder-decoder-attention.png" width="100%">
        <figcaption>The encoder-decoder model with additive attention mechanism in 
            <br>
            <a href="https://arxiv.org/pdf/1409.0473">Neural Machine Translation by Jointly Learning to Align and Translate</a>
        </figcaption>
    </figure>

    <br><br>

    Now let's define the attention mechanism. Say, we have a source sequence \(\mathbf{x}\) of length \(n\)
    and try to output a target sequence \(\mathbf{y} \) of lenfth \(m\) (variables in bold indicate that they are vectors):
    \[
        \,\\
        \begin{aligned}
        \mathbf{x} &= [x_1, x_2, \dots, x_n] \\
        \mathbf{y} &= [y_1, y_2, \dots, y_m]
        \end{aligned}
        \,\\
        \,\\
    \]

    The encoder is a bidirectional RNN with a forward hidden state \(\overrightarrow{h}_i \) and a backward one \(\overleftarrow{h}_i\).
    A simple concatenation of two represents the encoder state.
    \[
        \,\\
        h_i = [\overrightarrow{h}_i^\top; \overleftarrow{h}_i^\top]^\top, i=1,\dots,n
        \,\\
        \,\\
    \]

    The decoder network has hidden state \(s_t = f(s_{t-1}, \, y_{t-1}, \, c_t  ) \)
    for the output word at position \(t\, (t = 1, \, \dots, \, m) \),
    where the <strong>context vector</strong> \(c_t = \sum_{i=1}^n \alpha_{t,i} \, h_i \) <strong>is a sum of hidden states of the input sequence, weighted by alignment scores.</strong>
    The alignment model assigns a score \(\alpha_{t,i}\) to the pair of input at position \(i\)
    and output at position \(t\), \( (y_t, x_i) \), baed on how well they match.
    \[
        \,\\
        a_{t,i} = \frac{\exp(\text{score}(s_{t-1}, h_i))}{\sum_{i'=1}^n \exp(\text{score}(s_{t-1}, h_{i'}))} 
        \,\\
        \,\\
    \]

    The set of \( {\alpha_{t,i}} \) are weights defining how much of each source hidden state
    should be considered for each output.
    In Bahdanau's paper, the alignment score \(\alpha\) is parameterized by a
    feed-forward network with a single hidden layer, and this network is jointly
    trained with other parts of the model.
    The score function is therefore in the following form, given 
    that \(\tanh\) is used as the non-linear activation function:

    \[
        \,\\
        \text{score}(s_t, h_i) = \mathbf{v}_a^\top \tanh(\mathbf{W}_a[s_t; h_i])
        \,\\
        \,\\
    \]

    where both \(\mathbf{v}_a\) and \(\mathbf{W_a}\) are weight matrices to be learned in the alignment model.

    <br><br>

<pre><code class="python">
    class BahdanauAttention(nn.Module):
        def __init__(self, hidden_size):
            super(BahdanauAttention, self).__init__()
            self.Wa = nn.Linear(hidden_size, hidden_size)
            self.Ua = nn.Linear(hidden_size, hidden_size)
            self.Va = nn.Linear(hidden_size, 1)

        def forward(self, query, keys):
            # query: s_{t-1}
            # keys: h_i  

            scores = self.Va(torch.tanh(self.Wa(query) + self.Ua(keys)))
            scores = scores.squeeze(2).unsqueeze(1)

            weights = F.softmax(scores, dim=-1)
            context = torch.bmm(weights, keys)

            return context, weights
</code></pre>
<figcaption class="nofig">Additive Attention taken from
    <br>
     <a href="https://docs.pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html#:~:text=class%20BahdanauAttention(,context%2C%20weights">NLP From Scratch: Translation with a Sequence to Sequence Network and Attention</a>
</figcaption>
    <br><br>

    The matrix of alignment scores is a nice byproduct to explicitly show the <strong>correlation between source and target</strong> words.
    <figure>
        <img src="https://lilianweng.github.io/posts/2018-06-24-attention/bahdanau-fig3.png" width="70%">
        <figcaption>
            Alignment Matrix
        </figcaption>
    </figure>

    <!-- 
        https://chatgpt.com/c/68cfcbff-bb9c-8331-a868-a8fb0f5d09bc
    -->

</div><br><br>

<h3>A Family of Attention Mechanisms</h3>
<div class="article">

</div><br><br>

<h3>Transformer</h3>
<div class="article">

</div><br><br>

<h3>Self-Attention GAN</h3>
<div class="article">

</div><br><br>

<h3>References</h3>
<div class="article">
    <ul>
        <li>
            <a href="https://lilianweng.github.io/posts/2018-06-24-attention/">https://lilianweng.github.io/posts/2018-06-24-attention/</a>
        </li>
        <li>
            <a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/">https://colah.github.io/posts/2015-08-Understanding-LSTMs/</a>
        </li>
        <li>
            <a href="https://chatgpt.com/c/68cfcbff-bb9c-8331-a868-a8fb0f5d09bc">https://chatgpt.com/c/68cfcbff-bb9c-8331-a868-a8fb0f5d09bc</a>
        </li>
    </ul>
</div>


<br><br>