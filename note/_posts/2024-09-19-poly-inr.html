---
title: "Polynomial Implicit Neural Representations For Large Diverse Datasets"
layout: post
emoji: /emoji/brain.png 
---

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
      TeX: {
        extensions: ["cancel.js"]
      }
    });
</script>
<script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

<br>

<ul>
    <li>
        <a href="https://arxiv.org/pdf/2303.11424">Polynomial Implicit Neural Representations For Large Diverse Datasets</a>
    </li>
    <i>
       keywords to search: Frequency, 
       Positional Encoding, 
       Taylor Series
    </i>
    <br><br>
    <ol>
        <li>
            Abstract
        </li>
            <ul>
                <li>
                    Most INR architectures rely on sinusoidal <b>positional encoding</b>, which accounts for high-frequency information in data. 
                    However, the <b>finite encoding size restricts the model's representational power</b>.
                    <ul type="circle">
                        <li>
                            이미지 데이터에서, high-frequency는 sharp variations(edges, fine details), low-frequency는 smooth variations(bigger, less details) 의미
                        </li>
                        <li>
                            <b>Positional encodings</b> in the context of image data provide a way for neural networks, <b>to understand the spatial relationships</b> between different points or pixels in an image.
                        </li>
                    </ul>
                </li>
                <br>
                <li>
                    convolution, normalization, self-attention을 사용하지 않고 이미지 생성. i.e., <b>no interaction between the pixels</b>
                </li>
                <li>
                    <a href="https://github.com/Rajhans0/Poly_INR">https://github.com/Rajhans0/Poly_INR</a>
                </li>
            </ul>
        <br>
        <li>
            Introduction
        </li>
            <ul>
                <li>
                    최근 생성모델은 대부분 CNNs을 기반으로 하지만, <mark>Implicit Neural Representations(INRs)와 같은 연구는 <b>이미지를 좌표값에 대한 연속함수</b>로 나타내며, 각각의 픽셀은 독립적으로 합성된다.</mark>
                </li>
                <li>
                    INRs generally consists of a positional encoding module and a MLP. The positional encoding in INRs is based on sine functions, often reffered to as <b>Fourier features</b>.
                    <ul type="circle">
                        <li>
                            <a href="https://velog.io/@gjghks950/Fourier-Features-Let-Networks-Learn-High-Frequency-Functions-in-Low-Dimensional-Domains-%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0#:~:text=1.%20Introduction-,Fourier%2Dfeaturing%20%EC%9D%B4%EB%9E%80%2C%20coordinate%20space%20point%20%EB%A5%BC%20frequency%20space%20%EB%A1%9C%20embedding%20%ED%95%98%EB%8A%94%20function%20%EC%9D%98%20%EC%B4%9D%EC%B9%AD%20%EC%9D%B4%EB%8B%A4.,-Deeplearning%20model%20%EC%97%90%EC%84%9C%EC%9D%98">Fourier features</a> is a positional encoding that <b>embeds input coordinates to higher-dimensional vectors using sine functions</b>.
                        </li>
                    </ul>
                </li>
                <br>
                <li>
                    Several methods have shown that using MLP without sinusoidal positional encoding generates blurry outputs, i.e., only preserves low-frequency information.
                </li>
                <li>
                    ReLU의 piece-wise linear nature에 의해 2차 이상의 미분이 0이므로, ReLU 기반의 네트워크는 high-frequency 함수를 표현하는데 좋지 않다.
                    <ul type="circle">
                        <li>
                            In tasks requiring high precision for high-frequency information, 
                            such as image generation, signal processing, or implicit representations, ReLU-based MLP often fail to capture fine details, 
                            resulting in blurry or inaccurate outputs.
                        </li>
                    </ul>
                </li>
                <br>
                <li>
                    Positional encoding을 사용하지 않고, 점진적으로 다항식의 차수를 MLP의 깊이에 따라 점진적으로 증가시키며,
                    이는 feature와 각 ReLU 레이어 이후에 얻어진 affine transformed coordinate location(아핀변환 + 비선형성, \( \sigma(Wx + b)\))간의 element-wise multiplication을 통해 이루어진다. 
                </li>
                <li>
                    Affine 파라미터는 알려진 분포에서 샘플링된 latent code에 의해 학습된다.
                </li>
            </ul>
        <br>
        <li>
            Method
        </li>
            <ul>
                <li>
                    We are interested in a class of functions that represent an image in the form:
                    \[
                        \,\\
                        G(x, y) = g_{00} + g_{10}x + g_{01}y \,+\, ... \,+\, g_{pq}x^{p}y^{q}
                        \,\\
                    \]
                    <ul type="circle">
                        <li>
                            where, \((x, y)\) is the <b>normalized pixel location</b> sampled from a coordinate grid of size \(H \times W\),
                            while the coefficients of the polynomial \((g_{pq})\) are parameterized by a latent vector \(z\) sampled from a known distribution.
                        </li>
                        <li>
                            파라미터 \(g_{pq}\) 는 \((x, y)\) 좌표를 기반으로 다항식의 각 항의 behaivor 조절하고 low-frequency & high-frequency 정보를 모두 표현가능
                        </li>
                    </ul>
                </li>
                <br>
                <li>
                    Therefore, to form an image, we evaluate the generator \(G\) for all pixel locations \((x, y)\) for a given fixed \(z\):
                    \[
                        \,\\
                        I = \{ G(x, y; z) \,|\, (x, y) \in CoordinateGrid(H, W) \}
                        \,\\
                    \]
                    <ul type="circle">
                        <li>
                            where, \(CoordinateGrid(H, W) = \{ ( \frac{x}{W - 1}, \frac{y}{H - 1} ) \,|\, 0 \leq x < W, 0 \leq y < H \} \).
                        </li>
                        <li>
                            By sampling different latent vectors \(z\), we generate different polynomials and represent images over a distribution of real images.
                        </li>
                    </ul>
                </li>
                <br>
                <li>
                    Our model consists of two parts: a) <b>Mapping Newtork</b>, which takes the latent code \(z\) and maps it to affine parameters space \(\mathbf{W}\), 
                    and b) <b>Synthesis Network</b>, which takes the pixel location and generates the corresponding RGB value.
                    <br>
                    <img src="/img/poly-inr-1.png" width="100%">
                    <figcaption>Polynomial Implicit Neural Representation (Poly-INR) based generator architecture</figcaption>
                    <br>

                    <ol type="a">
                        <li>
                            <b>Mapping Network</b>
                        </li>
                            <ul type="circle">
                                <li>
                                    The mapping network takes the latent code \(z \in \mathbb{R}^{64} \) and maps it to the space \(\mathbf{W} \in \mathbb{R}^{512} \) (This mapping network is used in <a href="https://arxiv.org/pdf/2202.00273">StyleGAN-XL</a>).
                                </li>
                                <li>
                                    It consists of a <b>pre-trained class embedding</b>, 
                                    which embeds the one hot class label into a 512 dimension vector and concatenates it with the latent code \(z\)
                                    Then the mapping network consists of an MLP with two layers.
                                    <br>
<pre><code class="python">
    self.mapping_network = nn.Sequential(
        nn.Linear(64 + class_dim, 512)
        nn.ReLU()
        nn.Linear(512, 512)
    )
</code></pre>
                                </li>
                                <br>
                                <li>
                                    <code>mapping_network</code> is used to generate <b>affine transformation matrix</b>; hence we call \(\mathbf{W}\) as affine parameters space.
                                </li>
                            </ul>
                        <br>
                        <li>
                            <b>Synthesis Network</b>
                        </li>
                            <ul type="circle">
                                <li>
                                    The synthesis network generates the RGB (\(\mathbb{R}^3\)) value for the given pixel location \((x, y)\).
                                </li>
                                <li>
                                    The synthesis network consists of multiple levels; 
                                    at each level, <b>it receives the affine transformation parameters</b> from the mapping network and the pixel coordinate location
                                </li>
                                <li>
                                    Leaky-ReLU layer is used with <code>negative_slope=0.2</code>.
                                </li>
                                <li>
                                    The synthesis network can be expressed as:
                                    \[
                                        \,\\
                                            G_{syn} = \ldots \sigma(W_{2}((A_{2}X) \odot \sigma(W_{1}((A_{1}X) \odot \sigma(W_{0}(A_{0}X))))))
                                        \,\\
                                    \]
                                    <ul type="cir">
                                        <li>
                                            where \(X \in \mathbb{R}^{3 \times HW} \) is the coordinate grid of size \(H \times W\), 
                                        </li>
                                        <li>
                                            Affine transformation matrix \(A_{i} \in \mathbb{R}^{n \times 3}  \).
                                        </li>
                                        <li>
                                            <mark>element-wise multiplication을 level별로 수행하면서 <b>다항식의 차수</b>를 점진적으로 증가시키고 모델이 <b>표현의 복잡도</b>를 학습</mark>
                                        </li>
                                    </ul>
                                <br>
                                    이런느낌?
<pre><code class="python">
    def forward(self, z, class_embedding, x):
        w = self.mapping_network(z, class_embedding)

        for level in range(len(self.affine_layers)):
            # 1x3 @ 3x3 -> 1x3
            transformed_coord = self.affine_layers[level](w, x)

            # 3 -> 512
            linear_output = self.linears[level](transformed_coord)
            
            # apply Leaky-ReLU
            x = self.lrelu(x)

            # element-wise multiplication
            x *= linear_output

        # 512 -> 3
        rgb = self.output_layer(x)

        return rgb
</code></pre>
    <br>
                                </li>
                            </ul>
                    </ol>
                </li>
            </ul>
        <br>
        <li>
            Experiments
        </li>
            <!-- <ol type="a">
                <li>
                    Quantitative results
                </li>
                <li>
                    Qualitative results
                </li>
            </ol> -->
        <br>
        <li>
            Training details
        </li>
            <!-- <ul>
                <li>

                </li>
            </ul> -->
    </ol>
</ul>




<br><br>