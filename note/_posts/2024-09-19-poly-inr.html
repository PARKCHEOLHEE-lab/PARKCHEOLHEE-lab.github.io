---
title: "Polynomial Implicit Neural Representations For Large Diverse Datasets"
layout: post
emoji: /emoji/brain.png 
---

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
      TeX: {
        extensions: ["cancel.js"]
      }
    });
</script>
<script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

<br>

<ul>
    <li>
        <a href="https://arxiv.org/pdf/2303.11424">Polynomial Implicit Neural Representations For Large Diverse Datasets</a>
    </li>
    <i>
       keywords to search: Frequency, 
       Positional Encoding, 
       Taylor Series
    </i>
    <br><br>
    <ol>
        <li>
            Abstract
        </li>
            <ul>
                <li>
                    Most INR architectures rely on sinusoidal <b>positional encoding</b>, which accounts for high-frequency information in data. 
                    However, the <b>finite encoding size restricts the model's representational power</b>.
                    <ul type="circle">
                        <li>
                            이미지 데이터에서, high-frequency는 sharp variations(edges, fine details), low-frequency는 smooth variations(bigger, less details) 의미
                        </li>
                        <li>
                            <b>Positional encodings</b> in the context of image data provide a way for neural networks, <b>to understand the spatial relationships</b> between different points or pixels in an image.
                        </li>
                    </ul>
                </li>
                <br>
                <li>
                    convolution, normalization, self-attention을 사용하지 않고 이미지 생성. i.e., <b>no interaction between the pixels</b>
                </li>
                <li>
                    <a href="https://github.com/Rajhans0/Poly_INR">https://github.com/Rajhans0/Poly_INR</a>
                </li>
            </ul>
        <br>
        <li>
            Introduction
        </li>
            <ul>
                <li>
                    최근 생성모델은 대부분 CNNs을 기반으로 하지만, <mark>Implicit Neural Representations(INRs)와 같은 연구는 <b>이미지를 좌표값에 대한 연속함수</b>로 나타내며, 각각의 픽셀은 독립적으로 합성된다.</mark>
                </li>
                <li>
                    INRs generally consists of a positional encoding module and a MLP. The positional encoding in INRs is based on sine functions, often reffered to as <b>Fourier features</b>.
                    <ul type="circle">
                        <li>
                            <a href="https://velog.io/@gjghks950/Fourier-Features-Let-Networks-Learn-High-Frequency-Functions-in-Low-Dimensional-Domains-%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0#:~:text=1.%20Introduction-,Fourier%2Dfeaturing%20%EC%9D%B4%EB%9E%80%2C%20coordinate%20space%20point%20%EB%A5%BC%20frequency%20space%20%EB%A1%9C%20embedding%20%ED%95%98%EB%8A%94%20function%20%EC%9D%98%20%EC%B4%9D%EC%B9%AD%20%EC%9D%B4%EB%8B%A4.,-Deeplearning%20model%20%EC%97%90%EC%84%9C%EC%9D%98">Fourier features</a> is a positional encoding that <b>embeds input coordinates to higher-dimensional vectors using sine functions</b>.
                        </li>
                    </ul>
                </li>
                <br>
                <li>
                    Several methods have shown that using MLP without sinusoidal positional encoding generates blurry outputs, i.e., only preserves low-frequency information.
                </li>
                <li>
                    ReLU의 piece-wise linear nature에 의해 2차 이상의 미분이 0이므로, ReLU 기반의 네트워크는 high-frequency 함수를 표현하는데 좋지 않다.
                    <ul type="circle">
                        <li>
                            In tasks requiring high precision for high-frequency information, 
                            such as image generation, signal processing, or implicit representations, ReLU-based MLP often fail to capture fine details, 
                            resulting in blurry or inaccurate outputs.
                        </li>
                    </ul>
                </li>
                <br>
                <li>
                    Positional encoding을 사용하지 않고, 점진적으로 다항식의 차수를 MLP의 깊이에 따라 점진적으로 증가시키며,
                    이는 feature와 각 ReLU 레이어 이후에 얻어진 affine transformed coordinate location(아핀변환 + 비선형성, \( \sigma(Wx + b)\))간의 element-wise multiplication을 통해 이루어진다. 
                </li>
                <li>
                    Affine 파라미터는 알려진 분포에서 샘플링된 latent code에 의해 학습된다.
                </li>
            </ul>
        <br>
        <li>
            Method
        </li>
            <ul>
                <li>
                    We are interested in a class of functions that represent an image in the form:
                    \[
                        \,\\
                        G(x, y) = g_{00} + g_{10}x + g_{01}y \,+\, ... \,+\, g_{pq}x^{p}y^{q}
                        \,\\
                    \]
                    <ul type="circle">
                        <li>
                            where, \((x, y)\) is the <b>normalized pixel location</b> sampled from a coordinate grid of size \(H \times W\),
                            while the coefficients of the polynomial \((g_{pq})\) are parameterized by a latent vector \(z\) sampled from a known distribution.
                        </li>
                        <li>
                            파라미터 \(g_{pq}\) 는 \((x, y)\) 좌표를 기반으로 다항식의 각 항의 behaivor 조절하고 low-frequency & high-frequency 정보를 모두 표현가능
                        </li>
                    </ul>
                </li>
                <br>
                <li>
                    Therefore, to form an image, we evaluate the generator \(G\) for all pixel locations \((x, y)\) for a given fixed \(z\):
                    \[
                        \,\\
                        I = \{ G(x, y; z) \,|\, (x, y) \in CoordinateGrid(H, W) \}
                        \,\\
                    \]
                    <ul type="circle">
                        <li>
                            where, \(CoordinateGrid(H, W) = \{ ( \frac{x}{W - 1}, \frac{y}{H - 1} ) \,|\, 0 \leq x < W, 0 \leq y < H \} \).
                        </li>
                        <li>
                            By sampling different latent vectors \(z\), we generate different polynomials and represent images over a distribution of real images.
                        </li>
                    </ul>
                </li>
                <br>
                <li>
                    Our model consists of two parts: 1) <b>Mapping newtork</b>, which takes the latent code \(z\) and maps it to affine parameters space \(\mathbf{W}\), 
                    and 2) <b>Synthesis network</b>, which takes the pixel location and generates the corresponding RGB value.
                    <br>
                    <img src="/img/poly-inr-1.png" width="100%">
                    <figcaption>Polynomial Implicit Neural Representation (Poly-INR) based generator architecture</figcaption>
                    <br>
                    
                    <ul type="circle">
                        <li>
                            Mapping Network: The mapping network takes the latent code \(z \in \mathbb{R}^{64} \) and maps it to the space \(\mathbf{W} \in \mathbb{R}^{512} \) (This mapping network is used in <a href="https://arxiv.org/pdf/2202.00273">StyleGAN-XL</a>).
                        </li>
                        <li>
                            It consistsof a <b>pre-trained class embedding</b>, 
                            which embeds the one hot class label into a 512 dimension vector and concatenates it with the latent code \(z\)
                        </li>
                    </ul>
                </li>
            </ul>
        <br>
        <li>
            Experiments
        </li>
            <ol type="a">
                <li>
                    Quantitative results
                </li>
                <li>
                    Qualitative results
                </li>
            </ol>
        <br>
        <li>
            Training details
        </li>
            <ul>
                <li>

                </li>
            </ul>
    </ol>
</ul>




<br><br>