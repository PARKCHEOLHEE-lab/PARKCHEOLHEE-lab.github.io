---
title:  "Auto-Encoding Variational Bayes"
layout: post
emoji: /emoji/brain.png
---


<br>

<ul>
    <li>
        <a href="https://arxiv.org/pdf/1312.6114">Auto-Encoding Variational Bayes</a>
    </li>
        <ol>
            <li>
                Introduction
            </li>
                <ul>
                    <li>
                        For the case of an <a href="https://wikidocs.net/215060">i.i.d.</a> dataset 
                        and continuous latent variables per datapoint, we propose the Auto-Encoding VB (AEVB) algorithm. 
                    </li>
                    <li>
                        The learned approximate posterior inference model can also be used
                        for a host of tasks such as recognition, denoising, representation and visualization purposes. 
                        When a neural network is used for the recognition model, we arrive at the variational auto-encoder
                    </li>
                </ul>
            <br>
            <li>
                Method
            </li>
                <ul>
                    <li>
                        The strategy in this section can be 
                        <mark>
                            used to derive a 
                            <a href="https://seongukzz.tistory.com/3#:~:text=VAE%EB%8A%94%20AE%EC%97%90%20Generative%20Model%EC%9D%84%20%EC%A0%81%EC%9A%A9%ED%95%98%EA%B3%A0%EC%9E%90%20%ED%95%98%EB%8A%94%20%EA%B2%83%EC%9D%B4%20%EB%AA%A9%EC%A0%81%EC%9D%B4%EA%B3%A0%2C%20%EC%9D%B4%EB%95%8C%20%EC%9A%B0%EB%A6%AC%EB%8A%94%20%EC%A3%BC%EC%96%B4%EC%A7%84%20%EC%83%98%ED%94%8C%20X%EC%97%90%20%EB%8C%80%ED%95%9C%20%EB%B3%B5%EC%9E%A1%ED%95%9C%20%EB%B6%84%ED%8F%AC%EB%A5%BC%20%EC%95%8C%20%EC%88%98%20%EC%97%86%EA%B8%B0%20%EB%95%8C%EB%AC%B8%EC%97%90%20%EC%9D%B4%EB%A5%BC%20%EC%9E%98%20%EC%95%8C%EA%B3%A0%20%EC%9E%88%EB%8A%94%20%EC%A0%95%EA%B7%9C%20%EB%B6%84%ED%8F%AC%EB%A1%9C%20%EB%82%98%ED%83%80%EB%82%B4%EA%B3%A0%2C%20%EC%9D%B4%20%EC%A0%95%EA%B7%9C%20%EB%B6%84%ED%8F%AC%EB%A1%9C%EB%B6%80%ED%84%B0%20%EB%8B%A4%EC%8B%9C%20%EC%A3%BC%EC%96%B4%EC%A7%84%20%EC%83%98%ED%94%8C%20X%EC%9D%98%20%EB%B6%84%ED%8F%AC%EB%A5%BC%20%EB%94%B0%EB%A5%B4%EB%8A%94%20%EC%83%98%ED%94%8C%EC%9D%84%20%EC%83%9D%EC%84%B1%ED%95%B4%EB%82%B4%EB%8A%94%20%EA%B2%83%EC%9D%B4%EB%8B%A4.">
                                lower bound
                            </a> estimator</mark>
                        (a stochastic objective function) for a variety of directed graphical models with continuous latent variables.
                    </li>
                    <li>
                        We will restrict ourselves here to the common case 
                        <mark>
                            where we have an i.i.d. dataset with latent variables per datapoint, 
                        </mark>
                        and where we like to perform maximum likelihood (ML) 
                        or maximum a posteriori (MAP) inference on the (global) parameters, 
                        and variational inference on the latent variables.
                    </li>
                    <li>
                        Note that our method can be applied to online, non-stationary settings, 
                        e.g. streaming data, but here we assume a fixed dataset for simplicity.
                    </li>
                </ul>
                <br>
                <ol type="I">
                    <li>
                        Problem scenario
                    </li>
                        <ul type="circle">
                            <li>
                                We assume that the data are generated by some random process, 
                                involving an unobserved continuous random variable \(\mathbf{z}\). 
                                The process consists of two steps: 
                                <i>(1)</i> a value \(z^{(i)}\) is generated from some prior distribution \(p_{\theta^*} (\mathbf{z})\);
                                <i>(2)</i> a value \(\mathbf{x}^{(i)}\) is generated from some conditional distribution \(p_{\theta^*}(\mathbf{x}|\mathbf{z})\)
                            </li>
                            <li>
                                <mark>
                                    We assume that the prior \(p_{\theta^*} (\mathbf{z}) \) and likelihood \(p_{\theta^*} (\mathbf{x}|\mathbf{z}) \)
                                    come from parametric families of distributions \(p_{\theta} (\mathbf{z}) \) and \(p_{\theta} (\mathbf{x}|\mathbf{z}) \)
                                </mark>, and that their <a href="https://blog.naver.com/mykepzzang/220835810657">PDFs</a> are differentiable almost everywhere w.r.t both \(\theta\) and \(\mathbf{z} \)
                            </li>
                            <li>
                                We are interested in, and propose a solution to, problems in the above scenario:
                                    <ol type="i">
                                        <li>
                                            Efficient approximate ML or MAP estimation for the parameters \(\theta\).
                                        </li>
                                        <li>
                                            Efficient approximate posterior inference of the latent variable \(\mathbf{z}\) given an observed value \(\mathbf{x}\)
                                            for a choice of parameters \(\theta\). This is useful for coding or data representation tasks.
                                        </li>
                                    </ol>
                            </li>
                            <br>
                            <li>
                                For the purpose of solving the above problems, let us introduce a recognition model \(q_{\phi}(\mathbf{z}|\mathbf{x})\)
                                : an approximation to the intractable true posterior \(p_{\theta}(\mathbf{z}|\mathbf{x}) \).
                            </li>
                            <li>
                                From a coding theory perspective, the unobserved variables \(\mathbf{z}\) have an interpretation as a latent representation.
                                In this paper we will therefore also <mark>refer to the recognition model \(q_{\phi}(\mathbf{z}|\mathbf{x})\) as a probabilistic encoder</mark>,
                                since given a datapoint \(\mathbf{x}\) it produces a distribution over the possible values of the code \(\mathbf{z}\) from which the datapoint \(\mathbf{x}\) could have been generated.

                                <ol type="i">
                                    <li>
                                        \(q_{\phi}(\mathbf{z}|\mathbf{x})\) is the encoder, mapping \(\mathbf{x}\) to a distribution over \(\mathbf{z}\)
                                    </li>
                                    <li>
                                        \(p_{\theta}(\mathbf{z}|\mathbf{x}) \) is the decoder, generating \(\mathbf{x}\) from the latent variable \(\mathbf{z}\)
                                    </li>
                                </ol>
                            </li>
                        </ul>
                    <br>
                    <li>
                        The variational bound
                    </li>
                        <ul>
                            <li>
                                The marginal likelihood is composed of a sum over the marginal likelihoods of individual datapoints
                                \(\log p_{\theta} (\mathbf{x}^{(1)}, ..., \, \mathbf{x}^{(N)} ) \,=\, \sum^{N}_{i=1} \log p_{\theta}(\mathbf{x}^{(i)}) \), which can each be rewritten as:
                                
                                \[
                                    \,\\
                                       \log p_{\theta} (\mathbf{x}^{(i)}) = \text{D}_{KL}( q_{\phi}(\mathbf{z}|\mathbf{x}^{(i)}) \| p_{\theta}(\mathbf{z}|\mathbf{x}^{(i)}) ) + \mathcal{L}(\theta, \phi; \mathbf{x}^{(i)})
                                    \,\\
                                \]
                            </li>
                        </ul>
                </ol>
        </ol>
</ul>

<br><br>

<!-- https://chatgpt.com/g/g-LmRBZpu94-arxivgpt/c/675d0ffd-09d4-8011-a3b6-26de75359fed -->
 <!-- https://nbviewer.org/github/dreamgonfly/VAE/blob/master/variational_autoencoder.ipynb -->