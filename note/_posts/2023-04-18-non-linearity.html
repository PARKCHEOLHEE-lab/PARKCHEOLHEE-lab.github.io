---
title:  "Non-linearity"
layout: post
---


<br>


<ul>
    <li>
        Linearity
        <ul>
            <li>
                Stacking multiple linear functions without any non-linear functions results in a single linear function 
                because the <mark>combination of linear transformations is still linear</mark>.
                <ol>
                    <li>
                        <b>Linear function definition</b>: 
                        There is a linear function in its most basic form,
                        where \(W\) is a matrix, \(x\) is the input, and \(b\) is a bias.
                        \[
                            \,\\
                            f(x) = Ax + b
                            \,\\
                        \]
                    </li>
                    <li>
                        <b>Combination of two linear functions</b>: 
                        Let's say we have two linear functions as below:
                        \[
                            \,\\
                            f(x) = W_1 x + b_1 \\
                            g(x) = W_2 x + b_2
                            \,\\
                        \]
                        then applying one after the other results in:
                        \[
                            \,\\
                            g(f(x)) = W_2(W_1 x + b_1) + b_2 = W_2 W_1 x + W_2 b_1 + b_2
                            \,\\
                        \]

                        This is still a linear function of the form \(h(x) = Cx + d\), where \(C = W_2 W_1\) and \(d = W_2 b_1 + b_2\).
                    </li>
                    <br>
                    <li>
                        This result shows that stacking linear functions are equivalent to a single linear transformation with 
                        new parameters \(C\) and \(d\).
                        Therefore, no matter how many linear functions you stack, the result is always one linear function.
                    </li>
                </ol>
            </li>
        </ul>
    </li>
    <br>
    <li>
        Non-Linearity for Neural Networks
        <ul>
            <li>
                The world is non-linear. 
                Many real-world phenomena consist of complex relationships that cannot be modeled using only linear functions. 
                <mark>By giving non-linearity, neural networks can represent these complex relationships.</mark>
                <ul>
                    <li>
                        \(\text{Sigmoid}\): Maps the input to a value between 0 and 1 using the formula:
                        \[
                            \,\\
                            \text{Sigmoid}\,(x) = \frac{1}{1 + e^{-x}}
                            \,\\
                        \]
<!-- <pre><code class="python">
    def sigmoid(x):
        return 1 / (1 + np.exp(-x))
</code></pre><br> -->
                    </li>
                    <li>
                        \(\text{ReLU} \) (Rectified Linear Unit): A simple non-linear function that returns \(0\) if the given \(x\) is below than \(0\), else return itself. It is defined as:
                        \[
                            \,\\
                            \text{ReLU}\,(x) = \begin{cases} 
                            x & \text{if } x > 0 \\
                            0 & \text{if } x \leq 0
                            \end{cases}
                            \,\\
                        \]
<!-- <pre><code class="python">
    def relu(x):
        return np.where(x > 0, x, 0)
</code></pre><br> -->
                    </li>
                    <li>
                        \(\text{LeakyReLU}\): A simple non-linear function that returns \(x\) if \(x\) is greater than \(0\), otherwise it returns \( \alpha x \), where \( \alpha \) is a small constant. It is defined as:
                        \[
                            \,\\
                            \text{LeakyReLU}\,(x) = \begin{cases} 
                            x & \text{if } x > 0 \\
                            \alpha x & \text{if } x \leq 0
                            \end{cases}
                            \,\\
                        \]
<!-- <pre><code class="python">
    def leaky_relu(x, alpha):
        return np.where(x > 0, x, x * alpha)
</code></pre><br> -->
                    </li>
                    <li>
                        \(\text{Tanh}\): Maps the input to a value between -1 and 1 using the formula:
                        \[ 
                            \,\\
                            \text{Tanh}\,(x) = \frac{e^x \,-\, e^{-x}}{e^x \,+\, e^{-x}}
                            \,\\
                        \]
<!-- <pre><code class="python">
    def tanh(x):
        return (np.exp(x) - np.exp(-x)) / (np.exp(x) + np.exp(-x))
</code></pre><br> -->
                    </li>
                    <li>
                        \(\text{Softmax}\): It converts logits into a probability distribution, where each output is a value between 0 and 1, and the sum of the outputs is 1. The formula for softmax is:
                        \[
                            \,\\
                            \text{Softmax}(x)_i = \frac{e^{x_i}}{\sum_{j=1}^{n} e^{x_j}}
                            \,\\
                        \]
<!-- <pre><code class="python">
    def softmax(x):
        return np.exp(x) / np.sum(np.exp(x))
</code></pre> -->
                    </li>
                </ul>
            </li>
        </ul>
    </li>
    <br>
    <li>
        Visualization
    </li>
</ul>

<span style="display: block; margin-left: -2em;">
    {% include embed.html url="/notebooks/non-linearity.html" %}
</span>

<br><br>
